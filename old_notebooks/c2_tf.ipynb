{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shortuuid in /home/jeremy/anaconda3/envs/tf-gpu/lib/python3.7/site-packages (1.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install shortuuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "import shortuuid\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Get reproducible results\n",
    "random_state = 46\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "\n",
    "\n",
    "def ann_file(data_dir):\n",
    "    return os.path.join(data_dir, \"TrainAnnotations.csv\")\n",
    "\n",
    "\n",
    "TRAIN_DATA_DIR = \"data/TrainData-C2\"\n",
    "TRAIN_DATA_ANN_FILE = ann_file(TRAIN_DATA_DIR)\n",
    "\n",
    "TRAIN_SPLIT_DATA_DIR           = \"data/train/split\"\n",
    "TRAIN_SPLIT_ANN_FILE           = ann_file(TRAIN_SPLIT_DATA_DIR)\n",
    "TRAIN_SPLIT_AUGMENTED_DATA_DIR = \"data/train/augmented\"\n",
    "TRAIN_SPLIT_AUGMENTED_ANN_FILE = ann_file(TRAIN_SPLIT_AUGMENTED_DATA_DIR)\n",
    "TRAIN_SPLIT_PATCHES_DATA_DIR   = \"data/train/patches\"\n",
    "TRAIN_SPLIT_PATCHES_ANN_FILE   = ann_file(TRAIN_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TRAIN_ALL_AUGMENTED_DATA_DIR   = \"data/train-all/augmented\"\n",
    "TRAIN_ALL_AUGMENTED_ANN_FILE   = ann_file(TRAIN_ALL_AUGMENTED_DATA_DIR)\n",
    "TRAIN_ALL_PATCHES_DATA_DIR     = \"data/train-all/patches\"\n",
    "TRAIN_ALL_PATCHES_ANN_FILE     = ann_file(TRAIN_ALL_PATCHES_DATA_DIR)\n",
    "\n",
    "VAL_SPLIT_DATA_DIR         = \"data/val/split\"\n",
    "VAL_SPLIT_ANN_FILE         = ann_file(VAL_SPLIT_DATA_DIR)\n",
    "VAL_SPLIT_PATCHES_DATA_DIR = \"data/val/patches\"\n",
    "VAL_SPLIT_PATCHES_ANN_FILE = ann_file(VAL_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TEST_DATA_DIR         = \"data/TestData/\"\n",
    "\n",
    "TEST_PATCHES_DATA_DIR = \"data/test/\"\n",
    "# TEST_PATCHES_DATA_DIR = \"data/test/patches\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU configuration\n",
    "If you have a GPU, enable experimental memory growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Generate random, stratified 80/20 split for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories for splits already exist. Skipping\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists(TRAIN_SPLIT_DATA_DIR) or os.path.exists(VAL_SPLIT_DATA_DIR)):\n",
    "    print(\"Data directories for splits already exist. Skipping\")\n",
    "else:\n",
    "    # Generate 80/20 split\n",
    "\n",
    "    print(\"Reading {} annotations...\".format(TRAIN_DATA_ANN_FILE))\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "    print(\"Splitting data into training and validation sets...\")\n",
    "    train_df, val_df = train_test_split(ann_df,\n",
    "                                        train_size=0.80,\n",
    "                                        random_state=138,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=ann_df[['annotation']].to_numpy(dtype=np.int32).flatten())\n",
    "\n",
    "    os.makedirs(TRAIN_SPLIT_DATA_DIR)\n",
    "    os.makedirs(VAL_SPLIT_DATA_DIR)\n",
    "    \n",
    "    print(\"Copying files for training split...\")\n",
    "    for _, row in train_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(TRAIN_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating training split annotations...\")\n",
    "    train_df.sort_values('file_name').to_csv(TRAIN_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Copying files for validation split...\")\n",
    "    for _, row in val_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(VAL_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating validation split annotations...\")\n",
    "    val_df.sort_values('file_name').to_csv(VAL_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data\n",
    "Because the training dataset is unbalanced, augment the training data set by generating\n",
    "new images for the lower numbered samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DESIRED_CLASS_SAMPLE_COUNT = 400\n",
    "RANDOM_STATE = 13\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "PATCH_ROWS = 3\n",
    "PATCH_COLUMNS = 3\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.endswith(IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def augment_data(src_dir, src_ann_file, dest_dir, dest_ann_file, class_sample_count=500):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "    ann_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'}) \n",
    "    new_samples = {}\n",
    "    \n",
    "    for i in range(5):\n",
    "        class_df = ann_df.query(\"annotation == '{}'\".format(i))\n",
    "        num_class_samples = class_df.shape[0]\n",
    "        num_to_create = class_sample_count - num_class_samples\n",
    "            \n",
    "        print(\"Creating {} images for class {}\".format(num_to_create, i))\n",
    "        samples = class_df.sample(n=num_to_create, replace=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "        for idx, row in samples.iterrows():\n",
    "            new_filename = row['file_name'].split('.')[0] + \"_\" + shortuuid.uuid() + \".png\"\n",
    "    \n",
    "            # Apply transformations to each randomly selected sample\n",
    "            img = Image.open(src_dir + \"/\" + row['file_name'])\n",
    "            image_transforms = transforms.Compose([\n",
    "                #transforms.RandomAffine(degrees=20, translate=(0.2, 0.2)),\n",
    "                #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                #transforms.RandomResizedCrop((480, 640), scale=(1.0, 1.2)),\n",
    "                \n",
    "                transforms.RandomRotation((90,90), expand=True),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomHorizontalFlip()\n",
    "            ])\n",
    "            transformed_img = image_transforms(img)\n",
    "            transformed_img.save(os.path.join(dest_dir, new_filename))\n",
    "    \n",
    "            new_samples[new_filename] = row['annotation']\n",
    "    \n",
    "    # Add to new dataframe\n",
    "    balanced_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    balanced_df = balanced_df.append(pd.DataFrame.from_records([(k, v) for k, v in new_samples.items()],\n",
    "                                                 columns=['file_name', 'annotation']))\n",
    "    \n",
    "    # Write new annotations\n",
    "    balanced_df.sort_values('file_name').to_csv(dest_ann_file, index=False)\n",
    "    \n",
    "    # Copy images from training data split\n",
    "    for file in glob.glob(src_dir + \"/*\"):\n",
    "        if is_image_file(file):\n",
    "            shutil.copy(file, os.path.join(dest_dir, os.path.basename(file)))\n",
    "\n",
    "\n",
    "def generate_image_patches(img, rows, cols):\n",
    "    \"\"\"\n",
    "    Generates a list of in-memory image overlapping patches\n",
    "    \n",
    "    Args:\n",
    "        rows - number of rows of patchs to cover the height of the image\n",
    "        cols - number of colums of patches to cover the width of the image\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    sizeX = img.shape[1]\n",
    "    sizeY = img.shape[0]\n",
    "    \n",
    "    patch_sizeX = 224\n",
    "    patch_sizeY = 224\n",
    "    patch_relative_centerX = 112\n",
    "    patch_relative_centerY = 112\n",
    "\n",
    "    for i in range(0,rows):\n",
    "        for j in range(0, cols):\n",
    "            center = (patch_relative_centerX + (sizeX - patch_sizeX)/(rows - 1)*i, \n",
    "                      patch_relative_centerY + (sizeY - patch_sizeY)/(cols - 1)*j)\n",
    "            patches.append(cv2.getRectSubPix(img, (patch_sizeX, patch_sizeY), center))\n",
    "            \n",
    "    return patches\n",
    "\n",
    "\n",
    "def generate_patch_files(in_dir, out_dir, rows, cols):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    images = [f for f in os.listdir(in_dir) if os.path.isfile(os.path.join(in_dir, f)) and is_image_file(f)]   \n",
    "    for im in images:\n",
    "        img = cv2.imread(os.path.join(in_dir, im))\n",
    "        patches = generate_image_patches(img, rows, cols)\n",
    "        \n",
    "        for i in range(0,rows):\n",
    "            for j in range(0, cols):\n",
    "                patch = patches[i*rows + j]\n",
    "                patch_name = im.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                cv2.imwrite(out_dir + '/' + patch_name, patch)\n",
    "\n",
    "\n",
    "def generate_patch_annotations_df(df, rows, cols):\n",
    "    patches_ann = {}\n",
    "    \n",
    "    for ind in df.index: \n",
    "        file_name = df['file_name'][ind]\n",
    "        annotation = df['annotation'][ind]\n",
    "        \n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                patch_name = file_name.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                patches_ann[patch_name] = annotation\n",
    "    \n",
    "    return pd.DataFrame.from_records([(k, v) for k, v in patches_ann.items()], \n",
    "                                     columns=['file_name', 'annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run data augmentation\n",
    "\n",
    "Perform the data augmentation on the training data set split to balance the class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented SPLIT training data already exists. Skipping.\n",
      "Augmented ALL training data already exists. Skipping.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TRAIN_SPLIT_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented SPLIT training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for SPLIT training data...\")\n",
    "    augment_data(TRAIN_SPLIT_DATA_DIR,\n",
    "                 TRAIN_SPLIT_ANN_FILE,\n",
    "                 TRAIN_SPLIT_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_SPLIT_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=400)    \n",
    "    print(\"Done.\")\n",
    "\n",
    "if os.path.exists(TRAIN_ALL_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented ALL training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for ALL training data...\")\n",
    "    augment_data(TRAIN_DATA_DIR,\n",
    "                 TRAIN_DATA_ANN_FILE,\n",
    "                 TRAIN_ALL_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_ALL_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=500)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train/patches exists. Skipping.\n",
      "data/val/patches exists. Skipping.\n",
      "data/test/patches exists. Skipping.\n",
      "data/train-all/patches exists. Skipping.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# SPLIT train patches\n",
    "if os.path.exists(TRAIN_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT training data patches...\")\n",
    "    generate_patch_files(TRAIN_SPLIT_AUGMENTED_DATA_DIR, TRAIN_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "    \n",
    "# SPLIT val patches\n",
    "if os.path.exists(VAL_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(VAL_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT validation data patches...\")\n",
    "    generate_patch_files(VAL_SPLIT_DATA_DIR, VAL_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT validation patch data annotations...\")\n",
    "    image_df = pd.read_csv(VAL_SPLIT_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(VAL_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "# test patches\n",
    "if os.path.exists(TEST_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TEST_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating test data patches...\")\n",
    "    generate_patch_files(TEST_DATA_DIR, TEST_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    \n",
    "# ALL train patches\n",
    "if os.path.exists(TRAIN_ALL_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_ALL_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating ALL train data patches...\")\n",
    "    generate_patch_files(TRAIN_ALL_AUGMENTED_DATA_DIR, TRAIN_ALL_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating ALL training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_ALL_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_ALL_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoybeanDataGroup():\n",
    "    def __init__(self, class_weights, \n",
    "                 train_generator,\n",
    "                 val_generator=None,\n",
    "                 test_generator=None,\n",
    "                 train_patch_ann_df=None,\n",
    "                 val_patch_ann_df=None,\n",
    "                 train_whole_image_ann_df=None,\n",
    "                 val_whole_image_ann_df=None):\n",
    "        self.class_weights = class_weights\n",
    "        self.train_generator = train_generator\n",
    "        self.val_generator = val_generator\n",
    "        self.test_generator = test_generator\n",
    "        self.train_patch_ann_df = train_patch_ann_df\n",
    "        self.val_patch_ann_df = val_patch_ann_df\n",
    "        self.train_whole_image_ann_df = train_whole_image_ann_df\n",
    "        self.val_whole_image_ann_df = val_whole_image_ann_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split_group(class_weights=None):\n",
    "    \n",
    "    train_whole_image_ann_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_whole_image_ann_df   = pd.read_csv(VAL_SPLIT_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    train_ann_df = pd.read_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_ann_df   = pd.read_csv(VAL_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(train_ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "        \n",
    "        \n",
    "    data_gen_args = dict(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True,\n",
    "        #rotation_range=10,\n",
    "        #width_shift_range=0.2,\n",
    "        #height_shift_range=0.2,\n",
    "        #horizontal_flip=True\n",
    "    )\n",
    "    train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    val_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "    print(\"Defining train data generator...\")\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_ann_df,\n",
    "            directory=TRAIN_SPLIT_PATCHES_DATA_DIR,\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"annotation\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='sparse',\n",
    "            target_size=(224,224)\n",
    "    )\n",
    "    print(\"Defining validation data generator...\")\n",
    "    val_generator = val_datagen.flow_from_dataframe(\n",
    "            dataframe=val_ann_df,\n",
    "            directory=VAL_SPLIT_PATCHES_DATA_DIR,\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"annotation\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='sparse',\n",
    "            target_size=(224,224)\n",
    "    )\n",
    "        \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory=TEST_PATCHES_DATA_DIR,\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        target_size=(224,224)\n",
    "    )\n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, \n",
    "                            train_generator, \n",
    "                            val_generator, \n",
    "                            test_generator,\n",
    "                            train_ann_df,\n",
    "                            val_ann_df,\n",
    "                            train_whole_image_ann_df, \n",
    "                            val_whole_image_ann_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def all_train_data_group(class_weights=None):\n",
    "    \n",
    "    train_whole_image_ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})    \n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "        \n",
    "        \n",
    "    data_gen_args = dict(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True,\n",
    "    )\n",
    "    train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "    print(\"Defining train data generator...\")\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=ann_df,\n",
    "            directory=TRAIN_DATA_PATCHES_DIR,\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"annotation\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='sparse',\n",
    "            target_size=(224,224)\n",
    "    )\n",
    "   \n",
    "        \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory=TEST_DATA_PATCHES_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        target_size=(224,224)\n",
    "    )  \n",
    "    \n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, \n",
    "                            train_generator, \n",
    "                            None, \n",
    "                            test_generator,\n",
    "                            ann_df,\n",
    "                            None,\n",
    "                            train_whole_image_ann_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This model is based on the VGG16 network with custom classifier layers \n",
    "with the feature layers initialized with weights based on the ImageNet data. \n",
    "\n",
    "The number of neurons and dropout rates in the classifier layers are parameterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_model(n1, n2, dropout):\n",
    "    vgg_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "    vgg_model.trainable = False\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        vgg_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(n1, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(n2, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(5)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "For training and validation, this trains a model across a configured number of epochs and outputs the training and validation loss and accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(df, y_col):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping class labels to 'balanced' weights based on the\n",
    "    frequency of the weights across the labels in the specified dataframe\n",
    "    \"\"\"\n",
    "    y = df[[y_col]].to_numpy().flatten()\n",
    "    weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "    return {label: weight for label, weight in enumerate(weights)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# group = train_val_split_group()\n",
    "\n",
    "\n",
    "# train_whole_image_ann_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "# val_whole_image_ann_df   = pd.read_csv(VAL_SPLIT_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "# print(\"Reading annotations...\")\n",
    "# train_ann_df = pd.read_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "# val_ann_df   = pd.read_csv(VAL_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "\n",
    "# print(\"Computing class weights...\")\n",
    "# class_weights = compute_class_weights(train_ann_df, 'annotation')\n",
    "\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# print(\"Defining data generators...\")\n",
    "# data_gen_args = dict(\n",
    "#     rescale=1./255,\n",
    "#     featurewise_center=False,\n",
    "#     featurewise_std_normalization=False,\n",
    "#     samplewise_center=True,\n",
    "#     samplewise_std_normalization=True,\n",
    "#     #rotation_range=10,\n",
    "#     #width_shift_range=0.2,\n",
    "#     #height_shift_range=0.2,\n",
    "#     #horizontal_flip=True\n",
    "# )\n",
    "# train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "# val_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "# print(\"Defining train data generator...\")\n",
    "# train_generator = train_datagen.flow_from_dataframe(\n",
    "#         dataframe=train_patches_df,\n",
    "#         directory=train_dir_patches,\n",
    "#         x_col=\"file_name\",\n",
    "#         y_col=\"annotation\",\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='sparse',\n",
    "#         target_size=(224,224)\n",
    "# )\n",
    "# print(\"Defining validation data generator...\")\n",
    "# val_generator = val_datagen.flow_from_dataframe(\n",
    "#         dataframe=val_patches_df,\n",
    "#         directory=train_dir_patches,\n",
    "#         x_col=\"file_name\",\n",
    "#         y_col=\"annotation\",\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='sparse',\n",
    "#         target_size=(224,224)\n",
    "# )\n",
    "\n",
    "# # Model\n",
    "# print(\"Defining model...\")\n",
    "# model = get_model()\n",
    "# model.summary()\n",
    "\n",
    "# #model.load_weights('model_vgg16.h5')\n",
    "\n",
    "# print('Fitting model...')\n",
    "\n",
    "# history = model.fit(train_generator, \n",
    "#                     steps_per_epoch=int(train_patches_df.shape[0] / batch_size), \n",
    "#                     epochs=60, \n",
    "#                     validation_data=val_generator, \n",
    "#                     class_weight=class_weights, \n",
    "#                     verbose=1, \n",
    "#                     validation_steps=int(val_patches_df.shape[0] / batch_size))\n",
    "\n",
    "# print('Evaluating model...')\n",
    "# plt.plot(history.history['accuracy'], label='accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.ylim([0, 1])\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "# test_result = model.evaluate(val_generator, verbose=1)\n",
    "# print('Test result:', test_result)\n",
    "\n",
    "# # Save the model weights\n",
    "# model.save_weights('model_vgg16_c2_tf.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def train(run_id, model, group, num_epochs):     \n",
    "    model.summary()\n",
    "\n",
    "    print('Fitting model...')\n",
    "    \n",
    "    print('group.train_patch_ann_df.shape[0]=', group.train_patch_ann_df.shape[0])\n",
    "    print('int(group.train_patch_ann_df.shape[0] / BATCH_SIZE)', int(group.train_patch_ann_df.shape[0] / BATCH_SIZE))\n",
    "\n",
    "    history = model.fit(group.train_generator, \n",
    "                        steps_per_epoch=int(group.train_patch_ann_df.shape[0] / BATCH_SIZE), \n",
    "                        epochs=num_epochs,  \n",
    "                        class_weight=group.class_weights,\n",
    "                        validation_data=group.val_generator,\n",
    "                        validation_steps=int(group.val_patch_ann_df.shape[0] / BATCH_SIZE),\n",
    "                        verbose=1)\n",
    "\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def get_all_labels(loader):\n",
    "    all_labels = torch.tensor([], dtype=torch.long)\n",
    "    for batch in loader:\n",
    "        _, _, labels = batch\n",
    "        all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "    return all_labels\n",
    "\n",
    "\n",
    "def get_all_whole_image_filenames(loader):\n",
    "    return loader.dataset.whole_image_data['file_name'].to_numpy()\n",
    "    \n",
    "\n",
    "def get_all_whole_image_labels(loader):\n",
    "    return loader.dataset.whole_image_data['annotation'].to_numpy(dtype=int)\n",
    "\n",
    "\n",
    "def get_all_whole_image_predictions(patch_preds):\n",
    "    patch_pred_groups = np.split(patch_preds, int(len(patch_preds)/(PATCH_ROWS * PATCH_COLUMNS)))\n",
    "    image_preds = np.array(list(map(lambda x: stats.mode(x).mode[0], patch_pred_groups)))\n",
    "    return image_preds\n",
    "\n",
    "\n",
    "def plot_metrics(run_id, output_dir, model, history, train_dataloader, val_dataloader=None):\n",
    "    \n",
    "    print()\n",
    "    print('Metrics')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Training confusion matrix\n",
    "#     train_patch_labels = get_all_labels(train_generator)\n",
    "#     train_patch_predictions = predict(model, train_generator)\n",
    "    \n",
    "#     print(\"Training Confusion Matrix of Patches\")\n",
    "#     print(\"-\" * 30)\n",
    "#     print_confusion_matrix(train_patch_labels, train_patch_predictions)\n",
    "    \n",
    "#     # Generate prediction label results file\n",
    "#     write_patch_predictions(run_id, 'train', output_dir, train_generator, train_patch_predictions)\n",
    "    \n",
    "    \n",
    "#     print(\"Training Confusion Matrix of Whole Images\")\n",
    "#     print(\"-\" * 30)\n",
    "#     train_whole_image_filenames = get_all_whole_image_filenames(train_generator)\n",
    "#     train_whole_image_labels = get_all_whole_image_labels(train_generator)\n",
    "#     train_whole_image_predictions = get_all_whole_image_predictions(train_patch_predictions)\n",
    "#     print_confusion_matrix(train_whole_image_labels, train_whole_image_predictions)\n",
    "    \n",
    "#     write_whole_image_predictions(run_id, 'train', output_dir, \n",
    "#                                   train_whole_image_filenames, \n",
    "#                                   train_whole_image_labels, \n",
    "#                                   train_whole_image_predictions)\n",
    "    \n",
    "#     # Validation confusion matrix\n",
    "#     if val_generator is not None:\n",
    "#         val_patch_labels = get_all_labels(val_dataloader)\n",
    "#         val_patch_predictions = predict(model, val_dataloader)\n",
    "        \n",
    "#         print(\"Validation Confusion Matrix of Patches\")\n",
    "#         print(\"-\" * 30)\n",
    "#         print_confusion_matrix(val_patch_labels, val_patch_predictions)\n",
    "        \n",
    "#         print(\"Validation Confusion Matrix of Whole Images\")\n",
    "#         print(\"-\" * 30)\n",
    "#         val_whole_image_filenames = get_all_whole_image_filenames(val_dataloader)\n",
    "#         val_whole_image_labels = get_all_whole_image_labels(val_dataloader)\n",
    "#         val_whole_image_predictions = get_all_whole_image_predictions(val_patch_predictions)\n",
    "#         print_confusion_matrix(val_whole_image_labels, val_whole_image_predictions)\n",
    "        \n",
    "#         # Generate prediction label results file\n",
    "#         write_patch_predictions(run_id, 'val', output_dir, val_dataloader, val_patch_predictions)\n",
    "        \n",
    "#         write_whole_image_predictions(run_id, 'val', output_dir, \n",
    "#                                   val_whole_image_filenames, \n",
    "#                                   val_whole_image_labels, \n",
    "#                                   val_whole_image_predictions)\n",
    "    \n",
    "\n",
    "\n",
    "def train_and_test(run_id, model, group, num_epochs, output_dir):    \n",
    "    model_trained, history = train(run_id, model, group, num_epochs)\n",
    "    \n",
    "    # Save weights\n",
    "    model_trained.save_weights(os.path.join(output_dir, \"{}_weights.h5\".format(run_id)))\n",
    "    \n",
    "    # Plot history metrics\n",
    "    plot_metrics(run_id, output_dir, model_trained, history, group.train_generator, group.val_generator)\n",
    "    \n",
    "    # Classify test data\n",
    "    return predict(model_trained, group.test_generator)\n",
    "\n",
    "\n",
    "def predict(model, data_generator):\n",
    "    y_hat_logits = model.predict(data_generator)\n",
    "    y_hat = tf.map_fn(lambda x: tf.argmax(x), y_hat_logits, dtype=tf.int64)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "\n",
    "def predict_whole_images(patch_predictions, rows, columns, csvfile):\n",
    "    y_hat_test = patch_predictions\n",
    "    y_hat_patch_groups = np.split(y_hat_test, int(len(y_hat_test)/(rows * columns)))\n",
    "    y_hat_whole_images = list(map(lambda x: stats.mode(x).mode[0], y_hat_patch_groups))\n",
    "\n",
    "    for k, v in sorted(Counter(y_hat_whole_images).items()): \n",
    "        print(str(k) + ': '+ str(v))    \n",
    "\n",
    "    one_hots = [np.zeros((5,1)) for pred in y_hat_whole_images]\n",
    "    for i in range(len(one_hots)):\n",
    "        pred = y_hat_whole_images[i]  # the index of the one-hot encoding\n",
    "        one_hots[i][pred] = 1\n",
    "    with open(csvfile, 'w') as predictions_file:\n",
    "        writer = csv.writer(predictions_file)\n",
    "        for pred in one_hots:\n",
    "            pred = np.array(pred, dtype=int)\n",
    "            writer.writerow(pred.T.tolist()[0])\n",
    "    print('Finished generating predictions to', csvfile)\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y, y_hat):\n",
    "    confusion_matrix = np.zeros((5, 5))\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ground_truth = y==labels[i]\n",
    "            prediction = y_hat==labels[j]\n",
    "            confusion_matrix[i, j] = sum(np.bitwise_and(ground_truth, prediction))\n",
    "    df = pd.DataFrame(confusion_matrix, dtype=int)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def write_patch_predictions(run_id, phase, output_dir, dataloader, patch_predictions):\n",
    "    df = pd.DataFrame(columns=['file_name', 'annotation', 'prediction'])\n",
    "    i = 0\n",
    "    for file_names, _, labels in dataloader:\n",
    "        for j in range(BATCH_SIZE):\n",
    "            if len(file_names) > j:\n",
    "                df = df.append({'file_name': file_names[j], \n",
    "                                'annotation': labels[j].item(), \n",
    "                                'prediction': patch_predictions[i]}, ignore_index=True)\n",
    "                i += 1\n",
    "\n",
    "    df.to_csv(os.path.join(output_dir, \"{}_{}_patch_predictions.csv\".format(run_id, phase)), index=False)\n",
    "    \n",
    "\n",
    "def write_whole_image_predictions(run_id, phase, output_dir, filenames, labels, predictions):\n",
    "    print('filenames:', len(filenames))\n",
    "    print('labels:', len(labels))\n",
    "    print('predictions:', len(predictions))\n",
    "    \n",
    "    df = pd.DataFrame(columns=['file_name', 'annotation', 'prediction'])\n",
    "    for i in range(len(filenames)):\n",
    "        df = df.append({'file_name': filenames[i], \n",
    "                        'annotation': labels[i], \n",
    "                        'prediction': predictions[i]}, ignore_index=True)\n",
    "        \n",
    "    df.to_csv(os.path.join(output_dir, \"{}_{}_whole_image_predictions.csv\".format(run_id, phase)), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "The following hyperparameters can be tuned:\n",
    "1. `n1` - Number of neurons in the first classifier dense layer\n",
    "2. `n2` - Number of neurons in the second classifier dense layer\n",
    "3. `d` - Dropout rate after classifier dense layers\n",
    "4. class weights - `[1,1,1,1,1]` (default) or `[1,1,5,5,1]`\n",
    "5. batch normalization - `no` or `yes`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(name, model, class_weights=None, num_epochs=40):\n",
    "    run_id = shortuuid.uuid()\n",
    "    \n",
    "    # output directory\n",
    "    output_dir = os.path.join(\"output_tf\", run_id)\n",
    "    os.makedirs(output_dir)\n",
    "    print(\"Output generated to:\", output_dir)\n",
    "    \n",
    "    \n",
    "    y_hat_test = train_and_test(run_id, model, train_val_split_group(class_weights), num_epochs, output_dir)\n",
    "    predictions_file = os.path.join(output_dir, \"{}_predict_c2_{}_test.csv\".format(run_id, name))\n",
    "    print('predictions file:', predictions_file)\n",
    "    predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)\n",
    "    return y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1: 1024-128-5\n",
    "\n",
    "* DNN Structure: 1024-128-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output generated to: output_tf/cteBAzU28nipzJxVWHwmqx\n",
      "Reading annotations...\n",
      "Computing class weights...\n",
      "{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Defining train data generator...\n",
      "Found 18000 validated image filenames belonging to 5 classes.\n",
      "Defining validation data generator...\n",
      "Found 2295 validated image filenames belonging to 5 classes.\n",
      "Found 1800 images belonging to 1 classes.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, None, None, 512)   14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 15,371,845\n",
      "Trainable params: 657,157\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Fitting model...\n",
      "group.train_patch_ann_df.shape[0]= 18000\n",
      "int(group.train_patch_ann_df.shape[0] / BATCH_SIZE) 562\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 562 steps, validate for 71 steps\n",
      "Epoch 1/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.3404 - accuracy: 0.8663 - val_loss: 0.5820 - val_accuracy: 0.7989\n",
      "Epoch 2/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.3308 - accuracy: 0.8664 - val_loss: 0.5549 - val_accuracy: 0.8024\n",
      "Epoch 3/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.3338 - accuracy: 0.8636 - val_loss: 0.5954 - val_accuracy: 0.8024\n",
      "Epoch 4/20\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.3298 - accuracy: 0.8697 - val_loss: 0.6252 - val_accuracy: 0.8002\n",
      "Epoch 5/20\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.3270 - accuracy: 0.8677 - val_loss: 0.6565 - val_accuracy: 0.7848\n",
      "Epoch 6/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.3201 - accuracy: 0.8716 - val_loss: 0.6337 - val_accuracy: 0.7852\n",
      "Epoch 7/20\n",
      "562/562 [==============================] - 40s 71ms/step - loss: 0.3245 - accuracy: 0.8711 - val_loss: 0.6339 - val_accuracy: 0.7931\n",
      "Epoch 8/20\n",
      "562/562 [==============================] - 39s 70ms/step - loss: 0.3165 - accuracy: 0.8756 - val_loss: 0.6361 - val_accuracy: 0.7984\n",
      "Epoch 9/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.3160 - accuracy: 0.8737 - val_loss: 0.6222 - val_accuracy: 0.7936\n",
      "Epoch 10/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.2990 - accuracy: 0.8798 - val_loss: 0.6248 - val_accuracy: 0.7892\n",
      "Epoch 11/20\n",
      "562/562 [==============================] - 40s 72ms/step - loss: 0.3036 - accuracy: 0.8801 - val_loss: 0.6029 - val_accuracy: 0.7975\n",
      "Epoch 12/20\n",
      "562/562 [==============================] - 41s 72ms/step - loss: 0.3031 - accuracy: 0.8810 - val_loss: 0.6734 - val_accuracy: 0.7861\n",
      "Epoch 13/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.2878 - accuracy: 0.8864 - val_loss: 0.6343 - val_accuracy: 0.7958\n",
      "Epoch 14/20\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.2962 - accuracy: 0.8804 - val_loss: 0.6731 - val_accuracy: 0.7914\n",
      "Epoch 15/20\n",
      "562/562 [==============================] - 40s 71ms/step - loss: 0.2985 - accuracy: 0.8850 - val_loss: 0.6816 - val_accuracy: 0.7958\n",
      "Epoch 16/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.2862 - accuracy: 0.8871 - val_loss: 0.6607 - val_accuracy: 0.7804\n",
      "Epoch 17/20\n",
      "562/562 [==============================] - 39s 70ms/step - loss: 0.2936 - accuracy: 0.8842 - val_loss: 0.6045 - val_accuracy: 0.7958\n",
      "Epoch 18/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.2841 - accuracy: 0.8893 - val_loss: 0.6012 - val_accuracy: 0.8055\n",
      "Epoch 19/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.2841 - accuracy: 0.8902 - val_loss: 0.6348 - val_accuracy: 0.8015\n",
      "Epoch 20/20\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.2743 - accuracy: 0.8940 - val_loss: 0.6339 - val_accuracy: 0.7918\n",
      "\n",
      "Metrics\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAbjklEQVR4nO3dfXRc9X3n8fdX0kijZ8lYyA9SbJMagskCIcJNSELYTUqA7AIpuwGfZMszJ93CJmk3WfaQQ1Oac3KA07SHlkDJlgayJUDbpPVuISSbpvFmFxILYhPMo3FsLD/KsmRJSCPNw3f/uFfyeDSyx8h3Rtb9vM6ZM/fhNzNfXY3mo9/v3rnX3B0REYmvqkoXICIilaUgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmIssCMzsYTPbb2YvzbLezOw+M9tqZi+a2XlR1SIiIrOLskfwbeCSo6y/FFgd3m4BHoiwFhERmUVkQeDuG4CDR2lyBfCoB54D2sxsaVT1iIhIcTUVfO3lwM68+b5w2Z7ChmZ2C0GvgcbGxve/5z3vKUuBIiILxfPPP3/A3TuKratkEJTM3R8CHgLo6enx3t7eClckInJyMbMds62r5FFDu4DuvPmucJmIiJRRJYNgPfA74dFDHwAOufuMYSEREYlWZENDZvZd4CJgsZn1AX8IJADc/UHgKeAyYCswBlwfVS0iIjK7yILA3dcdY70DvxfV64uISGn0zWIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYiDQIzu8TMXjOzrWZ2e5H17zKzn5jZL83sRTO7LMp6RERkpsiCwMyqgfuBS4E1wDozW1PQ7CvAk+7+PuAa4JtR1SMiIsVF2SNYC2x1923uPgk8DlxR0MaBlnC6FdgdYT0iIlJElEGwHNiZN98XLsv3VeCzZtYHPAXcVuyJzOwWM+s1s97+/v4oahURia1K7yxeB3zb3buAy4DvmNmMmtz9IXfvcfeejo6OshcpIrKQRRkEu4DuvPmucFm+G4EnAdz9WSAJLI6wJhERKRBlEGwEVpvZKjOrJdgZvL6gzVvAxwDM7EyCINDYj4hIGUUWBO6eAW4FngFeITg6aIuZ3WVml4fN/gC42cw2A98FrnN3j6omERGZqSbKJ3f3pwh2AucvuzNv+mXgQ1HWICIiR1fpncUiIlJhCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYizQIzOwSM3vNzLaa2e2ztPm0mb1sZlvM7LEo6xERkZlqonpiM6sG7gd+C+gDNprZend/Oa/NauC/AR9y90EzOzWqekREpLgoewRrga3uvs3dJ4HHgSsK2twM3O/ugwDuvj/CekREpIgog2A5sDNvvi9clu904HQz+79m9pyZXVLsiczsFjPrNbPe/v7+iMoVEYmnSu8srgFWAxcB64BvmVlbYSN3f8jde9y9p6Ojo8wliogsbFEGwS6gO2++K1yWrw9Y7+5pd/818DpBMIiISJlEGQQbgdVmtsrMaoFrgPUFbf6BoDeAmS0mGCraFmFNIiJSILIgcPcMcCvwDPAK8KS7bzGzu8zs8rDZM8CAmb0M/AT4krsPRFWTiIjMZO5e6RqOS09Pj/f29la6DBGRk4qZPe/uPcXWVXpnsYiIVJiCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYKykIzKzRzKrC6dPN7HIzS0RbmoiIlEOpPYINQNLMlgM/BP4j8O2oihIRkfIpNQjM3ceA3wa+6e7/ATgrurJERKRcSg4CM/sg8Bngn8Jl1dGUJCIi5VRqEHyB4NrC3w/PIHoawdlCRUTkJFfSxevd/afATwHCncYH3P0/R1mYiIiUR6lHDT1mZi1m1gi8BLxsZl+KtjQRESmHUoeG1rj7MHAl8DSwiuDIIREROcmVGgSJ8HsDVxJeYxg4ua5oIyIiRZUaBH8JbAcagQ1mtgIYjqooEREpn1J3Ft8H3Je3aIeZ/etoShIRkXIqdWdxq5l9w8x6w9ufEPQORETkJFfq0NDDwAjw6fA2DPx1VEWJiEj5lDQ0BLzb3a/Km/8jM9sURUEiIlJepfYIxs3sw1MzZvYhYDyakkREpJxK7RF8DnjUzFrD+UHg2mhKEhGRcir1qKHNwDlm1hLOD5vZF4AXoyxORESid1xXKHP34fAbxgC/H0E9IiJSZnO5VKWdsCpERKRi5hIEOsWEiMgCcNR9BGY2QvEPfAPqI6lIRETK6qhB4O7N5SpEREQqYy5DQyIisgAoCEREYk5BICIScwoCEZGYUxCIiMRcpEFgZpeY2WtmttXMbj9Ku6vMzM2sJ8p6RERkpsiCwMyqgfuBS4E1wDozW1OkXTPweeDnUdUiIiKzi7JHsBbY6u7b3H0SeBy4oki7PwbuBlIR1iIiIrOIMgiWAzvz5vvCZdPM7Dyg293/6WhPZGa3TF0ms7+//8RXKiISYxXbWWxmVcA3gD84Vlt3f8jde9y9p6OjI/riRERiJMog2AV05813hcumNAPvBf7FzLYDHwDWa4exiEh5RRkEG4HVZrbKzGqBa4D1Uyvd/ZC7L3b3le6+EngOuNzdeyOsSURECkQWBO6eAW4FngFeAZ509y1mdpeZXR7V64qIyPEp9ZrF74i7PwU8VbDszlnaXhRlLSIiUpy+WSwiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjEXmyBIZ3O4e6XLEBGZdyI9fHQ++R/P7eDuH7xKV3sD71rUQHd7Pd2LGuheFM4vaqCpLjabQ0RkWmw++c5a1spnf3MFOwfHeOvgOL/49UFGJzJHtFnUWDszINob6F5Uz7K2ehLVhztQk5kc4+ks45NZxtNZxiYzpNJZxiYPLzu8LksqnWUym6OptobmZA1NyQRNdeF0XQ1NyWC6uS5BMlGFmZV7E4lITMUmCNauWsTaVYum592dobF0GAxj7Dw4zlsHx+gbHONXuw7xg5f2kskdHkqqsiAoJtJBAOSvK4UZJKqqmMzmjtm2usqCcAiDYiosGutqqE9UU19bTTIR3OoT1SQTVdPL62qC+/zlU20b66ppqI3Nr1xEShTbTwUzo72xlvbGWs7uapuxPptz9g6neGtgjJ2DY+w8OMaB0UmSiSoawg/a+trgg7kh/GBuqD38ITx1P7Wurib4L38yk2N0IsNoKsPIRJrRVCaYn8gwnMqE8+lwfYaRcNmB0Um2D4yRSge9jFQ6Syp97FAp1FBbTUdzHYub6uhoqqOjue7w/PR0LYub6kgmqkt+XndnPJ1lJJVhJJVmOHW49pFUmpFUholMlsVNdXS2JlnammRJS5LW+oR6PyIVFtsgOJbqKmN5Wz3L2+r5IKecsOetraliUU0tixpr5/xcuZwzEQ5RTQXE+GSWiUyW8ckjl6fSWUYnMhwYmeTA6AT9IxNs7R/luV8PMDSWLvr8LcmaI0KitT7B2GT2iA/6kVSa0TCwssfZSwJIJqpY0pKksyUIh87WJEtbkixpnVpWT0dzHdVVCguRqCgITmJVVRb0PGpL/8+9mIlMloHRwwExdTswOkF/uGzL7mEOjadprKumuS5Bc7KG5W1JmpPN08NXzcnE4fu6wmU11NZU0T8ywd5DKfYOp4L7vOneHYPsG06Rzh4ZKFUGpzYn6Wiuo6baqDKjyoJeXZURzhs2PU04f3i6usqoranivctb6VnRzpplLUfs8xGJMwWBUFdTzbK2YId41LraG+hqb5h1fS7nHBybZO+hFPuGU+zJuz8wOkE257hDzj28QTaXIze9LBimyrmTywXLptqPTmT4/i+DS2LUJ6o5p7uV81cu4v0r2jlvRTstyUTkP7/IfKQgkHmlqspY3BQMR713eesJf/6g53GQ3u2DPL9jkG/+y5tkc44ZnNHZTM/KdnpWBOHQ1V6v/RcSC3ayfcmqp6fHe3t17Ro5Md6eyLBp5xC92wfp3XGQX741NH1Y8ZKWJO9f2U7PiiAczlzaTM6Z3vk9nEozPD61czx/OsPweHifSjM8HrSfet78YS0zw+CI4a78Ia78+UR1FWd0NnFOdxtnd7Vx1rKW49qhL/FmZs+7e9ErQCoIRPJkc86re4d5fsfgdK9h19A4EBxAcKwd4mbQXFdDS32ClnD/yNR0U101ZjY9rBUMWR0eysqfd44c6nKHsckMW3YPs39kAoCaKuM9S5s5u6uNc7vaOLu7ldWnNp/QHevpbI59wyl2DwX7csYnM6TSOSYyWSbSOVLh/UQmRyqdZSITrJtuk8lNT2dzzrsWNXB6ZzNndDazurOJ1Z3N+iJnmSgIROZg99A4vTsGeXXPMPWJalrqww/4ZOLwdH2ClmQNjbU1VEV8hNPeQyk29w2xeecQL/YdYnPfECOpoLfRUFvNe5e1ck53axAQ3W2zDnG5O4fG0+waGmf3UIo9h8anp3cPjbN7aJx9wymOln1VxvTh0YX3dVP3NdXTX5LcMfA2r+8bOeLQ5672es7obOb0Jc2c3tnE6Z3NvLuj6bh7O+7OcCrD/uEgtPYNT7BvONjHtPdQioG3J8MvjQZfEu1ub6ArvG+MQRgpCEQWsFzO2T7wdhgOQTBs2T3MZCb4sG1vSHBOdxtnLGlmeDzNrrwP+rHJ7BHPVVtdxdK2JMtag4MHlrclpw8kWNKapLGu5ogP+3dy5FUu5+wcHOO1vSO8vm+E1/eN8vq+Ed7sH50+YqzKYOXiRk4/NQiIMzqbWbW4kbcnM9MHEuwLP+z3DqfYH06Pp7MzXq+1PsGSliTtjQkGRifpGxyf0W7qrAJdixroaq8PwyI4Fc2ytvoFMQSnIBCJmclMjtf3jRzRc3hj/yjtDYnggz38oF/WlmR5Wz1Lw+nFjXWR92hmk87m2H7gbV7bN8Lrew8HxPaBt4v2Supqqqa/b9LZkmRJSx2dLUlObUmG300J5gs/xN2dgbcn2XlwjJ2D4+w8OEbf4Dh94RdHdw2NzziEubOljuVt9dTVVDPVuTIDw8jvbE3t8zm8nuneWJUZp7bUTfdIpk5h09ZQni9VKghEBHc/KY+CSqWzbN0/yvaBt2mtTwQf/M1JWuprIvl5sjln/0iKnQfHw7AIgmL30Djp8BQx7uAwfUZjz1s21WBqWbDeyWSdfcMpBgu+wNlcV0NX2PuYOgHmVFB0tTecsN6IgkBEZJ4YSaWDkAl7IFM9k6lznRWeOqajuW76jMmfPr+bC969+B297tGCYOHvIRERmUeakwnWLEuwZlnLjHXuTv/oxOHeSNgjeevgGBu3D3LRGadGUpOCQERknjAzTm1OcmpzkvevaC/b6+pkKyIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzEUaBGZ2iZm9ZmZbzez2Iut/38xeNrMXzezHZrYiynpERGSmyILAzKqB+4FLgTXAOjNbU9Dsl0CPu58N/B1wT1T1iIhIcVH2CNYCW919m7tPAo8DV+Q3cPefuPtYOPsc0BVhPSIiUkSUQbAc2Jk33xcum82NwNPFVpjZLWbWa2a9/f39J7BEERGZFzuLzeyzQA9wb7H17v6Qu/e4e09HR0d5ixMRWeCivDDNLqA7b74rXHYEM/s4cAfwUXefiLAeEREpIsoewUZgtZmtMrNa4BpgfX4DM3sf8JfA5e6+P8JaRERkFpEFgbtngFuBZ4BXgCfdfYuZ3WVml4fN7gWagL81s01mtn6WpxMRkYhEes1id38KeKpg2Z150x+P8vVFROTY5sXOYhERqRwFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYivR6BiMh8lU6n6evrI5VKVbqUEyqZTNLV1UUikSj5MQoCEYmlvr4+mpubWblyJWZW6XJOCHdnYGCAvr4+Vq1aVfLjNDQkIrGUSqU45ZRTFkwIAJgZp5xyynH3chQEIhJbCykEpryTn0lBICIScwoCEZEKaWpqqnQJgIJARCT2dNSQiMTeH/3PLby8e/iEPueaZS384b87q6S27s6Xv/xlnn76acyMr3zlK1x99dXs2bOHq6++muHhYTKZDA888AAXXHABN954I729vZgZN9xwA1/84hfnVKuCQESkwr73ve+xadMmNm/ezIEDBzj//PO58MILeeyxx/jEJz7BHXfcQTabZWxsjE2bNrFr1y5eeuklAIaGhub8+goCEYm9Uv9zj8rPfvYz1q1bR3V1NZ2dnXz0ox9l48aNnH/++dxwww2k02muvPJKzj33XE477TS2bdvGbbfdxic/+UkuvvjiOb++9hGIiMxTF154IRs2bGD58uVcd911PProo7S3t7N582YuuugiHnzwQW666aY5v46CQESkwj7ykY/wxBNPkM1m6e/vZ8OGDaxdu5YdO3bQ2dnJzTffzE033cQLL7zAgQMHyOVyXHXVVXzta1/jhRdemPPra2hIRKTCPvWpT/Hss89yzjnnYGbcc889LFmyhEceeYR7772XRCJBU1MTjz76KLt27eL6668nl8sB8PWvf33Or2/uPucnKaeenh7v7e2tdBkicpJ75ZVXOPPMMytdRiSK/Wxm9ry79xRrr6EhEZGYUxCIiMScgkBEYutkGxovxTv5mRQEIhJLyWSSgYGBBRUGU9cjSCaTx/U4HTUkIrHU1dVFX18f/f39lS7lhJq6QtnxUBCISCwlEonjuorXQhbp0JCZXWJmr5nZVjO7vcj6OjN7Ilz/czNbGWU9IiIyU2RBYGbVwP3ApcAaYJ2ZrSlodiMw6O6/AfwpcHdU9YiISHFR9gjWAlvdfZu7TwKPA1cUtLkCeCSc/jvgY7YQrx0nIjKPRbmPYDmwM2++D/jN2dq4e8bMDgGnAAfyG5nZLcAt4eyomb32DmtaXPjc84zqmxvVN3fzvUbV986tmG3FSbGz2N0fAh6a6/OYWe9sX7GeD1Tf3Ki+uZvvNaq+aEQ5NLQL6M6b7wqXFW1jZjVAKzAQYU0iIlIgyiDYCKw2s1VmVgtcA6wvaLMeuDac/vfAP/tC+naHiMhJILKhoXDM/1bgGaAaeNjdt5jZXUCvu68H/gr4jpltBQ4ShEWU5jy8FDHVNzeqb+7me42qLwIn3WmoRUTkxNK5hkREYk5BICIScwsyCObzqS3MrNvMfmJmL5vZFjP7fJE2F5nZITPbFN7uLFd94etvN7Nfha8943JwFrgv3H4vmtl5ZaztjLztssnMhs3sCwVtyr79zOxhM9tvZi/lLVtkZj8yszfC+/ZZHntt2OYNM7u2WJsIarvXzF4Nf3/fN7O2WR571PdCxDV+1cx25f0eL5vlsUf9e4+wvifyattuZptmeWxZtuGcuPuCuhHsmH4TOA2oBTYDawra/CfgwXD6GuCJMta3FDgvnG4GXi9S30XA/6rgNtwOLD7K+suApwEDPgD8vIK/673AikpvP+BC4Dzgpbxl9wC3h9O3A3cXedwiYFt43x5Ot5ehtouBmnD67mK1lfJeiLjGrwL/pYT3wFH/3qOqr2D9nwB3VnIbzuW2EHsE8/rUFu6+x91fCKdHgFcIvmF9MrkCeNQDzwFtZra0AnV8DHjT3XdU4LWP4O4bCI58y5f/PnsEuLLIQz8B/MjdD7r7IPAj4JKoa3P3H7p7Jpx9juB7PhUzy/YrRSl/73N2tPrCz45PA9890a9bLgsxCIqd2qLwg/aIU1sAU6e2KKtwSOp9wM+LrP6gmW02s6fN7KyyFgYO/NDMng9P71GolG1cDtcw+x9fJbfflE533xNO7wU6i7SZD9vyBoIeXjHHei9E7dZw+OrhWYbW5sP2+wiwz93fmGV9pbfhMS3EIDgpmFkT8PfAF9x9uGD1CwTDHecAfw78Q5nL+7C7n0dw5tjfM7MLy/z6xxR+SfFy4G+LrK709pvBgzGCeXestpndAWSAv5mlSSXfCw8A7wbOBfYQDL/MR+s4em9g3v89LcQgmPentjCzBEEI/I27f69wvbsPu/toOP0UkDCzxeWqz913hff7ge8TdL/zlbKNo3Yp8IK77ytcUentl2ff1JBZeL+/SJuKbUszuw74t8BnwqCaoYT3QmTcfZ+7Z909B3xrlteu6Hsx/Pz4beCJ2dpUchuWaiEGwbw+tUU4nvhXwCvu/o1Z2iyZ2mdhZmsJfk9lCSozazSz5qlpgp2KLxU0Ww/8Tnj00AeAQ3lDIOUy639hldx+BfLfZ9cC/1ikzTPAxWbWHg59XBwui5SZXQJ8Gbjc3cdmaVPKeyHKGvP3O31qltcu5e89Sh8HXnX3vmIrK70NS1bpvdVR3AiOanmd4GiCO8JldxG86QGSBEMKW4FfAKeVsbYPEwwRvAhsCm+XAZ8DPhe2uRXYQnAExHPABWWs77TwdTeHNUxtv/z6jOCiQ28CvwJ6yvz7bST4YG/NW1bR7UcQSnuANME49Y0E+51+DLwB/G9gUdi2B/jveY+9IXwvbgWuL1NtWwnG1qfeg1NH0S0Dnjrae6GM2+874fvrRYIP96WFNYbzM/7ey1FfuPzbU++7vLYV2YZzuekUEyIiMbcQh4ZEROQ4KAhERGJOQSAiEnMKAhGRmFMQiIjEnIJApICZZQvOcHrCzmhpZivzz2ApMh9EdqlKkZPYuLufW+kiRMpFPQKREoXnlb8nPLf8L8zsN8LlK83sn8OTo/3YzN4VLu8Mz/W/ObxdED5VtZl9y4LrUfzQzOor9kOJoCAQKaa+YGjo6rx1h9z9XwF/AfxZuOzPgUfc/WyCk7fdFy6/D/ipBye/O4/gm6UAq4H73f0sYAi4KuKfR+So9M1ikQJmNuruTUWWbwf+jbtvC08cuNfdTzGzAwSnP0iHy/e4+2Iz6we63H0i7zlWElx/YHU4/1+BhLt/LfqfTKQ49QhEjo/PMn08JvKms2hfnVSYgkDk+Fydd/9sOP3/CM56CfAZ4P+E0z8GfhfAzKrNrLVcRYocD/0nIjJTfcGFyH/g7lOHkLab2YsE/9WvC5fdBvy1mX0J6AeuD5d/HnjIzG4k+M//dwnOYCkyr2gfgUiJwn0EPe5+oNK1iJxIGhoSEYk59QhERGJOPQIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYm5/w9jxdYAUg3G1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions file: output_tf/cteBAzU28nipzJxVWHwmqx/cteBAzU28nipzJxVWHwmqx_predict_c2_h1_test.csv\n",
      "0: 19\n",
      "1: 126\n",
      "4: 55\n",
      "Finished generating predictions to output_tf/cteBAzU28nipzJxVWHwmqx/cteBAzU28nipzJxVWHwmqx_predict_c2_h1_test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1800,), dtype=int64, numpy=array([1, 1, 1, ..., 1, 1, 1])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = get_model(1024, 128, 0.5)\n",
    "run_trial(\"h1\", model, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2: 2048-256-5\n",
    "\n",
    "* DNN Structure: 2048-256-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output generated to: output_tf/bBqfcKBHBCBaS2iQAA4WQZ\n",
      "Reading annotations...\n",
      "Computing class weights...\n",
      "{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Defining train data generator...\n",
      "Found 18000 validated image filenames belonging to 5 classes.\n",
      "Defining validation data generator...\n",
      "Found 2295 validated image filenames belonging to 5 classes.\n",
      "Found 1800 images belonging to 1 classes.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, None, None, 512)   14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2048)              1050624   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 16,291,141\n",
      "Trainable params: 1,576,453\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Fitting model...\n",
      "group.train_patch_ann_df.shape[0]= 18000\n",
      "int(group.train_patch_ann_df.shape[0] / BATCH_SIZE) 562\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 562 steps, validate for 71 steps\n",
      "Epoch 1/40\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.9850 - accuracy: 0.5833 - val_loss: 0.7899 - val_accuracy: 0.6818\n",
      "Epoch 2/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.8026 - accuracy: 0.6705 - val_loss: 0.7206 - val_accuracy: 0.7108\n",
      "Epoch 3/40\n",
      "562/562 [==============================] - 39s 70ms/step - loss: 0.7457 - accuracy: 0.6952 - val_loss: 0.7389 - val_accuracy: 0.7016\n",
      "Epoch 4/40\n",
      "562/562 [==============================] - 37s 66ms/step - loss: 0.7019 - accuracy: 0.7158 - val_loss: 0.6651 - val_accuracy: 0.7364\n",
      "Epoch 5/40\n",
      "562/562 [==============================] - 37s 67ms/step - loss: 0.6731 - accuracy: 0.7278 - val_loss: 0.6960 - val_accuracy: 0.7254\n",
      "Epoch 6/40\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.6436 - accuracy: 0.7413 - val_loss: 0.6534 - val_accuracy: 0.7403\n",
      "Epoch 7/40\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.6225 - accuracy: 0.7443 - val_loss: 0.6502 - val_accuracy: 0.7443\n",
      "Epoch 8/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.6036 - accuracy: 0.7560 - val_loss: 0.6112 - val_accuracy: 0.7500\n",
      "Epoch 9/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.5857 - accuracy: 0.7666 - val_loss: 0.6500 - val_accuracy: 0.7328\n",
      "Epoch 10/40\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.5740 - accuracy: 0.7682 - val_loss: 0.6168 - val_accuracy: 0.7452\n",
      "Epoch 11/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.5531 - accuracy: 0.7722 - val_loss: 0.6072 - val_accuracy: 0.7535\n",
      "Epoch 12/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.5413 - accuracy: 0.7807 - val_loss: 0.5841 - val_accuracy: 0.7676\n",
      "Epoch 13/40\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.5271 - accuracy: 0.7877 - val_loss: 0.5983 - val_accuracy: 0.7584\n",
      "Epoch 14/40\n",
      "562/562 [==============================] - 39s 70ms/step - loss: 0.5062 - accuracy: 0.7934 - val_loss: 0.5964 - val_accuracy: 0.7557\n",
      "Epoch 15/40\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.4951 - accuracy: 0.7988 - val_loss: 0.6030 - val_accuracy: 0.7579\n",
      "Epoch 16/40\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.4861 - accuracy: 0.8059 - val_loss: 0.6057 - val_accuracy: 0.7606\n",
      "Epoch 17/40\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.4739 - accuracy: 0.8069 - val_loss: 0.5629 - val_accuracy: 0.7658\n",
      "Epoch 18/40\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.4664 - accuracy: 0.8092 - val_loss: 0.5744 - val_accuracy: 0.7716\n",
      "Epoch 19/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.4604 - accuracy: 0.8107 - val_loss: 0.5978 - val_accuracy: 0.7724\n",
      "Epoch 20/40\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.4473 - accuracy: 0.8220 - val_loss: 0.5397 - val_accuracy: 0.7839\n",
      "Epoch 21/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.4342 - accuracy: 0.8241 - val_loss: 0.5603 - val_accuracy: 0.7720\n",
      "Epoch 22/40\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.4247 - accuracy: 0.8330 - val_loss: 0.5732 - val_accuracy: 0.7790\n",
      "Epoch 23/40\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.4247 - accuracy: 0.8271 - val_loss: 0.6028 - val_accuracy: 0.7729\n",
      "Epoch 24/40\n",
      "562/562 [==============================] - 40s 71ms/step - loss: 0.4058 - accuracy: 0.8376 - val_loss: 0.5728 - val_accuracy: 0.7790\n",
      "Epoch 25/40\n",
      "562/562 [==============================] - 39s 70ms/step - loss: 0.4120 - accuracy: 0.8349 - val_loss: 0.5861 - val_accuracy: 0.7645\n",
      "Epoch 26/40\n",
      "562/562 [==============================] - 40s 71ms/step - loss: 0.3951 - accuracy: 0.8414 - val_loss: 0.6088 - val_accuracy: 0.7733\n",
      "Epoch 27/40\n",
      "562/562 [==============================] - 40s 72ms/step - loss: 0.3933 - accuracy: 0.8442 - val_loss: 0.5636 - val_accuracy: 0.7852\n",
      "Epoch 28/40\n",
      "562/562 [==============================] - 40s 72ms/step - loss: 0.3843 - accuracy: 0.8454 - val_loss: 0.5314 - val_accuracy: 0.7865\n",
      "Epoch 29/40\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.3847 - accuracy: 0.8475 - val_loss: 0.5687 - val_accuracy: 0.7870\n",
      "Epoch 30/40\n",
      "562/562 [==============================] - 40s 71ms/step - loss: 0.3744 - accuracy: 0.8544 - val_loss: 0.5846 - val_accuracy: 0.7821\n",
      "Epoch 31/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.3617 - accuracy: 0.8555 - val_loss: 0.5996 - val_accuracy: 0.7724\n",
      "Epoch 32/40\n",
      "562/562 [==============================] - 37s 66ms/step - loss: 0.3627 - accuracy: 0.8560 - val_loss: 0.6006 - val_accuracy: 0.7751\n",
      "Epoch 33/40\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.3639 - accuracy: 0.8546 - val_loss: 0.5819 - val_accuracy: 0.7742\n",
      "Epoch 34/40\n",
      "562/562 [==============================] - 39s 70ms/step - loss: 0.3521 - accuracy: 0.8589 - val_loss: 0.5684 - val_accuracy: 0.7887\n",
      "Epoch 35/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.3499 - accuracy: 0.8625 - val_loss: 0.6092 - val_accuracy: 0.7777\n",
      "Epoch 36/40\n",
      "562/562 [==============================] - 37s 67ms/step - loss: 0.3491 - accuracy: 0.8621 - val_loss: 0.5972 - val_accuracy: 0.7773\n",
      "Epoch 37/40\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.3391 - accuracy: 0.8655 - val_loss: 0.5958 - val_accuracy: 0.7843\n",
      "Epoch 38/40\n",
      "562/562 [==============================] - 39s 69ms/step - loss: 0.3295 - accuracy: 0.8694 - val_loss: 0.5634 - val_accuracy: 0.7918\n",
      "Epoch 39/40\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.3269 - accuracy: 0.8700 - val_loss: 0.5879 - val_accuracy: 0.7826\n",
      "Epoch 40/40\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.3208 - accuracy: 0.8728 - val_loss: 0.5741 - val_accuracy: 0.7901\n",
      "\n",
      "Metrics\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions file: output_tf/bBqfcKBHBCBaS2iQAA4WQZ/bBqfcKBHBCBaS2iQAA4WQZ_predict_c2_h2_test.csv\n",
      "0: 27\n",
      "1: 118\n",
      "4: 55\n",
      "Finished generating predictions to output_tf/bBqfcKBHBCBaS2iQAA4WQZ/bBqfcKBHBCBaS2iQAA4WQZ_predict_c2_h2_test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1800,), dtype=int64, numpy=array([1, 1, 1, ..., 1, 0, 1])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(2048, 256, 0.5)\n",
    "run_trial(\"h2\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H3: 512-64-5\n",
    "\n",
    "* DNN Structure: 512-64-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(512, 64, 0.5)\n",
    "run_trial(\"h3\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H4: Best from above, dropout 0.25\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.25\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(2048, 256, 0.25)\n",
    "run_trial(\"h4\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5: Best from above, dropout 0.1\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.1\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(2048, 256, 0.1)\n",
    "run_trial(\"h5\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H6: Best from above, skewed class weights\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,5,5,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.5)\n",
    "run_trial(\"h6\", model, class_weights=[1., 1., 2., 2., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H7: Best from above, batch normalization\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: \n",
    "* Batch normalization: yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.5, batch_normalization=True).to(device)\n",
    "run_trial(\"h7\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_n1 = 1024\n",
    "optimal_n2 = 128\n",
    "optimal_d = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with all C2 data and optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(optimal_n1, optimal_n2, optimal_d)\n",
    "# model.load_state_dict(torch.load('cnn_pytorch_c2.pt'))\n",
    "y_hat_test = train_and_test(model, group_3(), num_epochs=40)\n",
    "predictions_file = \"predict_c2_{}.csv\".format(shortuuid.uuid())\n",
    "print('predictions file:', predictions_file)\n",
    "predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
