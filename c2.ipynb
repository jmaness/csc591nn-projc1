{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "import shortuuid\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from torchvision import models, transforms\n",
    "\n",
    "def ann_file(data_dir):\n",
    "    return os.path.join(data_dir, \"TrainAnnotations.csv\")\n",
    "\n",
    "\n",
    "TRAIN_DATA_DIR = \"data-sorted/TrainData-C2\"\n",
    "TRAIN_DATA_ANN_FILE = ann_file(TRAIN_DATA_DIR)\n",
    "\n",
    "TRAIN_SPLIT_DATA_DIR           = \"data/train/split\"\n",
    "TRAIN_SPLIT_ANN_FILE           = ann_file(TRAIN_SPLIT_DATA_DIR)\n",
    "TRAIN_SPLIT_AUGMENTED_DATA_DIR = \"data/train/augmented\"\n",
    "TRAIN_SPLIT_AUGMENTED_ANN_FILE = ann_file(TRAIN_SPLIT_AUGMENTED_DATA_DIR)\n",
    "TRAIN_SPLIT_PATCHES_DATA_DIR   = \"data/train/patches\"\n",
    "TRAIN_SPLIT_PATCHES_ANN_FILE   = ann_file(TRAIN_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TRAIN_ALL_AUGMENTED_DATA_DIR   = \"data/train-all/augmented\"\n",
    "TRAIN_ALL_AUGMENTED_ANN_FILE   = ann_file(TRAIN_ALL_AUGMENTED_DATA_DIR)\n",
    "TRAIN_ALL_PATCHES_DATA_DIR     = \"data/train-all/patches\"\n",
    "TRAIN_ALL_PATCHES_ANN_FILE     = ann_file(TRAIN_ALL_PATCHES_DATA_DIR)\n",
    "\n",
    "VAL_SPLIT_DATA_DIR         = \"data/val/split\"\n",
    "VAL_SPLIT_ANN_FILE         = ann_file(VAL_SPLIT_DATA_DIR)\n",
    "VAL_SPLIT_PATCHES_DATA_DIR = \"data/val/patches\"\n",
    "VAL_SPLIT_PATCHES_ANN_FILE = ann_file(VAL_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TEST_DATA_DIR         = \"data/TestData/\"\n",
    "TEST_PATCHES_DATA_DIR = \"data/test/patches\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Generate random, stratified 80/20 split for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(TRAIN_SPLIT_DATA_DIR) or os.path.exists(VAL_SPLIT_DATA_DIR)):\n",
    "    print(\"Data directories for splits already exist. Skipping\")\n",
    "else:\n",
    "    # Generate 80/20 split\n",
    "\n",
    "    print(\"Reading {} annotations...\".format(TRAIN_DATA_ANN_FILE))\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "    print(\"Splitting data into training and validation sets...\")\n",
    "    train_df, val_df = train_test_split(ann_df,\n",
    "                                        train_size=0.80,\n",
    "                                        random_state=138,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=ann_df[['annotation']].to_numpy(dtype=np.int32).flatten())\n",
    "\n",
    "    os.makedirs(TRAIN_SPLIT_DATA_DIR)\n",
    "    os.makedirs(VAL_SPLIT_DATA_DIR)\n",
    "    \n",
    "    print(\"Copying files for training split...\")\n",
    "    for _, row in train_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(TRAIN_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating training split annotations...\")\n",
    "    train_df.sort_values('file_name').to_csv(TRAIN_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Copying files for validation split...\")\n",
    "    for _, row in val_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(VAL_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating validation split annotations...\")\n",
    "    val_df.sort_values('file_name').to_csv(VAL_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data\n",
    "Because the training dataset is unbalanced, augment the training data set by generating\n",
    "new images for the lower numbered samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DESIRED_CLASS_SAMPLE_COUNT = 400\n",
    "RANDOM_STATE = 13\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "PATCH_ROWS = 5\n",
    "PATCH_COLUMNS = 5\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.endswith(IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def augment_data(src_dir, src_ann_file, dest_dir, dest_ann_file, class_sample_count=500):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "    ann_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'}) \n",
    "    new_samples = {}\n",
    "    \n",
    "    for i in range(5):\n",
    "        class_df = ann_df.query(\"annotation == '{}'\".format(i))\n",
    "        num_class_samples = class_df.shape[0]\n",
    "        num_to_create = class_sample_count - num_class_samples\n",
    "            \n",
    "        print(\"Creating {} images for class {}\".format(num_to_create, i))\n",
    "        samples = class_df.sample(n=num_to_create, replace=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "        for idx, row in samples.iterrows():\n",
    "            new_filename = row['file_name'].split('.')[0] + \"_\" + shortuuid.uuid() + \".png\"\n",
    "    \n",
    "            # Apply transformations to each randomly selected sample\n",
    "            img = Image.open(src_dir + \"/\" + row['file_name'])\n",
    "            image_transforms = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=20, translate=(0.2, 0.2)),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                transforms.RandomResizedCrop((480, 640), scale=(1.0, 1.2)),\n",
    "            ])\n",
    "            transformed_img = image_transforms(img)\n",
    "            transformed_img.save(os.path.join(dest_dir, new_filename))\n",
    "    \n",
    "            new_samples[new_filename] = row['annotation']\n",
    "    \n",
    "    # Add to new dataframe\n",
    "    balanced_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    balanced_df = balanced_df.append(pd.DataFrame.from_records([(k, v) for k, v in new_samples.items()],\n",
    "                                                 columns=['file_name', 'annotation']))\n",
    "    \n",
    "    # Write new annotations\n",
    "    balanced_df.sort_values('file_name').to_csv(dest_ann_file, index=False)\n",
    "    \n",
    "    # Copy images from training data split\n",
    "    for file in glob.glob(src_dir + \"/*\"):\n",
    "        if is_image_file(file):\n",
    "            shutil.copy(file, os.path.join(dest_dir, os.path.basename(file)))\n",
    "\n",
    "\n",
    "def generate_image_patches(img, rows, cols):\n",
    "    \"\"\"\n",
    "    Generates a list of in-memory image overlapping patches\n",
    "    \n",
    "    Args:\n",
    "        rows - number of rows of patchs to cover the height of the image\n",
    "        cols - number of colums of patches to cover the width of the image\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    sizeX = img.shape[1]\n",
    "    sizeY = img.shape[0]\n",
    "    \n",
    "    patch_sizeX = 224\n",
    "    patch_sizeY = 224\n",
    "    patch_relative_centerX = 112\n",
    "    patch_relative_centerY = 112\n",
    "\n",
    "    for i in range(0,rows):\n",
    "        for j in range(0, cols):\n",
    "            center = (patch_relative_centerX + (sizeX - patch_sizeX)/(rows - 1)*i, \n",
    "                      patch_relative_centerY + (sizeY - patch_sizeY)/(cols - 1)*j)\n",
    "            patches.append(cv2.getRectSubPix(img, (patch_sizeX, patch_sizeY), center))\n",
    "            \n",
    "    return patches\n",
    "\n",
    "\n",
    "def generate_patch_files(in_dir, out_dir, rows, cols):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    images = [f for f in os.listdir(in_dir) if os.path.isfile(os.path.join(in_dir, f)) and is_image_file(f)]   \n",
    "    for im in images:\n",
    "        img = cv2.imread(os.path.join(in_dir, im))\n",
    "        patches = generate_image_patches(img, rows, cols)\n",
    "        \n",
    "        for i in range(0,rows):\n",
    "            for j in range(0, cols):\n",
    "                patch = patches[i*rows + j]\n",
    "                patch_name = im.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                cv2.imwrite(out_dir + '/' + patch_name, patch)\n",
    "\n",
    "\n",
    "def generate_patch_annotations_df(df, rows, cols):\n",
    "    patches_ann = {}\n",
    "    \n",
    "    for ind in df.index: \n",
    "        file_name = df['file_name'][ind]\n",
    "        annotation = df['annotation'][ind]\n",
    "        \n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                patch_name = file_name.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                patches_ann[patch_name] = annotation\n",
    "    \n",
    "    return pd.DataFrame.from_records([(k, v) for k, v in patches_ann.items()], \n",
    "                                     columns=['file_name', 'annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run data augmentation\n",
    "\n",
    "Perform the data augmentation on the training data set split to balance the class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(TRAIN_SPLIT_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented SPLIT training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for SPLIT training data...\")\n",
    "    augment_data(TRAIN_SPLIT_DATA_DIR,\n",
    "                 TRAIN_SPLIT_ANN_FILE,\n",
    "                 TRAIN_SPLIT_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_SPLIT_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=400)    \n",
    "    print(\"Done.\")\n",
    "\n",
    "if os.path.exists(TRAIN_ALL_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented ALL training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for ALL training data...\")\n",
    "    augment_data(TRAIN_DATA_DIR,\n",
    "                 TRAIN_DATA_ANN_FILE,\n",
    "                 TRAIN_ALL_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_ALL_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=500)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT train patches\n",
    "if os.path.exists(TRAIN_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT training data patches...\")\n",
    "    generate_patch_files(TRAIN_SPLIT_AUGMENTED_DATA_DIR, TRAIN_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "    \n",
    "# SPLIT val patches\n",
    "if os.path.exists(VAL_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(VAL_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT validation data patches...\")\n",
    "    generate_patch_files(VAL_SPLIT_DATA_DIR, VAL_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT validation patch data annotations...\")\n",
    "    image_df = pd.read_csv(VAL_SPLIT_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(VAL_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "# test patches\n",
    "if os.path.exists(TEST_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TEST_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating test data patches...\")\n",
    "    generate_patch_files(TEST_DATA_DIR, TEST_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    \n",
    "# ALL train patches\n",
    "if os.path.exists(TRAIN_ALL_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_ALL_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating ALL train data patches...\")\n",
    "    generate_patch_files(TRAIN_ALL_AUGMENTED_DATA_DIR, TRAIN_ALL_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating ALL training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_ALL_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_ALL_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Given a directory of images and a CSV file of annotations, this defines a PyTorch Dataset which will load an image from disk and apply all configure transformations and return a tuple containing the image and label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SoybeanDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, data_path, ann_df, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): path to images\n",
    "            ann_df (string): pandas data frame containing file names and annotations\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.data = ann_df\n",
    "        self.labels = np.asarray(self.data.iloc[:, 1])\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print('index:', index)\n",
    "        image_label = int(self.labels[index])\n",
    "        img_path = os.path.join(self.data_path, self.data.file_name[index])\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Transform image\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "        # Return image and the label\n",
    "        return img, image_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "class SoybeanTestDatasetFolder(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, data_path, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): path to images\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.images = []\n",
    "        \n",
    "        for root, _, fnames in sorted(os.walk(data_path, followlinks=True)):\n",
    "            for fname in sorted(fnames):\n",
    "                path = fname\n",
    "                if is_image_file(path):\n",
    "                    self.images.append(path)\n",
    "\n",
    "                                       \n",
    "    def image_gen(self):\n",
    "        for i in self.images:\n",
    "            img_path = os.path.join(self.data_path, i)\n",
    "            img = Image.open(img_path)\n",
    "        \n",
    "            # Transform image\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "                \n",
    "            yield img\n",
    "            \n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.image_gen())\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "\n",
    "\n",
    "class SoybeanDataGroup():\n",
    "    def __init__(self, class_weights, \n",
    "                 train_dataloader,\n",
    "                 val_dataloader=None,\n",
    "                 test_dataloader=None):\n",
    "        self.class_weights = class_weights\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "\n",
    "def compute_class_weights(df, y_col):\n",
    "    \"\"\"\n",
    "    Returns a list of class labels to 'balanced' weights based on the\n",
    "    frequency of the weights across the labels in the specified dataframe\n",
    "    \"\"\"\n",
    "    y = df[[y_col]].to_numpy(dtype=np.int32).flatten()\n",
    "    weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "    return torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common image transformations\n",
    "These images transformations will apply to both train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class SamplewiseCenterNormalize(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.div(torch.add(tensor, torch.mul(torch.mean(tensor), -1)), torch.std(tensor) + 1e-6)\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "\n",
    "DATA_TRANSFORMS = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomAffine(degrees=20, translate=(0.2, 0.2)),\n",
    "    transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), shear=10, scale=(1.0, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    #transforms.RandomCrop(size=(480,640)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    # SamplewiseCenterNormalize()\n",
    "\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "TEST_DATA_TRANSFORMS = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    # SamplewiseCenterNormalize()\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 0.80/Val 0.20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_val_split_group():\n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    train_ann_df = pd.read_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_ann_df   = pd.read_csv(VAL_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    print(\"Computing class weights...\")\n",
    "    class_weights = compute_class_weights(train_ann_df, 'annotation')\n",
    "    \n",
    "        \n",
    "    train_dataset = SoybeanDataset(TRAIN_SPLIT_PATCHES_DATA_DIR, train_ann_df, transforms=DATA_TRANSFORMS)\n",
    "    val_dataset = SoybeanDataset(VAL_SPLIT_PATCHES_DATA_DIR, val_ann_df, transforms=TEST_DATA_TRANSFORMS)\n",
    "    test_dataset  = SoybeanTestDatasetFolder(TEST_PATCHES_DATA_DIR, transforms=TEST_DATA_TRANSFORMS)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   pin_memory=True, \n",
    "                                                   num_workers=16)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 pin_memory=True,\n",
    "                                                 num_workers=16)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  pin_memory=True,\n",
    "                                                  num_workers=0)\n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, train_dataloader, val_dataloader, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 100%\n",
    "\n",
    "Train with all the data in the `TrainData-C2` dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def all_train_data_group():\n",
    "    print(\"Reading annotations...\")\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    print(\"Computing class weights...\")\n",
    "    class_weights = compute_class_weights(ann_df, 'annotation')\n",
    "    print(class_weights)\n",
    "\n",
    "    train_dataset = SoybeanDataset(TRAIN_DATA_PATCHES_DIR, ann_df, transforms=DATA_TRANSFORMS)\n",
    "    test_dataset  = SoybeanTestDatasetFolder(TEST_DATA_PATCHES_DIR, transforms=TEST_DATA_TRANSFORMS)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   pin_memory=True, \n",
    "                                                   num_workers=16)\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  pin_memory=True,\n",
    "                                                  num_workers=0)\n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, train_dataloader, None, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This model is based on the VGG16 network with custom classifier layers \n",
    "with the feature layers initialized with weights based on the ImageNet data. \n",
    "\n",
    "The number of neurons and dropout rates in the classifier layers are parameterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_model(n1, d1, n2, d2):\n",
    "    model = models.vgg16(pretrained=True)\n",
    "\n",
    "    # Freeze training for all layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "    \n",
    "    # Replace the VGG16 classifier with a custom classifier for soybean wilting \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(512 * 1 * 1, n1, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=d1),\n",
    "        nn.Linear(n1, n2, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=d2),\n",
    "        nn.Linear(n2, 5, bias=True)\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "For training and validation, this trains a model across a configured number of epochs and outputs the training and validation loss and accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Trains the specified neural network model\n",
    "    \n",
    "    Args:\n",
    "        model:         - neural network model to train\n",
    "        criterion:     - loss function\n",
    "        optimizer:     - gradient descent optimization algorithm\n",
    "        dataloaders:   - dict of DataLoaders for training and validation data\n",
    "        num_epochs:    - number of epochs to train model\n",
    "    Returns:\n",
    "        model   - trained model with weights from the epoch with the best validation accuracy\n",
    "        history - dict of training and validation loss and accuracy for all epochs\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    # summary(model, input_size=(3, 224, 224))\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    history = {'train': {'loss': [], 'acc': []}}\n",
    "    phases = ['train']\n",
    "    if ('val' in dataloaders and dataloaders['val'] is not None):\n",
    "        phases += ['val']\n",
    "        history['val'] = {'loss': [], 'acc': []}\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and optionally, a validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                \n",
    "            phase_start = time.time()\n",
    "\n",
    "            sample_count = 0\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:               \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                        nn.utils.clip_grad_value_(model.parameters(), 0.5)\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                sample_count += inputs.size(0)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += preds.eq(labels.data.view_as(preds)).cpu().sum()\n",
    "            \n",
    "            print('Num samples', sample_count)\n",
    "            \n",
    "            epoch_loss = running_loss / sample_count\n",
    "            epoch_acc = running_corrects.double() / sample_count\n",
    "            \n",
    "            history[phase]['loss'].append(epoch_loss)\n",
    "            history[phase]['acc'].append(epoch_acc)\n",
    "            \n",
    "            phase_end = time.time()\n",
    "            phase_elapsed = phase_end - phase_start\n",
    "\n",
    "            print('{} {} loss: {:.4f} accuracy: {:.4f}'.format(\n",
    "                phase, str(timedelta(seconds=phase_elapsed)), epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if ('val' not in phases or phase == 'val') and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        epoch_elapsed = epoch_end - epoch_start\n",
    "        print('Elapsed time: {}'.format(str(timedelta(seconds=epoch_elapsed))))\n",
    "        \n",
    "        print()\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def get_sample_count(dataset, sampler):\n",
    "    if (sampler is not None):\n",
    "        return len(sampler)\n",
    "    elif (dataset is not None):\n",
    "        return len(dataset)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def train(model, group, num_epochs=20):\n",
    "    criterion = nn.CrossEntropyLoss(weight=group.class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, eps=1e-07)\n",
    "    \n",
    "    dataloaders = {\n",
    "        'train': group.train_dataloader,\n",
    "        'val': group.val_dataloader\n",
    "    }\n",
    " \n",
    "    model_trained, history = train_model(model, criterion, optimizer, dataloaders, num_epochs)\n",
    "    \n",
    "    \n",
    "    return model_trained, history\n",
    "\n",
    "\n",
    "def get_all_labels(loader):\n",
    "    all_labels = torch.tensor([], dtype=torch.long)\n",
    "    for batch in loader:\n",
    "        _, labels = batch\n",
    "        all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "    return all_labels\n",
    "\n",
    "\n",
    "def plot_metrics(model, history, train_dataloader, val_dataloader=None):\n",
    "    \n",
    "    print()\n",
    "    print('Metrics')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(history['train']['loss']) + 1)\n",
    "\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, history['train']['loss'], 'g-')\n",
    "    loss_legend = ['Training Loss']\n",
    "    \n",
    "    if ('val' in history and history['val'] is not None):\n",
    "        plt.plot(epoch_count, history['val']['loss'], 'b-')\n",
    "        loss_legend += ['Validation Loss']\n",
    "        \n",
    "    plt.legend(loss_legend)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize accuracy history\n",
    "    plt.plot(epoch_count, history['train']['acc'], 'g-')\n",
    "    acc_legend = ['Training Accuracy']\n",
    "    \n",
    "    if ('val' in history and history['val'] is not None):\n",
    "        plt.plot(epoch_count, history['val']['acc'], 'b-')\n",
    "        acc_legend += ['Validation Accuracy']\n",
    "    \n",
    "    plt.legend(acc_legend)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    # Training confusion matrix\n",
    "    print(\"Training Confusion Matrix\")\n",
    "    print(\"-\" * 30)\n",
    "    print_confusion_matrix(get_all_labels(train_dataloader).cpu().numpy(),\n",
    "                           predict(model, train_dataloader).cpu().numpy())\n",
    "    \n",
    "    # Validation confusion matrix\n",
    "    if val_dataloader is not None:\n",
    "        print(\"Validation Confusion Matrix\")\n",
    "        print(\"-\" * 30)\n",
    "        print_confusion_matrix(get_all_labels(val_dataloader).cpu().numpy(),\n",
    "                               predict(model, val_dataloader).cpu().numpy())\n",
    "\n",
    "\n",
    "def train_and_test(model, group, num_epochs=60):\n",
    "    model_trained, history = train(model, group, num_epochs)\n",
    "    \n",
    "    # Plot history metrics\n",
    "    plot_metrics(model_trained, history, group.train_dataloader, group.val_dataloader)\n",
    "    \n",
    "    # Classify test data\n",
    "    return predict(model_trained, group.test_dataloader)\n",
    "\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    predictions = torch.tensor([], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if (type(data) is list):\n",
    "                images = data[0].to(device)\n",
    "            else:\n",
    "                images = data.to(device)\n",
    "            model.eval()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions = torch.cat((predictions, predicted))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_whole_images(patch_predictions, rows, columns, csvfile):\n",
    "    y_hat_test = patch_predictions.cpu().numpy()\n",
    "    y_hat_patch_groups = np.split(y_hat_test, int(len(y_hat_test)/(rows * columns)))\n",
    "    y_hat_whole_images = list(map(lambda x: stats.mode(x).mode[0], y_hat_patch_groups))\n",
    "\n",
    "    for k, v in sorted(Counter(y_hat_whole_images).items()): \n",
    "        print(str(k) + ': '+ str(v))    \n",
    "\n",
    "    one_hots = [np.zeros((5,1)) for pred in y_hat_whole_images]\n",
    "    for i in range(len(one_hots)):\n",
    "        pred = y_hat_whole_images[i]  # the index of the one-hot encoding\n",
    "        one_hots[i][pred] = 1\n",
    "    with open(csvfile, 'w') as predictions_file:\n",
    "        writer = csv.writer(predictions_file)\n",
    "        for pred in one_hots:\n",
    "            pred = np.array(pred, dtype=int)\n",
    "            writer.writerow(pred.T.tolist()[0])\n",
    "    print('Finished generating predictions to', csvfile)\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y, y_hat):\n",
    "    confusion_matrix = np.zeros((5, 5))\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ground_truth = y==labels[i]\n",
    "            prediction = y_hat==labels[j]\n",
    "            confusion_matrix[i, j] = sum(np.bitwise_and(ground_truth, prediction))\n",
    "    df = pd.DataFrame(confusion_matrix, dtype=int)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test - Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model(1024, 0.5, 128, 0.5).to(device)\n",
    "y_hat_test = train_and_test(model, train_val_split_group(), num_epochs=1)\n",
    "predictions_file = \"predict_c2_g1_\" + shortuuid.uuid()\n",
    "print('predictions file:', predictions_file)\n",
    "predict_whole_images(y_hat_test, 5, 5, predictions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test - Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(1024, 0.1, 128, 0.1).to(device)\n",
    "y_hat_test = train_and_test(model, group_2())\n",
    "predict_whole_images(y_hat_test, 5, 5, 'predictions_c2_g2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test - Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(1024, 0.1, 128, 0.1).to(device)\n",
    "y_hat_test = train_and_test(model, group_3())\n",
    "predict_whole_images(y_hat_test, 5, 5, 'predictions_c2_g3.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train and Test - Group 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(1024, 0.5, 128, 0.5).to(device)\n",
    "y_hat_test = train_and_test(model, group_4(), num_epochs=1)\n",
    "y_hat_test.bincount()\n",
    "y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "The following hyperparameters can be tuned:\n",
    "1. `n1` - Number of neurons in the first classifier dense layer\n",
    "2. `d1` - Dropout rate after first classifier dense layer\n",
    "3. `n2` - Number of neurons in the second classifier dense layer\n",
    "4. `d2` - Dropout rate after the second classifier dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_n1 = 1024\n",
    "optimal_d1 = 0.1\n",
    "optimal_n2 = 128\n",
    "optimal_d2 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with all C2 data and optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(optimal_n1, optimal_d1, optimal_n2, optimal_d2).to(device)\n",
    "# model.load_state_dict(torch.load('cnn_pytorch_c2.pt'))\n",
    "y_hat_test = train_and_test(model, group_3(), num_epochs=40)\n",
    "predictions_file = \"predict_c2_\" + shortuuid.uuid()\n",
    "print('predictions file:', predictions_file)\n",
    "predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "print(y_hat_test)\n",
    "torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn_pytorch_c2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
