{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "import shortuuid\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from torchvision import models, transforms\n",
    "\n",
    "def ann_file(data_dir):\n",
    "    return os.path.join(data_dir, \"TrainAnnotations.csv\")\n",
    "\n",
    "\n",
    "TRAIN_DATA_DIR = \"data/TrainData-C2\"\n",
    "TRAIN_DATA_ANN_FILE = ann_file(TRAIN_DATA_DIR)\n",
    "\n",
    "TRAIN_SPLIT_DATA_DIR           = \"data/train/split\"\n",
    "TRAIN_SPLIT_ANN_FILE           = ann_file(TRAIN_SPLIT_DATA_DIR)\n",
    "TRAIN_SPLIT_AUGMENTED_DATA_DIR = \"data/train/augmented\"\n",
    "TRAIN_SPLIT_AUGMENTED_ANN_FILE = ann_file(TRAIN_SPLIT_AUGMENTED_DATA_DIR)\n",
    "TRAIN_SPLIT_PATCHES_DATA_DIR   = \"data/train/patches\"\n",
    "TRAIN_SPLIT_PATCHES_ANN_FILE   = ann_file(TRAIN_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TRAIN_ALL_AUGMENTED_DATA_DIR   = \"data/train-all/augmented\"\n",
    "TRAIN_ALL_AUGMENTED_ANN_FILE   = ann_file(TRAIN_ALL_AUGMENTED_DATA_DIR)\n",
    "TRAIN_ALL_PATCHES_DATA_DIR     = \"data/train-all/patches\"\n",
    "TRAIN_ALL_PATCHES_ANN_FILE     = ann_file(TRAIN_ALL_PATCHES_DATA_DIR)\n",
    "\n",
    "VAL_SPLIT_DATA_DIR         = \"data/val/split\"\n",
    "VAL_SPLIT_ANN_FILE         = ann_file(VAL_SPLIT_DATA_DIR)\n",
    "VAL_SPLIT_PATCHES_DATA_DIR = \"data/val/patches\"\n",
    "VAL_SPLIT_PATCHES_ANN_FILE = ann_file(VAL_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TEST_DATA_DIR         = \"data/TestData/\"\n",
    "TEST_PATCHES_DATA_DIR = \"data/test/patches\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Generate random, stratified 80/20 split for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories for splits already exist. Skipping\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists(TRAIN_SPLIT_DATA_DIR) or os.path.exists(VAL_SPLIT_DATA_DIR)):\n",
    "    print(\"Data directories for splits already exist. Skipping\")\n",
    "else:\n",
    "    # Generate 80/20 split\n",
    "\n",
    "    print(\"Reading {} annotations...\".format(TRAIN_DATA_ANN_FILE))\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "    print(\"Splitting data into training and validation sets...\")\n",
    "    train_df, val_df = train_test_split(ann_df,\n",
    "                                        train_size=0.80,\n",
    "                                        random_state=138,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=ann_df[['annotation']].to_numpy(dtype=np.int32).flatten())\n",
    "\n",
    "    os.makedirs(TRAIN_SPLIT_DATA_DIR)\n",
    "    os.makedirs(VAL_SPLIT_DATA_DIR)\n",
    "    \n",
    "    print(\"Copying files for training split...\")\n",
    "    for _, row in train_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(TRAIN_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating training split annotations...\")\n",
    "    train_df.sort_values('file_name').to_csv(TRAIN_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Copying files for validation split...\")\n",
    "    for _, row in val_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(VAL_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating validation split annotations...\")\n",
    "    val_df.sort_values('file_name').to_csv(VAL_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data\n",
    "Because the training dataset is unbalanced, augment the training data set by generating\n",
    "new images for the lower numbered samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DESIRED_CLASS_SAMPLE_COUNT = 400\n",
    "RANDOM_STATE = 13\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "PATCH_ROWS = 5\n",
    "PATCH_COLUMNS = 5\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.endswith(IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def augment_data(src_dir, src_ann_file, dest_dir, dest_ann_file, class_sample_count=500):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "    ann_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'}) \n",
    "    new_samples = {}\n",
    "    \n",
    "    for i in range(5):\n",
    "        class_df = ann_df.query(\"annotation == '{}'\".format(i))\n",
    "        num_class_samples = class_df.shape[0]\n",
    "        num_to_create = class_sample_count - num_class_samples\n",
    "            \n",
    "        print(\"Creating {} images for class {}\".format(num_to_create, i))\n",
    "        samples = class_df.sample(n=num_to_create, replace=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "        for idx, row in samples.iterrows():\n",
    "            new_filename = row['file_name'].split('.')[0] + \"_\" + shortuuid.uuid() + \".png\"\n",
    "    \n",
    "            # Apply transformations to each randomly selected sample\n",
    "            img = Image.open(src_dir + \"/\" + row['file_name'])\n",
    "            image_transforms = transforms.Compose([\n",
    "                transforms.RandomAffine(degrees=20, translate=(0.2, 0.2)),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                transforms.RandomResizedCrop((480, 640), scale=(1.0, 1.2)),\n",
    "            ])\n",
    "            transformed_img = image_transforms(img)\n",
    "            transformed_img.save(os.path.join(dest_dir, new_filename))\n",
    "    \n",
    "            new_samples[new_filename] = row['annotation']\n",
    "    \n",
    "    # Add to new dataframe\n",
    "    balanced_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    balanced_df = balanced_df.append(pd.DataFrame.from_records([(k, v) for k, v in new_samples.items()],\n",
    "                                                 columns=['file_name', 'annotation']))\n",
    "    \n",
    "    # Write new annotations\n",
    "    balanced_df.sort_values('file_name').to_csv(dest_ann_file, index=False)\n",
    "    \n",
    "    # Copy images from training data split\n",
    "    for file in glob.glob(src_dir + \"/*\"):\n",
    "        if is_image_file(file):\n",
    "            shutil.copy(file, os.path.join(dest_dir, os.path.basename(file)))\n",
    "\n",
    "\n",
    "def generate_image_patches(img, rows, cols):\n",
    "    \"\"\"\n",
    "    Generates a list of in-memory image overlapping patches\n",
    "    \n",
    "    Args:\n",
    "        rows - number of rows of patchs to cover the height of the image\n",
    "        cols - number of colums of patches to cover the width of the image\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    sizeX = img.shape[1]\n",
    "    sizeY = img.shape[0]\n",
    "    \n",
    "    patch_sizeX = 224\n",
    "    patch_sizeY = 224\n",
    "    patch_relative_centerX = 112\n",
    "    patch_relative_centerY = 112\n",
    "\n",
    "    for i in range(0,rows):\n",
    "        for j in range(0, cols):\n",
    "            center = (patch_relative_centerX + (sizeX - patch_sizeX)/(rows - 1)*i, \n",
    "                      patch_relative_centerY + (sizeY - patch_sizeY)/(cols - 1)*j)\n",
    "            patches.append(cv2.getRectSubPix(img, (patch_sizeX, patch_sizeY), center))\n",
    "            \n",
    "    return patches\n",
    "\n",
    "\n",
    "def generate_patch_files(in_dir, out_dir, rows, cols):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    images = [f for f in os.listdir(in_dir) if os.path.isfile(os.path.join(in_dir, f)) and is_image_file(f)]   \n",
    "    for im in images:\n",
    "        img = cv2.imread(os.path.join(in_dir, im))\n",
    "        patches = generate_image_patches(img, rows, cols)\n",
    "        \n",
    "        for i in range(0,rows):\n",
    "            for j in range(0, cols):\n",
    "                patch = patches[i*rows + j]\n",
    "                patch_name = im.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                cv2.imwrite(out_dir + '/' + patch_name, patch)\n",
    "\n",
    "\n",
    "def generate_patch_annotations_df(df, rows, cols):\n",
    "    patches_ann = {}\n",
    "    \n",
    "    for ind in df.index: \n",
    "        file_name = df['file_name'][ind]\n",
    "        annotation = df['annotation'][ind]\n",
    "        \n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                patch_name = file_name.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                patches_ann[patch_name] = annotation\n",
    "    \n",
    "    return pd.DataFrame.from_records([(k, v) for k, v in patches_ann.items()], \n",
    "                                     columns=['file_name', 'annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run data augmentation\n",
    "\n",
    "Perform the data augmentation on the training data set split to balance the class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented SPLIT training data already exists. Skipping.\n",
      "Augmented ALL training data already exists. Skipping.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TRAIN_SPLIT_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented SPLIT training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for SPLIT training data...\")\n",
    "    augment_data(TRAIN_SPLIT_DATA_DIR,\n",
    "                 TRAIN_SPLIT_ANN_FILE,\n",
    "                 TRAIN_SPLIT_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_SPLIT_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=400)    \n",
    "    print(\"Done.\")\n",
    "\n",
    "if os.path.exists(TRAIN_ALL_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented ALL training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for ALL training data...\")\n",
    "    augment_data(TRAIN_DATA_DIR,\n",
    "                 TRAIN_DATA_ANN_FILE,\n",
    "                 TRAIN_ALL_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_ALL_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=500)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train/patches exists. Skipping.\n",
      "data/val/patches exists. Skipping.\n",
      "data/test/patches exists. Skipping.\n",
      "data/train-all/patches exists. Skipping.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# SPLIT train patches\n",
    "if os.path.exists(TRAIN_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT training data patches...\")\n",
    "    generate_patch_files(TRAIN_SPLIT_AUGMENTED_DATA_DIR, TRAIN_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "    \n",
    "# SPLIT val patches\n",
    "if os.path.exists(VAL_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(VAL_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT validation data patches...\")\n",
    "    generate_patch_files(VAL_SPLIT_DATA_DIR, VAL_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT validation patch data annotations...\")\n",
    "    image_df = pd.read_csv(VAL_SPLIT_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(VAL_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "# test patches\n",
    "if os.path.exists(TEST_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TEST_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating test data patches...\")\n",
    "    generate_patch_files(TEST_DATA_DIR, TEST_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    \n",
    "# ALL train patches\n",
    "if os.path.exists(TRAIN_ALL_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_ALL_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating ALL train data patches...\")\n",
    "    generate_patch_files(TRAIN_ALL_AUGMENTED_DATA_DIR, TRAIN_ALL_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating ALL training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_ALL_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_ALL_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Given a directory of images and a CSV file of annotations, this defines a PyTorch Dataset which will load an image from disk and apply all configure transformations and return a tuple containing the image and label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SoybeanDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, data_path, ann_df, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): path to images\n",
    "            ann_df (string): pandas data frame containing file names and annotations\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.data = ann_df\n",
    "        self.labels = np.asarray(self.data.iloc[:, 1])\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print('index:', index)\n",
    "        image_label = int(self.labels[index])\n",
    "        img_path = os.path.join(self.data_path, self.data.file_name[index])\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Transform image\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            \n",
    "        # Return image and the label\n",
    "        return img, image_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "class SoybeanTestDatasetFolder(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, data_path, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): path to images\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.images = []\n",
    "        \n",
    "        for root, _, fnames in sorted(os.walk(data_path, followlinks=True)):\n",
    "            for fname in sorted(fnames):\n",
    "                path = fname\n",
    "                if is_image_file(path):\n",
    "                    self.images.append(path)\n",
    "\n",
    "                                       \n",
    "    def image_gen(self):\n",
    "        for i in self.images:\n",
    "            img_path = os.path.join(self.data_path, i)\n",
    "            img = Image.open(img_path)\n",
    "        \n",
    "            # Transform image\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "                \n",
    "            yield img\n",
    "            \n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.image_gen())\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "\n",
    "\n",
    "class SoybeanDataGroup():\n",
    "    def __init__(self, class_weights, \n",
    "                 train_dataloader,\n",
    "                 val_dataloader=None,\n",
    "                 test_dataloader=None):\n",
    "        self.class_weights = class_weights\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "\n",
    "def compute_class_weights(df, y_col):\n",
    "    \"\"\"\n",
    "    Returns a list of class labels to 'balanced' weights based on the\n",
    "    frequency of the weights across the labels in the specified dataframe\n",
    "    \"\"\"\n",
    "    y = df[[y_col]].to_numpy(dtype=np.int32).flatten()\n",
    "    weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "    return torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common image transformations\n",
    "These images transformations will apply to both train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class SamplewiseCenterNormalize(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.div(torch.add(tensor, torch.mul(torch.mean(tensor), -1)), torch.std(tensor) + 1e-6)\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "\n",
    "DATA_TRANSFORMS = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomAffine(degrees=20, translate=(0.2, 0.2)),\n",
    "    transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), shear=10, scale=(1.0, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    #transforms.RandomCrop(size=(480,640)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    # SamplewiseCenterNormalize()\n",
    "\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "TEST_DATA_TRANSFORMS = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    # SamplewiseCenterNormalize()\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 0.80/Val 0.20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_val_split_group(class_weights=None):\n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    train_ann_df = pd.read_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_ann_df   = pd.read_csv(VAL_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(train_ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "        \n",
    "    train_dataset = SoybeanDataset(TRAIN_SPLIT_PATCHES_DATA_DIR, train_ann_df, transforms=DATA_TRANSFORMS)\n",
    "    val_dataset = SoybeanDataset(VAL_SPLIT_PATCHES_DATA_DIR, val_ann_df, transforms=TEST_DATA_TRANSFORMS)\n",
    "    test_dataset  = SoybeanTestDatasetFolder(TEST_PATCHES_DATA_DIR, transforms=TEST_DATA_TRANSFORMS)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   pin_memory=True, \n",
    "                                                   num_workers=16)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 pin_memory=True,\n",
    "                                                 num_workers=16)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  pin_memory=True,\n",
    "                                                  num_workers=0)\n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, train_dataloader, val_dataloader, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 100%\n",
    "\n",
    "Train with all the data in the `TrainData-C2` dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def all_train_data_group(class_weights=None):\n",
    "    print(\"Reading annotations...\")\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "\n",
    "    train_dataset = SoybeanDataset(TRAIN_DATA_PATCHES_DIR, ann_df, transforms=DATA_TRANSFORMS)\n",
    "    test_dataset  = SoybeanTestDatasetFolder(TEST_DATA_PATCHES_DIR, transforms=TEST_DATA_TRANSFORMS)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   pin_memory=True, \n",
    "                                                   num_workers=16)\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  pin_memory=True,\n",
    "                                                  num_workers=0)\n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, train_dataloader, None, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This model is based on the VGG16 network with custom classifier layers \n",
    "with the feature layers initialized with weights based on the ImageNet data. \n",
    "\n",
    "The number of neurons and dropout rates in the classifier layers are parameterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_model(n1, n2, dropout, batch_normalization=False):\n",
    "    if batch_normalization:\n",
    "        model = models.vgg16_bn(pretrained=True)\n",
    "    else:\n",
    "        model = models.vgg16(pretrained=True) \n",
    "\n",
    "    # Freeze training for all layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "    \n",
    "    # Replace the VGG16 classifier with a custom classifier for soybean wilting \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(512 * 1 * 1, n1, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(n1, n2, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(n2, 5, bias=True)\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "For training and validation, this trains a model across a configured number of epochs and outputs the training and validation loss and accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Trains the specified neural network model\n",
    "    \n",
    "    Args:\n",
    "        model:         - neural network model to train\n",
    "        criterion:     - loss function\n",
    "        optimizer:     - gradient descent optimization algorithm\n",
    "        dataloaders:   - dict of DataLoaders for training and validation data\n",
    "        num_epochs:    - number of epochs to train model\n",
    "    Returns:\n",
    "        model   - trained model with weights from the epoch with the best validation accuracy\n",
    "        history - dict of training and validation loss and accuracy for all epochs\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    # summary(model, input_size=(3, 224, 224))\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    history = {'train': {'loss': [], 'acc': []}}\n",
    "    phases = ['train']\n",
    "    if ('val' in dataloaders and dataloaders['val'] is not None):\n",
    "        phases += ['val']\n",
    "        history['val'] = {'loss': [], 'acc': []}\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and optionally, a validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                \n",
    "            phase_start = time.time()\n",
    "\n",
    "            sample_count = 0\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:               \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                        nn.utils.clip_grad_value_(model.parameters(), 0.5)\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                sample_count += inputs.size(0)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += preds.eq(labels.data.view_as(preds)).cpu().sum()\n",
    "            \n",
    "            print('Num samples', sample_count)\n",
    "            \n",
    "            epoch_loss = running_loss / sample_count\n",
    "            epoch_acc = running_corrects.double() / sample_count\n",
    "            \n",
    "            history[phase]['loss'].append(epoch_loss)\n",
    "            history[phase]['acc'].append(epoch_acc)\n",
    "            \n",
    "            phase_end = time.time()\n",
    "            phase_elapsed = phase_end - phase_start\n",
    "\n",
    "            print('{} {} loss: {:.4f} accuracy: {:.4f}'.format(\n",
    "                phase, str(timedelta(seconds=phase_elapsed)), epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if ('val' not in phases or phase == 'val') and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        epoch_elapsed = epoch_end - epoch_start\n",
    "        print('Elapsed time: {}'.format(str(timedelta(seconds=epoch_elapsed))))\n",
    "        \n",
    "        print()\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def get_sample_count(dataset, sampler):\n",
    "    if (sampler is not None):\n",
    "        return len(sampler)\n",
    "    elif (dataset is not None):\n",
    "        return len(dataset)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def train(model, group, num_epochs=20):\n",
    "    criterion = nn.CrossEntropyLoss(weight=group.class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, eps=1e-07)\n",
    "    \n",
    "    dataloaders = {\n",
    "        'train': group.train_dataloader,\n",
    "        'val': group.val_dataloader\n",
    "    }\n",
    " \n",
    "    model_trained, history = train_model(model, criterion, optimizer, dataloaders, num_epochs)\n",
    "    \n",
    "    \n",
    "    return model_trained, history\n",
    "\n",
    "\n",
    "def get_all_labels(loader):\n",
    "    all_labels = torch.tensor([], dtype=torch.long)\n",
    "    for batch in loader:\n",
    "        _, labels = batch\n",
    "        all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "    return all_labels\n",
    "\n",
    "\n",
    "def get_all_whole_image_labels(patch_labels):\n",
    "    patch_label_groups = np.split(patch_labels, int(len(patch_labels)/(PATCH_ROWS * PATCH_COLUMNS)))\n",
    "    image_labels = list(map(lambda x: x[0], patch_label_groups))\n",
    "    return image_labels\n",
    "\n",
    "\n",
    "def get_all_whole_image_preductions(patch_preds):\n",
    "    patch_pred_groups = np.split(patch_preds, int(len(patch_labels)/(PATCH_ROWS * PATCH_COLUMNS)))\n",
    "    image_preds = list(map(lambda x: stats.mode(x).mode[0], patch_pred_groups))\n",
    "    return image_preds\n",
    "\n",
    "\n",
    "def plot_metrics(model, history, train_dataloader, val_dataloader=None):\n",
    "    \n",
    "    print()\n",
    "    print('Metrics')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(history['train']['loss']) + 1)\n",
    "\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, history['train']['loss'], 'g-')\n",
    "    loss_legend = ['Training Loss']\n",
    "    \n",
    "    if ('val' in history and history['val'] is not None):\n",
    "        plt.plot(epoch_count, history['val']['loss'], 'b-')\n",
    "        loss_legend += ['Validation Loss']\n",
    "        \n",
    "    plt.legend(loss_legend)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize accuracy history\n",
    "    plt.plot(epoch_count, history['train']['acc'], 'g-')\n",
    "    acc_legend = ['Training Accuracy']\n",
    "    \n",
    "    if ('val' in history and history['val'] is not None):\n",
    "        plt.plot(epoch_count, history['val']['acc'], 'b-')\n",
    "        acc_legend += ['Validation Accuracy']\n",
    "    \n",
    "    plt.legend(acc_legend)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    # Training confusion matrix\n",
    "    train_predictions = predict(model, train_dataloader).cpu().numpy()\n",
    "    \n",
    "    print(\"Training Confusion Matrix of Patches\")\n",
    "    print(\"-\" * 30)\n",
    "    print_confusion_matrix(get_all_labels(train_dataloader).cpu().numpy(),\n",
    "                           train_predictions)\n",
    "    \n",
    "    print(\"Training Confusion Matrix of Whole Images\")\n",
    "    print(\"-\" * 30)\n",
    "    print_confusion_matrix(get_all_whole_image_labels(train_dataloader).cpu().numpy(),\n",
    "                           get_all_whole_image_predictions(train_predictions))\n",
    "    \n",
    "    # Validation confusion matrix\n",
    "    if val_dataloader is not None:\n",
    "        val_predictions = predict(model, val_dataloader).cpu().numpy()\n",
    "        \n",
    "        print(\"Validation Confusion Matrix of Patches\")\n",
    "        print(\"-\" * 30)\n",
    "        print_confusion_matrix(get_all_labels(val_dataloader).cpu().numpy(),\n",
    "                               val_predictions)\n",
    "        \n",
    "        print(\"Validation Confusion Matrix of Whole Images\")\n",
    "        print(\"-\" * 30)\n",
    "        print_confusion_matrix(get_all_whole_image_labels(val_dataloader).cpu().numpy(),\n",
    "                               get_all_whole_image_predictions(val_predictions))\n",
    "\n",
    "\n",
    "def train_and_test(model, group, num_epochs=60):\n",
    "    model_trained, history = train(model, group, num_epochs)\n",
    "    \n",
    "    # Plot history metrics\n",
    "    plot_metrics(model_trained, history, group.train_dataloader, group.val_dataloader)\n",
    "    \n",
    "    # Classify test data\n",
    "    return predict(model_trained, group.test_dataloader)\n",
    "\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    predictions = torch.tensor([], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if (type(data) is list):\n",
    "                images = data[0].to(device)\n",
    "            else:\n",
    "                images = data.to(device)\n",
    "            model.eval()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions = torch.cat((predictions, predicted))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_whole_images(patch_predictions, rows, columns, csvfile):\n",
    "    y_hat_test = patch_predictions.cpu().numpy()\n",
    "    y_hat_patch_groups = np.split(y_hat_test, int(len(y_hat_test)/(rows * columns)))\n",
    "    y_hat_whole_images = list(map(lambda x: stats.mode(x).mode[0], y_hat_patch_groups))\n",
    "\n",
    "    for k, v in sorted(Counter(y_hat_whole_images).items()): \n",
    "        print(str(k) + ': '+ str(v))    \n",
    "\n",
    "    one_hots = [np.zeros((5,1)) for pred in y_hat_whole_images]\n",
    "    for i in range(len(one_hots)):\n",
    "        pred = y_hat_whole_images[i]  # the index of the one-hot encoding\n",
    "        one_hots[i][pred] = 1\n",
    "    with open(csvfile, 'w') as predictions_file:\n",
    "        writer = csv.writer(predictions_file)\n",
    "        for pred in one_hots:\n",
    "            pred = np.array(pred, dtype=int)\n",
    "            writer.writerow(pred.T.tolist()[0])\n",
    "    print('Finished generating predictions to', csvfile)\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y, y_hat):\n",
    "    confusion_matrix = np.zeros((5, 5))\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ground_truth = y==labels[i]\n",
    "            prediction = y_hat==labels[j]\n",
    "            confusion_matrix[i, j] = sum(np.bitwise_and(ground_truth, prediction))\n",
    "    df = pd.DataFrame(confusion_matrix, dtype=int)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test - 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model(1024, 128, 0.5).to(device)\n",
    "y_hat_test = train_and_test(model, train_val_split_group(), num_epochs=1)\n",
    "predictions_file = \"predict_c2_g1_\" + shortuuid.uuid()\n",
    "print('predictions file:', predictions_file)\n",
    "predict_whole_images(y_hat_test, 5, 5, predictions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test - All Train Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(1024, 128, 0.1).to(device)\n",
    "y_hat_test = train_and_test(model, group_3())\n",
    "predict_whole_images(y_hat_test, 5, 5, 'predictions_c2_g3.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "The following hyperparameters can be tuned:\n",
    "1. `n1` - Number of neurons in the first classifier dense layer\n",
    "2. `n2` - Number of neurons in the second classifier dense layer\n",
    "3. `d` - Dropout rate after classifier dense layers\n",
    "4. class weights - `[1,1,1,1,1]` (default) or `[1,1,5,5,1]`\n",
    "5. batch normalization - `no` or `yes`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(name, model, class_weights=None, num_epochs=40):\n",
    "    y_hat_test = train_and_test(model, train_val_split_group(class_weights), num_epochs)\n",
    "    predictions_file = \"predict_c2_{}_{}.csv\".format(name, shortuuid.uuid())\n",
    "    print('predictions file:', predictions_file)\n",
    "    predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)\n",
    "    return y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1: 1024-128-5\n",
    "\n",
    "* DNN Structure: 1024-128-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading annotations...\n",
      "Computing class weights...\n",
      "tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Epoch 0/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.531849 loss: 1.3353 accuracy: 0.4804\n",
      "Num samples 6375\n",
      "val 0:00:11.932568 loss: 8.8915 accuracy: 0.3824\n",
      "Elapsed time: 0:01:43.467829\n",
      "\n",
      "Epoch 1/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.583554 loss: 1.2214 accuracy: 0.5275\n",
      "Num samples 6375\n",
      "val 0:00:11.872501 loss: 7.3561 accuracy: 0.3865\n",
      "Elapsed time: 0:01:43.459120\n",
      "\n",
      "Epoch 2/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.657559 loss: 1.1407 accuracy: 0.5542\n",
      "Num samples 6375\n",
      "val 0:00:11.975341 loss: 7.1673 accuracy: 0.3879\n",
      "Elapsed time: 0:01:43.635853\n",
      "\n",
      "Epoch 3/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.535995 loss: 1.1174 accuracy: 0.5704\n",
      "Num samples 6375\n",
      "val 0:00:11.995234 loss: 8.0486 accuracy: 0.3857\n",
      "Elapsed time: 0:01:43.531998\n",
      "\n",
      "Epoch 4/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.708489 loss: 1.0996 accuracy: 0.5852\n",
      "Num samples 6375\n",
      "val 0:00:12.373285 loss: 7.3063 accuracy: 0.3873\n",
      "Elapsed time: 0:01:44.082561\n",
      "\n",
      "Epoch 5/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.045913 loss: 1.0770 accuracy: 0.5935\n",
      "Num samples 6375\n",
      "val 0:00:11.939610 loss: 5.9913 accuracy: 0.3951\n",
      "Elapsed time: 0:01:43.987597\n",
      "\n",
      "Epoch 6/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.686416 loss: 1.0612 accuracy: 0.6005\n",
      "Num samples 6375\n",
      "val 0:00:11.900206 loss: 6.3459 accuracy: 0.3976\n",
      "Elapsed time: 0:01:43.589480\n",
      "\n",
      "Epoch 7/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.562290 loss: 1.0405 accuracy: 0.6094\n",
      "Num samples 6375\n",
      "val 0:00:11.948135 loss: 5.5561 accuracy: 0.3973\n",
      "Elapsed time: 0:01:43.511174\n",
      "\n",
      "Epoch 8/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.445175 loss: 1.0347 accuracy: 0.6136\n",
      "Num samples 6375\n",
      "val 0:00:12.174680 loss: 6.4980 accuracy: 0.3923\n",
      "Elapsed time: 0:01:44.620632\n",
      "\n",
      "Epoch 9/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.621171 loss: 1.0350 accuracy: 0.6179\n",
      "Num samples 6375\n",
      "val 0:00:11.960671 loss: 7.1228 accuracy: 0.3950\n",
      "Elapsed time: 0:01:45.582656\n",
      "\n",
      "Epoch 10/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.085194 loss: 1.0176 accuracy: 0.6202\n",
      "Num samples 6375\n",
      "val 0:00:12.054290 loss: 6.1625 accuracy: 0.3962\n",
      "Elapsed time: 0:01:44.140182\n",
      "\n",
      "Epoch 11/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.319846 loss: 1.0105 accuracy: 0.6252\n",
      "Num samples 6375\n",
      "val 0:00:12.282286 loss: 6.6455 accuracy: 0.3984\n",
      "Elapsed time: 0:01:44.605245\n",
      "\n",
      "Epoch 12/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.241395 loss: 1.0135 accuracy: 0.6230\n",
      "Num samples 6375\n",
      "val 0:00:12.260488 loss: 6.1253 accuracy: 0.3965\n",
      "Elapsed time: 0:01:45.502640\n",
      "\n",
      "Epoch 13/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.504834 loss: 1.0118 accuracy: 0.6252\n",
      "Num samples 6375\n",
      "val 0:00:12.468259 loss: 4.8673 accuracy: 0.4006\n",
      "Elapsed time: 0:01:44.976159\n",
      "\n",
      "Epoch 14/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.384943 loss: 0.9952 accuracy: 0.6273\n",
      "Num samples 6375\n",
      "val 0:00:12.101861 loss: 4.9938 accuracy: 0.3991\n",
      "Elapsed time: 0:01:44.487547\n",
      "\n",
      "Epoch 15/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.272068 loss: 0.9928 accuracy: 0.6332\n",
      "Num samples 6375\n",
      "val 0:00:12.104801 loss: 5.1086 accuracy: 0.3973\n",
      "Elapsed time: 0:01:44.377702\n",
      "\n",
      "Epoch 16/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.452878 loss: 0.9830 accuracy: 0.6356\n",
      "Num samples 6375\n",
      "val 0:00:12.107597 loss: 5.1342 accuracy: 0.3992\n",
      "Elapsed time: 0:01:44.561337\n",
      "\n",
      "Epoch 17/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.419734 loss: 0.9839 accuracy: 0.6375\n",
      "Num samples 6375\n",
      "val 0:00:12.149975 loss: 4.8238 accuracy: 0.3976\n",
      "Elapsed time: 0:01:44.570578\n",
      "\n",
      "Epoch 18/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.988033 loss: 0.9915 accuracy: 0.6345\n",
      "Num samples 6375\n",
      "val 0:00:12.083672 loss: 4.3496 accuracy: 0.4016\n",
      "Elapsed time: 0:01:45.074736\n",
      "\n",
      "Epoch 19/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.773001 loss: 0.9736 accuracy: 0.6374\n",
      "Num samples 6375\n",
      "val 0:00:12.453998 loss: 4.4013 accuracy: 0.4035\n",
      "Elapsed time: 0:01:45.230030\n",
      "\n",
      "Epoch 20/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.149611 loss: 0.9728 accuracy: 0.6402\n",
      "Num samples 6375\n",
      "val 0:00:11.964405 loss: 5.1540 accuracy: 0.3995\n",
      "Elapsed time: 0:01:45.114937\n",
      "\n",
      "Epoch 21/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.806585 loss: 0.9721 accuracy: 0.6422\n",
      "Num samples 6375\n",
      "val 0:00:12.280538 loss: 5.3886 accuracy: 0.3991\n",
      "Elapsed time: 0:01:45.087916\n",
      "\n",
      "Epoch 22/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.588058 loss: 0.9715 accuracy: 0.6439\n",
      "Num samples 6375\n",
      "val 0:00:11.959378 loss: 4.5076 accuracy: 0.4033\n",
      "Elapsed time: 0:01:44.548250\n",
      "\n",
      "Epoch 23/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.810880 loss: 0.9663 accuracy: 0.6452\n",
      "Num samples 6375\n",
      "val 0:00:12.022118 loss: 3.7720 accuracy: 0.4055\n",
      "Elapsed time: 0:01:43.835937\n",
      "\n",
      "Epoch 24/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.772614 loss: 0.9753 accuracy: 0.6454\n",
      "Num samples 6375\n",
      "val 0:00:12.072736 loss: 4.2929 accuracy: 0.4086\n",
      "Elapsed time: 0:01:43.848311\n",
      "\n",
      "Epoch 25/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.795765 loss: 0.9677 accuracy: 0.6488\n",
      "Num samples 6375\n",
      "val 0:00:12.008899 loss: 4.7449 accuracy: 0.4047\n",
      "Elapsed time: 0:01:43.805506\n",
      "\n",
      "Epoch 26/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.535501 loss: 0.9557 accuracy: 0.6487\n",
      "Num samples 6375\n",
      "val 0:00:12.042979 loss: 4.5693 accuracy: 0.4044\n",
      "Elapsed time: 0:01:44.579324\n",
      "\n",
      "Epoch 27/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.143186 loss: 0.9698 accuracy: 0.6477\n",
      "Num samples 6375\n",
      "val 0:00:11.995254 loss: 3.8070 accuracy: 0.4130\n",
      "Elapsed time: 0:01:44.141312\n",
      "\n",
      "Epoch 28/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.629462 loss: 0.9497 accuracy: 0.6491\n",
      "Num samples 6375\n",
      "val 0:00:12.014841 loss: 4.3107 accuracy: 0.4038\n",
      "Elapsed time: 0:01:43.645077\n",
      "\n",
      "Epoch 29/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.881392 loss: 0.9632 accuracy: 0.6481\n",
      "Num samples 6375\n",
      "val 0:00:12.018527 loss: 3.9981 accuracy: 0.4049\n",
      "Elapsed time: 0:01:43.900722\n",
      "\n",
      "Epoch 30/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.744972 loss: 0.9455 accuracy: 0.6495\n",
      "Num samples 6375\n",
      "val 0:00:11.907187 loss: 4.1226 accuracy: 0.4111\n",
      "Elapsed time: 0:01:43.653110\n",
      "\n",
      "Epoch 31/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.747157 loss: 0.9493 accuracy: 0.6512\n",
      "Num samples 6375\n",
      "val 0:00:12.013083 loss: 3.8241 accuracy: 0.4067\n",
      "Elapsed time: 0:01:43.761026\n",
      "\n",
      "Epoch 32/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.737334 loss: 0.9517 accuracy: 0.6536\n",
      "Num samples 6375\n",
      "val 0:00:11.944206 loss: 3.8620 accuracy: 0.4118\n",
      "Elapsed time: 0:01:43.682364\n",
      "\n",
      "Epoch 33/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.282696 loss: 0.9455 accuracy: 0.6545\n",
      "Num samples 6375\n",
      "val 0:00:12.119191 loss: 4.5249 accuracy: 0.4014\n",
      "Elapsed time: 0:01:44.402765\n",
      "\n",
      "Epoch 34/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.785591 loss: 0.9539 accuracy: 0.6524\n",
      "Num samples 6375\n",
      "val 0:00:12.061908 loss: 4.2682 accuracy: 0.4061\n",
      "Elapsed time: 0:01:43.848405\n",
      "\n",
      "Epoch 35/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.785176 loss: 0.9615 accuracy: 0.6554\n",
      "Num samples 6375\n",
      "val 0:00:11.945251 loss: 3.4696 accuracy: 0.4201\n",
      "Elapsed time: 0:01:43.732585\n",
      "\n",
      "Epoch 36/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.868693 loss: 0.9461 accuracy: 0.6562\n",
      "Num samples 6375\n",
      "val 0:00:11.988945 loss: 3.8019 accuracy: 0.4108\n",
      "Elapsed time: 0:01:43.858253\n",
      "\n",
      "Epoch 37/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.847759 loss: 0.9446 accuracy: 0.6559\n",
      "Num samples 6375\n",
      "val 0:00:12.077971 loss: 3.8725 accuracy: 0.4094\n",
      "Elapsed time: 0:01:43.926504\n",
      "\n",
      "Epoch 38/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.731462 loss: 0.9411 accuracy: 0.6538\n",
      "Num samples 6375\n",
      "val 0:00:12.036381 loss: 3.3748 accuracy: 0.4215\n",
      "Elapsed time: 0:01:43.769972\n",
      "\n",
      "Epoch 39/39\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.838570 loss: 0.9389 accuracy: 0.6585\n",
      "Num samples 6375\n",
      "val 0:00:12.186984 loss: 3.6370 accuracy: 0.4058\n",
      "Elapsed time: 0:01:44.026185\n",
      "\n",
      "Training complete in 69m 28s\n",
      "Best acc: 0.421490\n",
      "\n",
      "Metrics\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>9943</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9662</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8965</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5884</td>\n",
       "      <td>7</td>\n",
       "      <td>109</td>\n",
       "      <td>3998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>461</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1   2    3     4\n",
       "0  10  9943   1    5    41\n",
       "1   1  9662   1    1   335\n",
       "2   0  8965  20   14  1001\n",
       "3   2  5884   7  109  3998\n",
       "4   0   461   0    3  9536"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Confusion Matrix\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>2435</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1632</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1  2    3    4\n",
       "0  14  2435  0    1    0\n",
       "1   1  1632  0    4   13\n",
       "2   9   563  1   33   44\n",
       "3  23   302  0  106  219\n",
       "4   0    29  0   12  934"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions file: predict_c2_h1_KLign3dcdS2C4TXcuJKJPy.csv\n",
      "1: 145\n",
      "4: 55\n",
      "Finished generating predictions to predict_c2_h1_KLign3dcdS2C4TXcuJKJPy.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(1024, 128, 0.5).to(device)\n",
    "run_trial(\"h1\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2: 2048-256-5\n",
    "\n",
    "* DNN Structure: 2048-256-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(2048, 256, 0.5).to(device)\n",
    "run_trial(\"h2\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H3: 512-64-5\n",
    "\n",
    "* DNN Structure: 512-64-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(512, 64, 0.5).to(device)\n",
    "run_trial(\"h3\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H4: Best from above, dropout 0.25\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.25\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.25).to(device)\n",
    "run_trial(\"h4\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5: Best from above, dropout 0.1\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.1\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.1).to(device)\n",
    "run_trial(\"h5\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H6: Best from above, skewed class weights\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,5,5,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.5).to(device)\n",
    "run_trial(\"h6\", model, class_weights=[1., 1., 5., 5., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H7: Best from above, batch normalization\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: \n",
    "* Batch normalization: yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.5, batch_normalization=True).to(device)\n",
    "run_trial(\"h7\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_n1 = 1024\n",
    "optimal_n2 = 128\n",
    "optimal_d = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with all C2 data and optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(optimal_n1, optimal_n2, optimal_d).to(device)\n",
    "# model.load_state_dict(torch.load('cnn_pytorch_c2.pt'))\n",
    "y_hat_test = train_and_test(model, group_3(), num_epochs=40)\n",
    "predictions_file = \"predict_c2_\" + shortuuid.uuid()\n",
    "print('predictions file:', predictions_file)\n",
    "predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "print(y_hat_test)\n",
    "torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn_pytorch_c2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
