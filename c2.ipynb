{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "import shortuuid\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from torchvision import models, transforms\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def ann_file(data_dir):\n",
    "    return os.path.join(data_dir, \"TrainAnnotations.csv\")\n",
    "\n",
    "\n",
    "TRAIN_DATA_DIR = \"data/TrainData-C2\"\n",
    "TRAIN_DATA_ANN_FILE = ann_file(TRAIN_DATA_DIR)\n",
    "\n",
    "TRAIN_SPLIT_DATA_DIR           = \"data/train/split\"\n",
    "TRAIN_SPLIT_ANN_FILE           = ann_file(TRAIN_SPLIT_DATA_DIR)\n",
    "TRAIN_SPLIT_AUGMENTED_DATA_DIR = \"data/train/augmented\"\n",
    "TRAIN_SPLIT_AUGMENTED_ANN_FILE = ann_file(TRAIN_SPLIT_AUGMENTED_DATA_DIR)\n",
    "TRAIN_SPLIT_PATCHES_DATA_DIR   = \"data/train/patches\"\n",
    "TRAIN_SPLIT_PATCHES_ANN_FILE   = ann_file(TRAIN_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TRAIN_ALL_AUGMENTED_DATA_DIR   = \"data/train-all/augmented\"\n",
    "TRAIN_ALL_AUGMENTED_ANN_FILE   = ann_file(TRAIN_ALL_AUGMENTED_DATA_DIR)\n",
    "TRAIN_ALL_PATCHES_DATA_DIR     = \"data/train-all/patches\"\n",
    "TRAIN_ALL_PATCHES_ANN_FILE     = ann_file(TRAIN_ALL_PATCHES_DATA_DIR)\n",
    "\n",
    "VAL_SPLIT_DATA_DIR         = \"data/val/split\"\n",
    "VAL_SPLIT_ANN_FILE         = ann_file(VAL_SPLIT_DATA_DIR)\n",
    "VAL_SPLIT_PATCHES_DATA_DIR = \"data/val/patches\"\n",
    "VAL_SPLIT_PATCHES_ANN_FILE = ann_file(VAL_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TEST_DATA_DIR         = \"data/TestData/\"\n",
    "TEST_PATCHES_DATA_DIR = \"data/test/patches\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Generate random, stratified 80/20 split for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories for splits already exist. Skipping\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists(TRAIN_SPLIT_DATA_DIR) or os.path.exists(VAL_SPLIT_DATA_DIR)):\n",
    "    print(\"Data directories for splits already exist. Skipping\")\n",
    "else:\n",
    "    # Generate 80/20 split\n",
    "\n",
    "    print(\"Reading {} annotations...\".format(TRAIN_DATA_ANN_FILE))\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "    print(\"Splitting data into training and validation sets...\")\n",
    "    train_df, val_df = train_test_split(ann_df,\n",
    "                                        train_size=0.80,\n",
    "                                        random_state=138,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=ann_df[['annotation']].to_numpy(dtype=np.int32).flatten())\n",
    "\n",
    "    os.makedirs(TRAIN_SPLIT_DATA_DIR)\n",
    "    os.makedirs(VAL_SPLIT_DATA_DIR)\n",
    "    \n",
    "    print(\"Copying files for training split...\")\n",
    "    for _, row in train_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(TRAIN_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating training split annotations...\")\n",
    "    train_df.sort_values('file_name').to_csv(TRAIN_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Copying files for validation split...\")\n",
    "    for _, row in val_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(VAL_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating validation split annotations...\")\n",
    "    val_df.sort_values('file_name').to_csv(VAL_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data\n",
    "Because the training dataset is unbalanced, augment the training data set by generating\n",
    "new images for the lower numbered samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DESIRED_CLASS_SAMPLE_COUNT = 400\n",
    "RANDOM_STATE = 13\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "PATCH_ROWS = 3\n",
    "PATCH_COLUMNS = 3\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.endswith(IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def augment_data(src_dir, src_ann_file, dest_dir, dest_ann_file, class_sample_count=500):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "    ann_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'}) \n",
    "    new_samples = {}\n",
    "    \n",
    "    for i in range(5):\n",
    "        class_df = ann_df.query(\"annotation == '{}'\".format(i))\n",
    "        num_class_samples = class_df.shape[0]\n",
    "        num_to_create = class_sample_count - num_class_samples\n",
    "            \n",
    "        print(\"Creating {} images for class {}\".format(num_to_create, i))\n",
    "        samples = class_df.sample(n=num_to_create, replace=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "        for idx, row in samples.iterrows():\n",
    "            new_filename = row['file_name'].split('.')[0] + \"_\" + shortuuid.uuid() + \".png\"\n",
    "    \n",
    "            # Apply transformations to each randomly selected sample\n",
    "            img = Image.open(src_dir + \"/\" + row['file_name'])\n",
    "            image_transforms = transforms.Compose([\n",
    "                #transforms.RandomAffine(degrees=20, translate=(0.2, 0.2)),\n",
    "                #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                #transforms.RandomResizedCrop((480, 640), scale=(1.0, 1.2)),\n",
    "                \n",
    "                transforms.RandomRotation((90,90), expand=True),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomHorizontalFlip()\n",
    "            ])\n",
    "            transformed_img = image_transforms(img)\n",
    "            transformed_img.save(os.path.join(dest_dir, new_filename))\n",
    "    \n",
    "            new_samples[new_filename] = row['annotation']\n",
    "    \n",
    "    # Add to new dataframe\n",
    "    balanced_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    balanced_df = balanced_df.append(pd.DataFrame.from_records([(k, v) for k, v in new_samples.items()],\n",
    "                                                 columns=['file_name', 'annotation']))\n",
    "    \n",
    "    # Write new annotations\n",
    "    balanced_df.sort_values('file_name').to_csv(dest_ann_file, index=False)\n",
    "    \n",
    "    # Copy images from training data split\n",
    "    for file in glob.glob(src_dir + \"/*\"):\n",
    "        if is_image_file(file):\n",
    "            shutil.copy(file, os.path.join(dest_dir, os.path.basename(file)))\n",
    "\n",
    "\n",
    "def generate_image_patches(img, rows, cols):\n",
    "    \"\"\"\n",
    "    Generates a list of in-memory image overlapping patches\n",
    "    \n",
    "    Args:\n",
    "        rows - number of rows of patchs to cover the height of the image\n",
    "        cols - number of colums of patches to cover the width of the image\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    sizeX = img.shape[1]\n",
    "    sizeY = img.shape[0]\n",
    "    \n",
    "    patch_sizeX = 224\n",
    "    patch_sizeY = 224\n",
    "    patch_relative_centerX = 112\n",
    "    patch_relative_centerY = 112\n",
    "\n",
    "    for i in range(0,rows):\n",
    "        for j in range(0, cols):\n",
    "            center = (patch_relative_centerX + (sizeX - patch_sizeX)/(rows - 1)*i, \n",
    "                      patch_relative_centerY + (sizeY - patch_sizeY)/(cols - 1)*j)\n",
    "            patches.append(cv2.getRectSubPix(img, (patch_sizeX, patch_sizeY), center))\n",
    "            \n",
    "    return patches\n",
    "\n",
    "\n",
    "def generate_patch_files(in_dir, out_dir, rows, cols):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    images = [f for f in os.listdir(in_dir) if os.path.isfile(os.path.join(in_dir, f)) and is_image_file(f)]   \n",
    "    for im in images:\n",
    "        img = cv2.imread(os.path.join(in_dir, im))\n",
    "        patches = generate_image_patches(img, rows, cols)\n",
    "        \n",
    "        for i in range(0,rows):\n",
    "            for j in range(0, cols):\n",
    "                patch = patches[i*rows + j]\n",
    "                patch_name = im.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                cv2.imwrite(out_dir + '/' + patch_name, patch)\n",
    "\n",
    "\n",
    "def generate_patch_annotations_df(df, rows, cols):\n",
    "    patches_ann = {}\n",
    "    \n",
    "    for ind in df.index: \n",
    "        file_name = df['file_name'][ind]\n",
    "        annotation = df['annotation'][ind]\n",
    "        \n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                patch_name = file_name.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                patches_ann[patch_name] = annotation\n",
    "    \n",
    "    return pd.DataFrame.from_records([(k, v) for k, v in patches_ann.items()], \n",
    "                                     columns=['file_name', 'annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run data augmentation\n",
    "\n",
    "Perform the data augmentation on the training data set split to balance the class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented SPLIT training data already exists. Skipping.\n",
      "Augmented ALL training data already exists. Skipping.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TRAIN_SPLIT_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented SPLIT training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for SPLIT training data...\")\n",
    "    augment_data(TRAIN_SPLIT_DATA_DIR,\n",
    "                 TRAIN_SPLIT_ANN_FILE,\n",
    "                 TRAIN_SPLIT_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_SPLIT_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=400)    \n",
    "    print(\"Done.\")\n",
    "\n",
    "if os.path.exists(TRAIN_ALL_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented ALL training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for ALL training data...\")\n",
    "    augment_data(TRAIN_DATA_DIR,\n",
    "                 TRAIN_DATA_ANN_FILE,\n",
    "                 TRAIN_ALL_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_ALL_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=500)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train/patches exists. Skipping.\n",
      "data/val/patches exists. Skipping.\n",
      "data/test/patches exists. Skipping.\n",
      "data/train-all/patches exists. Skipping.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# SPLIT train patches\n",
    "if os.path.exists(TRAIN_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT training data patches...\")\n",
    "    generate_patch_files(TRAIN_SPLIT_AUGMENTED_DATA_DIR, TRAIN_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "    \n",
    "# SPLIT val patches\n",
    "if os.path.exists(VAL_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(VAL_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT validation data patches...\")\n",
    "    generate_patch_files(VAL_SPLIT_DATA_DIR, VAL_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT validation patch data annotations...\")\n",
    "    image_df = pd.read_csv(VAL_SPLIT_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(VAL_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "# test patches\n",
    "if os.path.exists(TEST_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TEST_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating test data patches...\")\n",
    "    generate_patch_files(TEST_DATA_DIR, TEST_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    \n",
    "# ALL train patches\n",
    "if os.path.exists(TRAIN_ALL_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_ALL_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating ALL train data patches...\")\n",
    "    generate_patch_files(TRAIN_ALL_AUGMENTED_DATA_DIR, TRAIN_ALL_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating ALL training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_ALL_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_ALL_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Given a directory of images and a CSV file of annotations, this defines a PyTorch Dataset which will load an image from disk and apply all configure transformations and return a tuple containing the image and label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SoybeanDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, data_path, ann_df, whole_image_ann_df, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): path to images\n",
    "            ann_df (string): pandas data frame containing file names and annotations\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.data = ann_df\n",
    "        self.whole_image_data = whole_image_ann_df\n",
    "        self.labels = np.asarray(self.data.iloc[:, 1])\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print('index:', index)\n",
    "        image_label = int(self.labels[index])\n",
    "        file_name = self.data.file_name[index]\n",
    "        img_path = os.path.join(self.data_path, file_name)\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Transform image\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return file_name, img, image_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "class SoybeanTestDatasetFolder(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, data_path, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): path to images\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.images = []\n",
    "        \n",
    "        for root, _, fnames in sorted(os.walk(data_path, followlinks=True)):\n",
    "            for fname in sorted(fnames):\n",
    "                path = fname\n",
    "                if is_image_file(path):\n",
    "                    self.images.append(path)\n",
    "\n",
    "                                       \n",
    "    def image_gen(self):\n",
    "        for i in self.images:\n",
    "            img_path = os.path.join(self.data_path, i)\n",
    "            img = Image.open(img_path)\n",
    "        \n",
    "            # Transform image\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "                \n",
    "            yield img\n",
    "            \n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.image_gen())\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "\n",
    "\n",
    "class SoybeanDataGroup():\n",
    "    def __init__(self, class_weights, \n",
    "                 train_dataloader,\n",
    "                 val_dataloader=None,\n",
    "                 test_dataloader=None):\n",
    "        self.class_weights = class_weights\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "\n",
    "def compute_class_weights(df, y_col):\n",
    "    \"\"\"\n",
    "    Returns a list of class labels to 'balanced' weights based on the\n",
    "    frequency of the weights across the labels in the specified dataframe\n",
    "    \"\"\"\n",
    "    y = df[[y_col]].to_numpy(dtype=np.int32).flatten()\n",
    "    weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "    return torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common image transformations\n",
    "These images transformations will apply to both train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class SamplewiseCenterNormalize(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.div(torch.add(tensor, torch.mul(torch.mean(tensor), -1)), torch.std(tensor) + 1e-6)\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "\n",
    "DATA_TRANSFORMS = transforms.Compose([\n",
    "    \n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    \n",
    "    \n",
    "    #transforms.RandomAffine(degrees=20, translate=(0.2, 0.2)),\n",
    "    #transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), shear=10, scale=(1.0, 1.2)),\n",
    "    #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    #transforms.RandomCrop(size=(480,640)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    # SamplewiseCenterNormalize()\n",
    "\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "TEST_DATA_TRANSFORMS = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    # SamplewiseCenterNormalize()\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 0.80/Val 0.20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_val_split_group(class_weights=None):\n",
    "    \n",
    "    train_whole_image_ann_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_whole_image_ann_df   = pd.read_csv(VAL_SPLIT_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    train_ann_df = pd.read_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_ann_df   = pd.read_csv(VAL_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(train_ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "        \n",
    "    train_dataset = SoybeanDataset(TRAIN_SPLIT_PATCHES_DATA_DIR, train_ann_df, train_whole_image_ann_df, transforms=DATA_TRANSFORMS)\n",
    "    val_dataset = SoybeanDataset(VAL_SPLIT_PATCHES_DATA_DIR, val_ann_df, val_whole_image_ann_df, transforms=TEST_DATA_TRANSFORMS)\n",
    "    test_dataset  = SoybeanTestDatasetFolder(TEST_PATCHES_DATA_DIR, transforms=TEST_DATA_TRANSFORMS)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   pin_memory=True, \n",
    "                                                   num_workers=16)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 pin_memory=True,\n",
    "                                                 num_workers=16)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  pin_memory=True,\n",
    "                                                  num_workers=0)\n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, train_dataloader, val_dataloader, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 100%\n",
    "\n",
    "Train with all the data in the `TrainData-C2` dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def all_train_data_group(class_weights=None):\n",
    "    \n",
    "    train_whole_image_ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})    \n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "\n",
    "    train_dataset = SoybeanDataset(TRAIN_DATA_PATCHES_DIR, ann_df, train_whole_image_ann_df, transforms=DATA_TRANSFORMS)\n",
    "    test_dataset  = SoybeanTestDatasetFolder(TEST_DATA_PATCHES_DIR, transforms=TEST_DATA_TRANSFORMS)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   pin_memory=True, \n",
    "                                                   num_workers=16)\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  pin_memory=True,\n",
    "                                                  num_workers=0)\n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, train_dataloader, None, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This model is based on the VGG16 network with custom classifier layers \n",
    "with the feature layers initialized with weights based on the ImageNet data. \n",
    "\n",
    "The number of neurons and dropout rates in the classifier layers are parameterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_model(n1, n2, dropout, batch_normalization=False):\n",
    "    if batch_normalization:\n",
    "        model = models.vgg16_bn(pretrained=True)\n",
    "    else:\n",
    "        model = models.vgg16(pretrained=True) \n",
    "\n",
    "    # Freeze training for all layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "    \n",
    "    # Replace the VGG16 classifier with a custom classifier for soybean wilting \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(512 * 1 * 1, n1, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(n1, n2, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(n2, 5, bias=True)\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "For training and validation, this trains a model across a configured number of epochs and outputs the training and validation loss and accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs):\n",
    "    \"\"\"\n",
    "    Trains the specified neural network model\n",
    "    \n",
    "    Args:\n",
    "        model:         - neural network model to train\n",
    "        criterion:     - loss function\n",
    "        optimizer:     - gradient descent optimization algorithm\n",
    "        dataloaders:   - dict of DataLoaders for training and validation data\n",
    "        num_epochs:    - number of epochs to train model\n",
    "    Returns:\n",
    "        model   - trained model with weights from the epoch with the best validation accuracy\n",
    "        history - dict of training and validation loss and accuracy for all epochs\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    # summary(model, input_size=(3, 224, 224))\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    history = {'train': {'loss': [], 'acc': []}}\n",
    "    phases = ['train']\n",
    "    if ('val' in dataloaders and dataloaders['val'] is not None):\n",
    "        phases += ['val']\n",
    "        history['val'] = {'loss': [], 'acc': []}\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and optionally, a validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                \n",
    "            phase_start = time.time()\n",
    "\n",
    "            sample_count = 0\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for _, inputs, labels in dataloaders[phase]:               \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                        nn.utils.clip_grad_value_(model.parameters(), 0.5)\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                sample_count += inputs.size(0)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += preds.eq(labels.data.view_as(preds)).cpu().sum()\n",
    "            \n",
    "            print('Num samples', sample_count)\n",
    "            \n",
    "            epoch_loss = running_loss / sample_count\n",
    "            epoch_acc = running_corrects.double() / sample_count\n",
    "            \n",
    "            history[phase]['loss'].append(epoch_loss)\n",
    "            history[phase]['acc'].append(epoch_acc)\n",
    "            \n",
    "            phase_end = time.time()\n",
    "            phase_elapsed = phase_end - phase_start\n",
    "\n",
    "            print('{} {} loss: {:.4f} accuracy: {:.4f}'.format(\n",
    "                phase, str(timedelta(seconds=phase_elapsed)), epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if ('val' not in phases or phase == 'val') and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        epoch_elapsed = epoch_end - epoch_start\n",
    "        print('Elapsed time: {}'.format(str(timedelta(seconds=epoch_elapsed))))\n",
    "        \n",
    "        print()\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def get_sample_count(dataset, sampler):\n",
    "    if (sampler is not None):\n",
    "        return len(sampler)\n",
    "    elif (dataset is not None):\n",
    "        return len(dataset)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def train(run_id, model, group, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss(weight=group.class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, eps=1e-07)\n",
    "    \n",
    "    dataloaders = {\n",
    "        'train': group.train_dataloader,\n",
    "        'val': group.val_dataloader\n",
    "    }\n",
    " \n",
    "    model_trained, history = train_model(model, criterion, optimizer, dataloaders, num_epochs)\n",
    "    \n",
    "    \n",
    "    return model_trained, history\n",
    "\n",
    "\n",
    "def get_all_labels(loader):\n",
    "    all_labels = torch.tensor([], dtype=torch.long)\n",
    "    for batch in loader:\n",
    "        _, _, labels = batch\n",
    "        all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "    return all_labels\n",
    "\n",
    "\n",
    "def get_all_whole_image_filenames(loader):\n",
    "    return loader.dataset.whole_image_data['file_name'].to_numpy()\n",
    "    # all_filenames = []\n",
    "    #for batch in loader:\n",
    "    #    file_names, _, _ = batch\n",
    "    #    \n",
    "    #    for file_name in file_names:\n",
    "    #        # Hack: Trim off the patch coordinates from filename to get the whole image filename\n",
    "    #        whole_image_file_name = re.match(\"(.*)_\\d+_\\d+.png\", file_name).groups()[0] + '.jpg'\n",
    "    #        \n",
    "    #        \n",
    "    #        \n",
    "    #        if len(all_filenames) == 0 or all_filenames[-1] != whole_image_file_name:\n",
    "    #            all_filenames = all_filenames + [ whole_image_file_name ]\n",
    "    #         \n",
    "    # return all_filenames\n",
    "    \n",
    "\n",
    "def get_all_whole_image_labels(loader):\n",
    "    return loader.dataset.whole_image_data['annotation'].to_numpy(dtype=int)\n",
    "    #patch_label_groups = np.split(patch_labels, int(len(patch_labels)/(PATCH_ROWS * PATCH_COLUMNS)))\n",
    "    #image_labels = np.array(list(map(lambda x: x[0], patch_label_groups)))\n",
    "    #return image_labels\n",
    "\n",
    "\n",
    "def get_all_whole_image_predictions(patch_preds):\n",
    "    patch_pred_groups = np.split(patch_preds, int(len(patch_preds)/(PATCH_ROWS * PATCH_COLUMNS)))\n",
    "    image_preds = np.array(list(map(lambda x: stats.mode(x).mode[0], patch_pred_groups)))\n",
    "    return image_preds\n",
    "\n",
    "\n",
    "def plot_metrics(run_id, output_dir, model, history, train_dataloader, val_dataloader=None):\n",
    "    \n",
    "    print()\n",
    "    print('Metrics')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(history['train']['loss']) + 1)\n",
    "\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, history['train']['loss'], 'g-')\n",
    "    loss_legend = ['Training Loss']\n",
    "    \n",
    "    if ('val' in history and history['val'] is not None):\n",
    "        plt.plot(epoch_count, history['val']['loss'], 'b-')\n",
    "        loss_legend += ['Validation Loss']\n",
    "        \n",
    "    plt.legend(loss_legend)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize accuracy history\n",
    "    plt.plot(epoch_count, history['train']['acc'], 'g-')\n",
    "    acc_legend = ['Training Accuracy']\n",
    "    \n",
    "    if ('val' in history and history['val'] is not None):\n",
    "        plt.plot(epoch_count, history['val']['acc'], 'b-')\n",
    "        acc_legend += ['Validation Accuracy']\n",
    "    \n",
    "    plt.legend(acc_legend)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    # Training confusion matrix\n",
    "    train_patch_labels = get_all_labels(train_dataloader).cpu().numpy()\n",
    "    train_patch_predictions = predict(model, train_dataloader).cpu().numpy()\n",
    "    \n",
    "    print(\"Training Confusion Matrix of Patches\")\n",
    "    print(\"-\" * 30)\n",
    "    print_confusion_matrix(train_patch_labels, train_patch_predictions)\n",
    "    \n",
    "    # Generate prediction label results file\n",
    "    write_patch_predictions(run_id, 'train', output_dir, train_dataloader, train_patch_predictions)\n",
    "    \n",
    "    \n",
    "    print(\"Training Confusion Matrix of Whole Images\")\n",
    "    print(\"-\" * 30)\n",
    "    train_whole_image_filenames = get_all_whole_image_filenames(train_dataloader)\n",
    "    train_whole_image_labels = get_all_whole_image_labels(train_dataloader)\n",
    "    train_whole_image_predictions = get_all_whole_image_predictions(train_patch_predictions)\n",
    "    print_confusion_matrix(train_whole_image_labels, train_whole_image_predictions)\n",
    "    \n",
    "    write_whole_image_predictions(run_id, 'train', output_dir, \n",
    "                                  train_whole_image_filenames, \n",
    "                                  train_whole_image_labels, \n",
    "                                  train_whole_image_predictions)\n",
    "    \n",
    "    # Validation confusion matrix\n",
    "    if val_dataloader is not None:\n",
    "        val_patch_labels = get_all_labels(val_dataloader).cpu().numpy()\n",
    "        val_patch_predictions = predict(model, val_dataloader).cpu().numpy()\n",
    "        \n",
    "        print(\"Validation Confusion Matrix of Patches\")\n",
    "        print(\"-\" * 30)\n",
    "        print_confusion_matrix(val_patch_labels, val_patch_predictions)\n",
    "        \n",
    "        print(\"Validation Confusion Matrix of Whole Images\")\n",
    "        print(\"-\" * 30)\n",
    "        val_whole_image_filenames = get_all_whole_image_filenames(val_dataloader)\n",
    "        val_whole_image_labels = get_all_whole_image_labels(val_dataloader)\n",
    "        val_whole_image_predictions = get_all_whole_image_predictions(val_patch_predictions)\n",
    "        print_confusion_matrix(val_whole_image_labels, val_whole_image_predictions)\n",
    "        \n",
    "        # Generate prediction label results file\n",
    "        write_patch_predictions(run_id, 'val', output_dir, val_dataloader, val_patch_predictions)\n",
    "        \n",
    "        write_whole_image_predictions(run_id, 'val', output_dir, \n",
    "                                  val_whole_image_filenames, \n",
    "                                  val_whole_image_labels, \n",
    "                                  val_whole_image_predictions)\n",
    "    \n",
    "\n",
    "\n",
    "def train_and_test(run_id, model, group, num_epochs, output_dir):    \n",
    "    model_trained, history = train(run_id, model, group, num_epochs)\n",
    "    \n",
    "    # Save weights\n",
    "    torch.save(model_trained, os.path.join(output_dir, \"{}_weights.pt\".format(run_id)))\n",
    "    \n",
    "    # Plot history metrics\n",
    "    plot_metrics(run_id, output_dir, model_trained, history, group.train_dataloader, group.val_dataloader)\n",
    "    \n",
    "    # Classify test data\n",
    "    return predict(model_trained, group.test_dataloader)\n",
    "\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    predictions = torch.tensor([], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if (type(data) is list):\n",
    "                images = data[1].to(device)\n",
    "            else:\n",
    "                images = data.to(device)\n",
    "            model.eval()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions = torch.cat((predictions, predicted))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_whole_images(patch_predictions, rows, columns, csvfile):\n",
    "    y_hat_test = patch_predictions.cpu().numpy()\n",
    "    y_hat_patch_groups = np.split(y_hat_test, int(len(y_hat_test)/(rows * columns)))\n",
    "    y_hat_whole_images = list(map(lambda x: stats.mode(x).mode[0], y_hat_patch_groups))\n",
    "\n",
    "    for k, v in sorted(Counter(y_hat_whole_images).items()): \n",
    "        print(str(k) + ': '+ str(v))    \n",
    "\n",
    "    one_hots = [np.zeros((5,1)) for pred in y_hat_whole_images]\n",
    "    for i in range(len(one_hots)):\n",
    "        pred = y_hat_whole_images[i]  # the index of the one-hot encoding\n",
    "        one_hots[i][pred] = 1\n",
    "    with open(csvfile, 'w') as predictions_file:\n",
    "        writer = csv.writer(predictions_file)\n",
    "        for pred in one_hots:\n",
    "            pred = np.array(pred, dtype=int)\n",
    "            writer.writerow(pred.T.tolist()[0])\n",
    "    print('Finished generating predictions to', csvfile)\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y, y_hat):\n",
    "    confusion_matrix = np.zeros((5, 5))\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ground_truth = y==labels[i]\n",
    "            prediction = y_hat==labels[j]\n",
    "            confusion_matrix[i, j] = sum(np.bitwise_and(ground_truth, prediction))\n",
    "    df = pd.DataFrame(confusion_matrix, dtype=int)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def write_patch_predictions(run_id, phase, output_dir, dataloader, patch_predictions):\n",
    "    df = pd.DataFrame(columns=['file_name', 'annotation', 'prediction'])\n",
    "    i = 0\n",
    "    for file_names, _, labels in dataloader:\n",
    "        for j in range(BATCH_SIZE):\n",
    "            if len(file_names) > j:\n",
    "                df = df.append({'file_name': file_names[j], \n",
    "                                'annotation': labels[j].item(), \n",
    "                                'prediction': patch_predictions[i]}, ignore_index=True)\n",
    "                i += 1\n",
    "\n",
    "    df.to_csv(os.path.join(output_dir, \"{}_{}_patch_predictions.csv\".format(run_id, phase)), index=False)\n",
    "    \n",
    "\n",
    "def write_whole_image_predictions(run_id, phase, output_dir, filenames, labels, predictions):\n",
    "    print('filenames:', len(filenames))\n",
    "    print('labels:', len(labels))\n",
    "    print('predictions:', len(predictions))\n",
    "    \n",
    "    df = pd.DataFrame(columns=['file_name', 'annotation', 'prediction'])\n",
    "    for i in range(len(filenames)):\n",
    "        df = df.append({'file_name': filenames[i], \n",
    "                        'annotation': labels[i], \n",
    "                        'prediction': predictions[i]}, ignore_index=True)\n",
    "        \n",
    "    df.to_csv(os.path.join(output_dir, \"{}_{}_whole_image_predictions.csv\".format(run_id, phase)), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "The following hyperparameters can be tuned:\n",
    "1. `n1` - Number of neurons in the first classifier dense layer\n",
    "2. `n2` - Number of neurons in the second classifier dense layer\n",
    "3. `d` - Dropout rate after classifier dense layers\n",
    "4. class weights - `[1,1,1,1,1]` (default) or `[1,1,5,5,1]`\n",
    "5. batch normalization - `no` or `yes`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(name, model, class_weights=None, num_epochs=60):\n",
    "    run_id = shortuuid.uuid()\n",
    "    \n",
    "    # output directory\n",
    "    output_dir = os.path.join(\"output\", run_id)\n",
    "    os.makedirs(output_dir)\n",
    "    print(\"Output generated to:\", output_dir)\n",
    "    \n",
    "    \n",
    "    y_hat_test = train_and_test(run_id, model, train_val_split_group(class_weights), num_epochs, output_dir)\n",
    "    predictions_file = os.path.join(output_dir, \"{}_predict_c2_{}_test.csv\".format(run_id, name))\n",
    "    print('predictions file:', predictions_file)\n",
    "    predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)\n",
    "    return y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1: 1024-128-5\n",
    "\n",
    "* DNN Structure: 1024-128-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output generated to: output/j8XzC2WpsmbuNFSTUrdSxY\n",
      "Reading annotations...\n",
      "Computing class weights...\n",
      "tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Epoch 0/0\n",
      "----------\n",
      "Num samples 18000\n",
      "train 0:00:33.396120 loss: 1.0776 accuracy: 0.5326\n",
      "Num samples 2295\n",
      "val 0:00:04.608928 loss: 6.0119 accuracy: 0.4013\n",
      "Elapsed time: 0:00:38.008035\n",
      "\n",
      "Training complete in 0m 38s\n",
      "Best acc: 0.401307\n",
      "\n",
      "Metrics\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix of Patches\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3358</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2  3     4\n",
       "0  0  3599  0  0     1\n",
       "1  0  3585  0  0    15\n",
       "2  0  3358  0  0   242\n",
       "3  0  2228  0  0  1372\n",
       "4  0   127  0  0  3473"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix of Whole Images\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>385</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1  2  3    4\n",
       "0  0  400  0  0    0\n",
       "1  0  400  0  0    0\n",
       "2  0  385  0  0   15\n",
       "3  0  251  0  0  149\n",
       "4  0    4  0  0  396"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filenames: 2000\n",
      "labels: 2000\n",
      "predictions: 2000\n",
      "Validation Confusion Matrix of Patches\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>881</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>592</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1  2  3    4\n",
       "0  0  881  0  0    1\n",
       "1  0  592  0  0    2\n",
       "2  0  220  0  0   14\n",
       "3  0  187  0  0   47\n",
       "4  0   22  0  0  329"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Confusion Matrix of Whole Images\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1  2  3   4\n",
       "0  0  98  0  0   0\n",
       "1  0  66  0  0   0\n",
       "2  0  25  0  0   1\n",
       "3  0  24  0  0   2\n",
       "4  0   1  0  0  38"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filenames: 255\n",
      "labels: 255\n",
      "predictions: 255\n",
      "predictions file: output/j8XzC2WpsmbuNFSTUrdSxY/j8XzC2WpsmbuNFSTUrdSxY_predict_c2_h1_test.csv\n",
      "1: 145\n",
      "4: 55\n",
      "Finished generating predictions to output/j8XzC2WpsmbuNFSTUrdSxY/j8XzC2WpsmbuNFSTUrdSxY_predict_c2_h1_test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(1024, 128, 0.5).to(device)\n",
    "run_trial(\"h1\", model, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2: 2048-256-5\n",
    "\n",
    "* DNN Structure: 2048-256-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading annotations...\n",
      "Computing class weights...\n",
      "tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Epoch 0/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.446317 loss: 1.3905 accuracy: 0.4789\n",
      "Num samples 6375\n",
      "val 0:00:12.701587 loss: 9.2033 accuracy: 0.3824\n",
      "Elapsed time: 0:01:46.150866\n",
      "\n",
      "Epoch 1/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.409611 loss: 1.2681 accuracy: 0.5193\n",
      "Num samples 6375\n",
      "val 0:00:12.719525 loss: 8.2036 accuracy: 0.3848\n",
      "Elapsed time: 0:01:49.132174\n",
      "\n",
      "Epoch 2/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.383533 loss: 1.1811 accuracy: 0.5454\n",
      "Num samples 6375\n",
      "val 0:00:12.390535 loss: 9.0899 accuracy: 0.3915\n",
      "Elapsed time: 0:01:47.777464\n",
      "\n",
      "Epoch 3/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:34.742720 loss: 1.1304 accuracy: 0.5666\n",
      "Num samples 6375\n",
      "val 0:00:12.707570 loss: 8.9100 accuracy: 0.3962\n",
      "Elapsed time: 0:01:47.453331\n",
      "\n",
      "Epoch 4/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:34.798719 loss: 1.1193 accuracy: 0.5775\n",
      "Num samples 6375\n",
      "val 0:00:12.342146 loss: 8.5697 accuracy: 0.3942\n",
      "Elapsed time: 0:01:47.141854\n",
      "\n",
      "Epoch 5/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.846532 loss: 1.1040 accuracy: 0.5884\n",
      "Num samples 6375\n",
      "val 0:00:12.117848 loss: 8.1710 accuracy: 0.3909\n",
      "Elapsed time: 0:01:44.965247\n",
      "\n",
      "Epoch 6/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.870075 loss: 1.0611 accuracy: 0.6009\n",
      "Num samples 6375\n",
      "val 0:00:12.059290 loss: 7.0186 accuracy: 0.3962\n",
      "Elapsed time: 0:01:45.930078\n",
      "\n",
      "Epoch 7/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.317368 loss: 1.0619 accuracy: 0.6049\n",
      "Num samples 6375\n",
      "val 0:00:12.378101 loss: 6.9861 accuracy: 0.3933\n",
      "Elapsed time: 0:01:47.696587\n",
      "\n",
      "Epoch 8/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.770303 loss: 1.0465 accuracy: 0.6130\n",
      "Num samples 6375\n",
      "val 0:00:12.031612 loss: 7.5295 accuracy: 0.3947\n",
      "Elapsed time: 0:01:44.802763\n",
      "\n",
      "Epoch 9/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.368777 loss: 1.0386 accuracy: 0.6172\n",
      "Num samples 6375\n",
      "val 0:00:12.865901 loss: 7.6036 accuracy: 0.3958\n",
      "Elapsed time: 0:01:46.235446\n",
      "\n",
      "Epoch 10/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:34.609665 loss: 1.0223 accuracy: 0.6228\n",
      "Num samples 6375\n",
      "val 0:00:12.547639 loss: 7.0384 accuracy: 0.3936\n",
      "Elapsed time: 0:01:47.158131\n",
      "\n",
      "Epoch 11/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.925797 loss: 1.0205 accuracy: 0.6236\n",
      "Num samples 6375\n",
      "val 0:00:12.139263 loss: 6.1884 accuracy: 0.3940\n",
      "Elapsed time: 0:01:46.065908\n",
      "\n",
      "Epoch 12/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.408318 loss: 0.9992 accuracy: 0.6279\n",
      "Num samples 6375\n",
      "val 0:00:12.110076 loss: 6.7486 accuracy: 0.3944\n",
      "Elapsed time: 0:01:45.519231\n",
      "\n",
      "Epoch 13/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:34.145896 loss: 1.0120 accuracy: 0.6296\n",
      "Num samples 6375\n",
      "val 0:00:12.138463 loss: 4.9034 accuracy: 0.3995\n",
      "Elapsed time: 0:01:46.287674\n",
      "\n",
      "Epoch 14/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:34.735940 loss: 0.9926 accuracy: 0.6278\n",
      "Num samples 6375\n",
      "val 0:00:12.586497 loss: 6.1110 accuracy: 0.3986\n",
      "Elapsed time: 0:01:47.323260\n",
      "\n",
      "Epoch 15/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.195469 loss: 1.0007 accuracy: 0.6359\n",
      "Num samples 6375\n",
      "val 0:00:12.079826 loss: 5.8901 accuracy: 0.3944\n",
      "Elapsed time: 0:01:48.276203\n",
      "\n",
      "Epoch 16/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:34.949790 loss: 0.9878 accuracy: 0.6347\n",
      "Num samples 6375\n",
      "val 0:00:12.632655 loss: 5.2959 accuracy: 0.3995\n",
      "Elapsed time: 0:01:47.583317\n",
      "\n",
      "Epoch 17/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:37.341331 loss: 0.9828 accuracy: 0.6378\n",
      "Num samples 6375\n",
      "val 0:00:12.922867 loss: 4.7530 accuracy: 0.4083\n",
      "Elapsed time: 0:01:50.267232\n",
      "\n",
      "Epoch 18/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.369819 loss: 0.9807 accuracy: 0.6355\n",
      "Num samples 6375\n",
      "val 0:00:12.278671 loss: 5.1359 accuracy: 0.4030\n",
      "Elapsed time: 0:01:47.649202\n",
      "\n",
      "Epoch 19/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.316848 loss: 0.9868 accuracy: 0.6389\n",
      "Num samples 6375\n",
      "val 0:00:12.687606 loss: 6.0638 accuracy: 0.3958\n",
      "Elapsed time: 0:01:48.005154\n",
      "\n",
      "Epoch 20/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:37.062906 loss: 0.9865 accuracy: 0.6418\n",
      "Num samples 6375\n",
      "val 0:00:12.195055 loss: 5.5440 accuracy: 0.3991\n",
      "Elapsed time: 0:01:49.258653\n",
      "\n",
      "Epoch 21/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:34.753743 loss: 0.9710 accuracy: 0.6411\n",
      "Num samples 6375\n",
      "val 0:00:12.131851 loss: 5.1079 accuracy: 0.4006\n",
      "Elapsed time: 0:01:46.886456\n",
      "\n",
      "Epoch 22/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:34.909401 loss: 0.9764 accuracy: 0.6474\n",
      "Num samples 6375\n",
      "val 0:00:12.709875 loss: 6.8924 accuracy: 0.3975\n",
      "Elapsed time: 0:01:47.620156\n",
      "\n",
      "Epoch 23/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.326983 loss: 0.9851 accuracy: 0.6472\n",
      "Num samples 6375\n",
      "val 0:00:12.648403 loss: 5.4397 accuracy: 0.4017\n",
      "Elapsed time: 0:01:48.976289\n",
      "\n",
      "Epoch 24/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.292951 loss: 0.9741 accuracy: 0.6470\n",
      "Num samples 6375\n",
      "val 0:00:12.646255 loss: 5.5916 accuracy: 0.4000\n",
      "Elapsed time: 0:01:48.939996\n",
      "\n",
      "Epoch 25/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.381934 loss: 0.9698 accuracy: 0.6462\n",
      "Num samples 6375\n",
      "val 0:00:12.547987 loss: 4.8595 accuracy: 0.4014\n",
      "Elapsed time: 0:01:48.930686\n",
      "\n",
      "Epoch 26/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.485311 loss: 0.9738 accuracy: 0.6451\n",
      "Num samples 6375\n",
      "val 0:00:11.954821 loss: 4.9890 accuracy: 0.4050\n",
      "Elapsed time: 0:01:44.440988\n",
      "\n",
      "Epoch 27/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.552881 loss: 0.9666 accuracy: 0.6467\n",
      "Num samples 6375\n",
      "val 0:00:11.897653 loss: 4.4027 accuracy: 0.4069\n",
      "Elapsed time: 0:01:43.451291\n",
      "\n",
      "Epoch 28/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.736447 loss: 0.9544 accuracy: 0.6508\n",
      "Num samples 6375\n",
      "val 0:00:12.037029 loss: 5.9621 accuracy: 0.3980\n",
      "Elapsed time: 0:01:43.774290\n",
      "\n",
      "Epoch 29/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.643084 loss: 0.9734 accuracy: 0.6486\n",
      "Num samples 6375\n",
      "val 0:00:11.949557 loss: 4.3853 accuracy: 0.4042\n",
      "Elapsed time: 0:01:43.593463\n",
      "\n",
      "Epoch 30/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.994320 loss: 0.9676 accuracy: 0.6533\n",
      "Num samples 6375\n",
      "val 0:00:12.599160 loss: 4.4201 accuracy: 0.4053\n",
      "Elapsed time: 0:01:48.594348\n",
      "\n",
      "Epoch 31/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.392432 loss: 0.9415 accuracy: 0.6515\n",
      "Num samples 6375\n",
      "val 0:00:12.721320 loss: 4.9963 accuracy: 0.4017\n",
      "Elapsed time: 0:01:49.114616\n",
      "\n",
      "Epoch 32/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.490819 loss: 0.9626 accuracy: 0.6547\n",
      "Num samples 6375\n",
      "val 0:00:12.696318 loss: 4.6290 accuracy: 0.4104\n",
      "Elapsed time: 0:01:49.190421\n",
      "\n",
      "Epoch 33/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.953801 loss: 0.9510 accuracy: 0.6506\n",
      "Num samples 6375\n",
      "val 0:00:12.083073 loss: 5.1706 accuracy: 0.4045\n",
      "Elapsed time: 0:01:45.037656\n",
      "\n",
      "Epoch 34/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.945940 loss: 0.9482 accuracy: 0.6523\n",
      "Num samples 6375\n",
      "val 0:00:11.989662 loss: 4.6999 accuracy: 0.4017\n",
      "Elapsed time: 0:01:43.936547\n",
      "\n",
      "Epoch 35/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.148239 loss: 0.9412 accuracy: 0.6564\n",
      "Num samples 6375\n",
      "val 0:00:12.102806 loss: 5.3489 accuracy: 0.4019\n",
      "Elapsed time: 0:01:44.251984\n",
      "\n",
      "Epoch 36/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.662902 loss: 0.9496 accuracy: 0.6559\n",
      "Num samples 6375\n",
      "val 0:00:11.984693 loss: 4.7599 accuracy: 0.4140\n",
      "Elapsed time: 0:01:47.650865\n",
      "\n",
      "Epoch 37/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.445390 loss: 0.9492 accuracy: 0.6578\n",
      "Num samples 6375\n",
      "val 0:00:12.031265 loss: 4.6545 accuracy: 0.3987\n",
      "Elapsed time: 0:01:44.477509\n",
      "\n",
      "Epoch 38/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.077905 loss: 0.9321 accuracy: 0.6579\n",
      "Num samples 6375\n",
      "val 0:00:11.956241 loss: 4.6406 accuracy: 0.4099\n",
      "Elapsed time: 0:01:45.035012\n",
      "\n",
      "Epoch 39/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.901776 loss: 0.9306 accuracy: 0.6610\n",
      "Num samples 6375\n",
      "val 0:00:12.586820 loss: 4.9777 accuracy: 0.4069\n",
      "Elapsed time: 0:01:45.489425\n",
      "\n",
      "Epoch 40/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.068323 loss: 0.9397 accuracy: 0.6599\n",
      "Num samples 6375\n",
      "val 0:00:12.180112 loss: 5.4308 accuracy: 0.4086\n",
      "Elapsed time: 0:01:48.249300\n",
      "\n",
      "Epoch 41/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.942581 loss: 0.9507 accuracy: 0.6610\n",
      "Num samples 6375\n",
      "val 0:00:11.991653 loss: 5.4869 accuracy: 0.4009\n",
      "Elapsed time: 0:01:43.935080\n",
      "\n",
      "Epoch 42/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.944693 loss: 0.9398 accuracy: 0.6598\n",
      "Num samples 6375\n",
      "val 0:00:12.004042 loss: 4.3599 accuracy: 0.4199\n",
      "Elapsed time: 0:01:43.951889\n",
      "\n",
      "Epoch 43/59\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples 50000\n",
      "train 0:01:32.168537 loss: 0.9234 accuracy: 0.6613\n",
      "Num samples 6375\n",
      "val 0:00:12.006393 loss: 4.0248 accuracy: 0.4187\n",
      "Elapsed time: 0:01:44.175667\n",
      "\n",
      "Epoch 44/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.943241 loss: 0.9327 accuracy: 0.6625\n",
      "Num samples 6375\n",
      "val 0:00:11.973558 loss: 4.5963 accuracy: 0.4104\n",
      "Elapsed time: 0:01:43.917574\n",
      "\n",
      "Epoch 45/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.773142 loss: 0.9347 accuracy: 0.6598\n",
      "Num samples 6375\n",
      "val 0:00:12.553264 loss: 3.8805 accuracy: 0.4196\n",
      "Elapsed time: 0:01:46.327330\n",
      "\n",
      "Epoch 46/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.834409 loss: 0.9298 accuracy: 0.6640\n",
      "Num samples 6375\n",
      "val 0:00:12.550517 loss: 4.4783 accuracy: 0.4182\n",
      "Elapsed time: 0:01:48.385803\n",
      "\n",
      "Epoch 47/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.948553 loss: 0.9351 accuracy: 0.6652\n",
      "Num samples 6375\n",
      "val 0:00:12.607567 loss: 4.7004 accuracy: 0.4085\n",
      "Elapsed time: 0:01:48.556997\n",
      "\n",
      "Epoch 48/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.938042 loss: 0.9326 accuracy: 0.6635\n",
      "Num samples 6375\n",
      "val 0:00:12.562334 loss: 4.0226 accuracy: 0.4173\n",
      "Elapsed time: 0:01:48.501211\n",
      "\n",
      "Epoch 49/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:32.534904 loss: 0.9259 accuracy: 0.6655\n",
      "Num samples 6375\n",
      "val 0:00:11.948730 loss: 4.4150 accuracy: 0.4187\n",
      "Elapsed time: 0:01:44.484800\n",
      "\n",
      "Epoch 50/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:34.240318 loss: 0.9277 accuracy: 0.6613\n",
      "Num samples 6375\n",
      "val 0:00:12.566927 loss: 4.6054 accuracy: 0.4130\n",
      "Elapsed time: 0:01:46.808152\n",
      "\n",
      "Epoch 51/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.629989 loss: 0.9370 accuracy: 0.6624\n",
      "Num samples 6375\n",
      "val 0:00:12.538041 loss: 4.5618 accuracy: 0.4300\n",
      "Elapsed time: 0:01:48.171096\n",
      "\n",
      "Epoch 52/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.862685 loss: 0.9323 accuracy: 0.6671\n",
      "Num samples 6375\n",
      "val 0:00:12.558466 loss: 4.7291 accuracy: 0.4122\n",
      "Elapsed time: 0:01:48.422112\n",
      "\n",
      "Epoch 53/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.891378 loss: 0.9243 accuracy: 0.6665\n",
      "Num samples 6375\n",
      "val 0:00:12.517007 loss: 4.9608 accuracy: 0.4044\n",
      "Elapsed time: 0:01:48.409240\n",
      "\n",
      "Epoch 54/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:35.913488 loss: 0.9262 accuracy: 0.6735\n",
      "Num samples 6375\n",
      "val 0:00:11.987215 loss: 4.5197 accuracy: 0.4108\n",
      "Elapsed time: 0:01:47.901576\n",
      "\n",
      "Epoch 55/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:31.715587 loss: 0.9290 accuracy: 0.6636\n",
      "Num samples 6375\n",
      "val 0:00:12.201329 loss: 5.3647 accuracy: 0.4019\n",
      "Elapsed time: 0:01:43.917765\n",
      "\n",
      "Epoch 56/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.885692 loss: 0.9199 accuracy: 0.6655\n",
      "Num samples 6375\n",
      "val 0:00:12.091285 loss: 4.3536 accuracy: 0.4216\n",
      "Elapsed time: 0:01:45.977813\n",
      "\n",
      "Epoch 57/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:33.015637 loss: 0.9279 accuracy: 0.6656\n",
      "Num samples 6375\n",
      "val 0:00:12.728252 loss: 3.1709 accuracy: 0.4290\n",
      "Elapsed time: 0:01:45.744978\n",
      "\n",
      "Epoch 58/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.844857 loss: 0.9237 accuracy: 0.6652\n",
      "Num samples 6375\n",
      "val 0:00:12.813831 loss: 4.2233 accuracy: 0.4158\n",
      "Elapsed time: 0:01:49.659527\n",
      "\n",
      "Epoch 59/59\n",
      "----------\n",
      "Num samples 50000\n",
      "train 0:01:36.706013 loss: 0.9211 accuracy: 0.6659\n",
      "Num samples 6375\n",
      "val 0:00:12.600847 loss: 4.2600 accuracy: 0.4180\n",
      "Elapsed time: 0:01:49.307810\n",
      "\n",
      "Training complete in 106m 45s\n",
      "Best acc: 0.429961\n",
      "\n",
      "Metrics\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix of Patches\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>9955</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9721</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9046</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6154</td>\n",
       "      <td>12</td>\n",
       "      <td>357</td>\n",
       "      <td>3473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>493</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>9490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1   2    3     4\n",
       "0  13  9955   0    6    26\n",
       "1   2  9721   0    7   270\n",
       "2   2  9046  44   54   854\n",
       "3   4  6154  12  357  3473\n",
       "4   0   493   1   16  9490"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix of Whole Images\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1  2  3    4\n",
       "0  0  400  0  0    0\n",
       "1  0  399  0  0    1\n",
       "2  0  382  0  0   18\n",
       "3  0  289  0  0  111\n",
       "4  0   13  0  0  387"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Confusion Matrix of Patches\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>2429</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1639</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>571</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>303</td>\n",
       "      <td>1</td>\n",
       "      <td>211</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1  2    3    4\n",
       "0  18  2429  0    3    0\n",
       "1   2  1639  0    3    6\n",
       "2  11   571  2   40   26\n",
       "3  34   303  1  211  101\n",
       "4   2    39  1   62  871"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Confusion Matrix of Whole Images\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1  2  3   4\n",
       "0  0  98  0  0   0\n",
       "1  0  66  0  0   0\n",
       "2  0  23  0  2   1\n",
       "3  1  14  0  8   3\n",
       "4  0   0  0  1  38"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions file: predict_c2_h2_nFrnLAKTySysHV2ngdtruW.csv\n",
      "1: 145\n",
      "4: 55\n",
      "Finished generating predictions to predict_c2_h2_nFrnLAKTySysHV2ngdtruW.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(2048, 256, 0.5).to(device)\n",
    "run_trial(\"h2\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H3: 512-64-5\n",
    "\n",
    "* DNN Structure: 512-64-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(512, 64, 0.5).to(device)\n",
    "run_trial(\"h3\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H4: Best from above, dropout 0.25\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.25\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.25).to(device)\n",
    "run_trial(\"h4\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5: Best from above, dropout 0.1\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.1\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.1).to(device)\n",
    "run_trial(\"h5\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H6: Best from above, skewed class weights\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,5,5,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.5).to(device)\n",
    "run_trial(\"h6\", model, class_weights=[1., 1., 5., 5., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H7: Best from above, batch normalization\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: \n",
    "* Batch normalization: yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.5, batch_normalization=True).to(device)\n",
    "run_trial(\"h7\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_n1 = 1024\n",
    "optimal_n2 = 128\n",
    "optimal_d = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with all C2 data and optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(optimal_n1, optimal_n2, optimal_d).to(device)\n",
    "# model.load_state_dict(torch.load('cnn_pytorch_c2.pt'))\n",
    "y_hat_test = train_and_test(model, group_3(), num_epochs=40)\n",
    "predictions_file = \"predict_c2_{}.csv\".format(shortuuid.uuid())\n",
    "print('predictions file:', predictions_file)\n",
    "predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "print(y_hat_test)\n",
    "torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn_pytorch_c2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
