{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shortuuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "import shortuuid\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Get reproducible results\n",
    "random_state = 46\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "\n",
    "\n",
    "def ann_file(data_dir):\n",
    "    return os.path.join(data_dir, \"TrainAnnotations.csv\")\n",
    "\n",
    "\n",
    "TRAIN_DATA_DIR = \"data/TrainData-C1\"\n",
    "TRAIN_DATA_ANN_FILE = ann_file(TRAIN_DATA_DIR)\n",
    "\n",
    "TRAIN_SPLIT_DATA_DIR           = \"data-c1/train/split\"\n",
    "TRAIN_SPLIT_ANN_FILE           = ann_file(TRAIN_SPLIT_DATA_DIR)\n",
    "TRAIN_SPLIT_AUGMENTED_DATA_DIR = \"data-c1/train/augmented\"\n",
    "TRAIN_SPLIT_AUGMENTED_ANN_FILE = ann_file(TRAIN_SPLIT_AUGMENTED_DATA_DIR)\n",
    "TRAIN_SPLIT_PATCHES_DATA_DIR   = \"data-c1/train/patches\"\n",
    "TRAIN_SPLIT_PATCHES_ANN_FILE   = ann_file(TRAIN_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TRAIN_ALL_AUGMENTED_DATA_DIR   = \"data-c1/train-all/augmented\"\n",
    "TRAIN_ALL_AUGMENTED_ANN_FILE   = ann_file(TRAIN_ALL_AUGMENTED_DATA_DIR)\n",
    "TRAIN_ALL_PATCHES_DATA_DIR     = \"data-c1/train-all/patches\"\n",
    "TRAIN_ALL_PATCHES_ANN_FILE     = ann_file(TRAIN_ALL_PATCHES_DATA_DIR)\n",
    "\n",
    "VAL_SPLIT_DATA_DIR         = \"data-c1/val/split\"\n",
    "VAL_SPLIT_ANN_FILE         = ann_file(VAL_SPLIT_DATA_DIR)\n",
    "VAL_SPLIT_PATCHES_DATA_DIR = \"data-c1/val/patches\"\n",
    "VAL_SPLIT_PATCHES_ANN_FILE = ann_file(VAL_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TEST_DATA_DIR         = \"data/TestData/\"\n",
    "\n",
    "TEST_PATCHES_DATA_DIR = \"data/test/\"\n",
    "#TEST_PATCHES_DATA_DIR = \"data/test/patches\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU configuration\n",
    "If you have a GPU, enable experimental memory growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Generate random, stratified 80/20 split for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories for splits already exist. Skipping\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists(TRAIN_SPLIT_DATA_DIR) or os.path.exists(VAL_SPLIT_DATA_DIR)):\n",
    "    print(\"Data directories for splits already exist. Skipping\")\n",
    "else:\n",
    "    # Generate 80/20 split\n",
    "\n",
    "    print(\"Reading {} annotations...\".format(TRAIN_DATA_ANN_FILE))\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "    print(\"Splitting data into training and validation sets...\")\n",
    "    train_df, val_df = train_test_split(ann_df,\n",
    "                                        train_size=0.80,\n",
    "                                        random_state=138,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=ann_df[['annotation']].to_numpy(dtype=np.int32).flatten())\n",
    "\n",
    "    os.makedirs(TRAIN_SPLIT_DATA_DIR)\n",
    "    os.makedirs(VAL_SPLIT_DATA_DIR)\n",
    "    \n",
    "    print(\"Copying files for training split...\")\n",
    "    for _, row in train_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(TRAIN_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating training split annotations...\")\n",
    "    train_df.sort_values('file_name').to_csv(TRAIN_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Copying files for validation split...\")\n",
    "    for _, row in val_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(VAL_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating validation split annotations...\")\n",
    "    val_df.sort_values('file_name').to_csv(VAL_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data\n",
    "Because the training dataset is unbalanced, augment the training data set by generating\n",
    "new images for the lower numbered samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DESIRED_CLASS_SAMPLE_COUNT = 400\n",
    "RANDOM_STATE = 13\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "PATCH_ROWS = 5\n",
    "PATCH_COLUMNS = 5\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.endswith(IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def augment_data(src_dir, src_ann_file, dest_dir, dest_ann_file, class_sample_count=500):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "    ann_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'}) \n",
    "    new_samples = {}\n",
    "    \n",
    "    for i in range(5):\n",
    "        class_df = ann_df.query(\"annotation == '{}'\".format(i))\n",
    "        num_class_samples = class_df.shape[0]\n",
    "        num_to_create = class_sample_count - num_class_samples\n",
    "            \n",
    "        print(\"Creating {} images for class {}\".format(num_to_create, i))\n",
    "        samples = class_df.sample(n=num_to_create, replace=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "        for idx, row in samples.iterrows():\n",
    "            new_filename = row['file_name'].split('.')[0] + \"_\" + shortuuid.uuid() + \".png\"\n",
    "    \n",
    "            # Apply transformations to each randomly selected sample\n",
    "            img = Image.open(src_dir + \"/\" + row['file_name'])\n",
    "            image_transforms = transforms.Compose([\n",
    "                #transforms.RandomAffine(degrees=20, translate=(0.2, 0.2)),\n",
    "                #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                #transforms.RandomResizedCrop((480, 640), scale=(1.0, 1.2)),\n",
    "                \n",
    "                transforms.RandomRotation((90,90), expand=True),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomHorizontalFlip()\n",
    "            ])\n",
    "            transformed_img = image_transforms(img)\n",
    "            transformed_img.save(os.path.join(dest_dir, new_filename))\n",
    "    \n",
    "            new_samples[new_filename] = row['annotation']\n",
    "    \n",
    "    # Add to new dataframe\n",
    "    balanced_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    balanced_df = balanced_df.append(pd.DataFrame.from_records([(k, v) for k, v in new_samples.items()],\n",
    "                                                 columns=['file_name', 'annotation']))\n",
    "    \n",
    "    # Write new annotations\n",
    "    balanced_df.sort_values('file_name').to_csv(dest_ann_file, index=False)\n",
    "    \n",
    "    # Copy images from training data split\n",
    "    for file in glob.glob(src_dir + \"/*\"):\n",
    "        if is_image_file(file):\n",
    "            shutil.copy(file, os.path.join(dest_dir, os.path.basename(file)))\n",
    "\n",
    "\n",
    "def generate_image_patches(img, rows, cols):\n",
    "    \"\"\"\n",
    "    Generates a list of in-memory image overlapping patches\n",
    "    \n",
    "    Args:\n",
    "        rows - number of rows of patchs to cover the height of the image\n",
    "        cols - number of colums of patches to cover the width of the image\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    sizeX = img.shape[1]\n",
    "    sizeY = img.shape[0]\n",
    "    \n",
    "    patch_sizeX = 224\n",
    "    patch_sizeY = 224\n",
    "    patch_relative_centerX = 112\n",
    "    patch_relative_centerY = 112\n",
    "\n",
    "    for i in range(0,rows):\n",
    "        for j in range(0, cols):\n",
    "            center = (patch_relative_centerX + (sizeX - patch_sizeX)/(rows - 1)*i, \n",
    "                      patch_relative_centerY + (sizeY - patch_sizeY)/(cols - 1)*j)\n",
    "            patches.append(cv2.getRectSubPix(img, (patch_sizeX, patch_sizeY), center))\n",
    "            \n",
    "    return patches\n",
    "\n",
    "\n",
    "def generate_patch_files(in_dir, out_dir, rows, cols):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    images = [f for f in os.listdir(in_dir) if os.path.isfile(os.path.join(in_dir, f)) and is_image_file(f)]   \n",
    "    for im in images:\n",
    "        img = cv2.imread(os.path.join(in_dir, im))\n",
    "        patches = generate_image_patches(img, rows, cols)\n",
    "        \n",
    "        for i in range(0,rows):\n",
    "            for j in range(0, cols):\n",
    "                patch = patches[i*rows + j]\n",
    "                patch_name = im.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                cv2.imwrite(out_dir + '/' + patch_name, patch)\n",
    "\n",
    "\n",
    "def generate_patch_annotations_df(df, rows, cols):\n",
    "    patches_ann = {}\n",
    "    \n",
    "    for ind in df.index: \n",
    "        file_name = df['file_name'][ind]\n",
    "        annotation = df['annotation'][ind]\n",
    "        \n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                patch_name = file_name.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                patches_ann[patch_name] = annotation\n",
    "    \n",
    "    return pd.DataFrame.from_records([(k, v) for k, v in patches_ann.items()], \n",
    "                                     columns=['file_name', 'annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run data augmentation\n",
    "\n",
    "Perform the data augmentation on the training data set split to balance the class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented SPLIT training data already exists. Skipping.\n",
      "Augmented ALL training data already exists. Skipping.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TRAIN_SPLIT_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented SPLIT training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for SPLIT training data...\")\n",
    "    augment_data(TRAIN_SPLIT_DATA_DIR,\n",
    "                 TRAIN_SPLIT_ANN_FILE,\n",
    "                 TRAIN_SPLIT_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_SPLIT_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=400)    \n",
    "    print(\"Done.\")\n",
    "\n",
    "if os.path.exists(TRAIN_ALL_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented ALL training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for ALL training data...\")\n",
    "    augment_data(TRAIN_DATA_DIR,\n",
    "                 TRAIN_DATA_ANN_FILE,\n",
    "                 TRAIN_ALL_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_ALL_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=500)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-c1/train/patches exists. Skipping.\n",
      "data-c1/val/patches exists. Skipping.\n",
      "Generating test data patches...\n",
      "data-c1/train-all/patches exists. Skipping.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# SPLIT train patches\n",
    "if os.path.exists(TRAIN_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT training data patches...\")\n",
    "    generate_patch_files(TRAIN_SPLIT_AUGMENTED_DATA_DIR, TRAIN_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "    \n",
    "# SPLIT val patches\n",
    "if os.path.exists(VAL_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(VAL_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT validation data patches...\")\n",
    "    generate_patch_files(VAL_SPLIT_DATA_DIR, VAL_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT validation patch data annotations...\")\n",
    "    image_df = pd.read_csv(VAL_SPLIT_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(VAL_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "# test patches\n",
    "if os.path.exists(TEST_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TEST_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating test data patches...\")\n",
    "    generate_patch_files(TEST_DATA_DIR, TEST_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    \n",
    "# ALL train patches\n",
    "if os.path.exists(TRAIN_ALL_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_ALL_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating ALL train data patches...\")\n",
    "    generate_patch_files(TRAIN_ALL_AUGMENTED_DATA_DIR, TRAIN_ALL_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating ALL training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_ALL_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_ALL_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoybeanDataGroup():\n",
    "    def __init__(self, class_weights, \n",
    "                 train_generator,\n",
    "                 val_generator=None,\n",
    "                 test_generator=None,\n",
    "                 train_patch_ann_df=None,\n",
    "                 val_patch_ann_df=None,\n",
    "                 train_whole_image_ann_df=None,\n",
    "                 val_whole_image_ann_df=None):\n",
    "        self.class_weights = class_weights\n",
    "        self.train_generator = train_generator\n",
    "        self.val_generator = val_generator\n",
    "        self.test_generator = test_generator\n",
    "        self.train_patch_ann_df = train_patch_ann_df\n",
    "        self.val_patch_ann_df = val_patch_ann_df\n",
    "        self.train_whole_image_ann_df = train_whole_image_ann_df\n",
    "        self.val_whole_image_ann_df = val_whole_image_ann_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split_group(class_weights=None):\n",
    "    \n",
    "    train_whole_image_ann_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_whole_image_ann_df   = pd.read_csv(VAL_SPLIT_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    train_ann_df = pd.read_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_ann_df   = pd.read_csv(VAL_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(train_ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "        \n",
    "        \n",
    "    data_gen_args = dict(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True,\n",
    "        #rotation_range=10,\n",
    "        #width_shift_range=0.2,\n",
    "        #height_shift_range=0.2,\n",
    "        #horizontal_flip=True\n",
    "    )\n",
    "    train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    val_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "    print(\"Defining train data generator...\")\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_ann_df,\n",
    "            directory=TRAIN_SPLIT_PATCHES_DATA_DIR,\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"annotation\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='sparse',\n",
    "            target_size=(224,224)\n",
    "    )\n",
    "    print(\"Defining validation data generator...\")\n",
    "    val_generator = val_datagen.flow_from_dataframe(\n",
    "            dataframe=val_ann_df,\n",
    "            directory=VAL_SPLIT_PATCHES_DATA_DIR,\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"annotation\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='sparse',\n",
    "            target_size=(224,224)\n",
    "    )\n",
    "        \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory=TEST_PATCHES_DATA_DIR,\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        target_size=(224,224)\n",
    "    )\n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, \n",
    "                            train_generator, \n",
    "                            val_generator, \n",
    "                            test_generator,\n",
    "                            train_ann_df,\n",
    "                            val_ann_df,\n",
    "                            train_whole_image_ann_df, \n",
    "                            val_whole_image_ann_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def all_train_data_group(class_weights=None):\n",
    "    \n",
    "    train_whole_image_ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})    \n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "        \n",
    "        \n",
    "    data_gen_args = dict(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True,\n",
    "    )\n",
    "    train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "    print(\"Defining train data generator...\")\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=ann_df,\n",
    "            directory=TRAIN_DATA_PATCHES_DIR,\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"annotation\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='sparse',\n",
    "            target_size=(224,224)\n",
    "    )\n",
    "   \n",
    "        \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory=TEST_DATA_PATCHES_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        target_size=(224,224)\n",
    "    )  \n",
    "    \n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, \n",
    "                            train_generator, \n",
    "                            None, \n",
    "                            test_generator,\n",
    "                            ann_df,\n",
    "                            None,\n",
    "                            train_whole_image_ann_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This model is based on the VGG16 network with custom classifier layers \n",
    "with the feature layers initialized with weights based on the ImageNet data. \n",
    "\n",
    "The number of neurons and dropout rates in the classifier layers are parameterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_model(n1, n2, dropout):\n",
    "    vgg_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "    vgg_model.trainable = False\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        vgg_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(n1, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(n2, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(5)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "For training and validation, this trains a model across a configured number of epochs and outputs the training and validation loss and accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(df, y_col):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping class labels to 'balanced' weights based on the\n",
    "    frequency of the weights across the labels in the specified dataframe\n",
    "    \"\"\"\n",
    "    y = df[[y_col]].to_numpy().flatten()\n",
    "    weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "    return {label: weight for label, weight in enumerate(weights)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# group = train_val_split_group()\n",
    "\n",
    "\n",
    "# train_whole_image_ann_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "# val_whole_image_ann_df   = pd.read_csv(VAL_SPLIT_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "# print(\"Reading annotations...\")\n",
    "# train_ann_df = pd.read_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "# val_ann_df   = pd.read_csv(VAL_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "\n",
    "# print(\"Computing class weights...\")\n",
    "# class_weights = compute_class_weights(train_ann_df, 'annotation')\n",
    "\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# print(\"Defining data generators...\")\n",
    "# data_gen_args = dict(\n",
    "#     rescale=1./255,\n",
    "#     featurewise_center=False,\n",
    "#     featurewise_std_normalization=False,\n",
    "#     samplewise_center=True,\n",
    "#     samplewise_std_normalization=True,\n",
    "#     #rotation_range=10,\n",
    "#     #width_shift_range=0.2,\n",
    "#     #height_shift_range=0.2,\n",
    "#     #horizontal_flip=True\n",
    "# )\n",
    "# train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "# val_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "# print(\"Defining train data generator...\")\n",
    "# train_generator = train_datagen.flow_from_dataframe(\n",
    "#         dataframe=train_patches_df,\n",
    "#         directory=train_dir_patches,\n",
    "#         x_col=\"file_name\",\n",
    "#         y_col=\"annotation\",\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='sparse',\n",
    "#         target_size=(224,224)\n",
    "# )\n",
    "# print(\"Defining validation data generator...\")\n",
    "# val_generator = val_datagen.flow_from_dataframe(\n",
    "#         dataframe=val_patches_df,\n",
    "#         directory=train_dir_patches,\n",
    "#         x_col=\"file_name\",\n",
    "#         y_col=\"annotation\",\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='sparse',\n",
    "#         target_size=(224,224)\n",
    "# )\n",
    "\n",
    "# # Model\n",
    "# print(\"Defining model...\")\n",
    "# model = get_model()\n",
    "# model.summary()\n",
    "\n",
    "# #model.load_weights('model_vgg16.h5')\n",
    "\n",
    "# print('Fitting model...')\n",
    "\n",
    "# history = model.fit(train_generator, \n",
    "#                     steps_per_epoch=int(train_patches_df.shape[0] / batch_size), \n",
    "#                     epochs=60, \n",
    "#                     validation_data=val_generator, \n",
    "#                     class_weight=class_weights, \n",
    "#                     verbose=1, \n",
    "#                     validation_steps=int(val_patches_df.shape[0] / batch_size))\n",
    "\n",
    "# print('Evaluating model...')\n",
    "# plt.plot(history.history['accuracy'], label='accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.ylim([0, 1])\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "# test_result = model.evaluate(val_generator, verbose=1)\n",
    "# print('Test result:', test_result)\n",
    "\n",
    "# # Save the model weights\n",
    "# model.save_weights('model_vgg16_c2_tf.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def train(run_id, model, group, num_epochs):     \n",
    "    model.summary()\n",
    "\n",
    "    print('Fitting model...')\n",
    "    \n",
    "    print('group.train_patch_ann_df.shape[0]=', group.train_patch_ann_df.shape[0])\n",
    "    print('int(group.train_patch_ann_df.shape[0] / BATCH_SIZE)', int(group.train_patch_ann_df.shape[0] / BATCH_SIZE))\n",
    "\n",
    "    history = model.fit(group.train_generator, \n",
    "                        steps_per_epoch=int(group.train_patch_ann_df.shape[0] / BATCH_SIZE), \n",
    "                        epochs=num_epochs,  \n",
    "                        class_weight=group.class_weights,\n",
    "                        validation_data=group.val_generator,\n",
    "                        validation_steps=int(group.val_patch_ann_df.shape[0] / BATCH_SIZE),\n",
    "                        verbose=1)\n",
    "\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def get_all_labels(loader):\n",
    "    all_labels = torch.tensor([], dtype=torch.long)\n",
    "    for batch in loader:\n",
    "        _, _, labels = batch\n",
    "        all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "    return all_labels\n",
    "\n",
    "\n",
    "def get_all_whole_image_filenames(loader):\n",
    "    return loader.dataset.whole_image_data['file_name'].to_numpy()\n",
    "    \n",
    "\n",
    "def get_all_whole_image_labels(loader):\n",
    "    return loader.dataset.whole_image_data['annotation'].to_numpy(dtype=int)\n",
    "\n",
    "\n",
    "def get_all_whole_image_predictions(patch_preds):\n",
    "    patch_pred_groups = np.split(patch_preds, int(len(patch_preds)/(PATCH_ROWS * PATCH_COLUMNS)))\n",
    "    image_preds = np.array(list(map(lambda x: stats.mode(x).mode[0], patch_pred_groups)))\n",
    "    return image_preds\n",
    "\n",
    "\n",
    "def plot_metrics(run_id, output_dir, model, history, train_dataloader, val_dataloader=None):\n",
    "    \n",
    "    print()\n",
    "    print('Metrics')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Training confusion matrix\n",
    "#     train_patch_labels = get_all_labels(train_generator)\n",
    "#     train_patch_predictions = predict(model, train_generator)\n",
    "    \n",
    "#     print(\"Training Confusion Matrix of Patches\")\n",
    "#     print(\"-\" * 30)\n",
    "#     print_confusion_matrix(train_patch_labels, train_patch_predictions)\n",
    "    \n",
    "#     # Generate prediction label results file\n",
    "#     write_patch_predictions(run_id, 'train', output_dir, train_generator, train_patch_predictions)\n",
    "    \n",
    "    \n",
    "#     print(\"Training Confusion Matrix of Whole Images\")\n",
    "#     print(\"-\" * 30)\n",
    "#     train_whole_image_filenames = get_all_whole_image_filenames(train_generator)\n",
    "#     train_whole_image_labels = get_all_whole_image_labels(train_generator)\n",
    "#     train_whole_image_predictions = get_all_whole_image_predictions(train_patch_predictions)\n",
    "#     print_confusion_matrix(train_whole_image_labels, train_whole_image_predictions)\n",
    "    \n",
    "#     write_whole_image_predictions(run_id, 'train', output_dir, \n",
    "#                                   train_whole_image_filenames, \n",
    "#                                   train_whole_image_labels, \n",
    "#                                   train_whole_image_predictions)\n",
    "    \n",
    "#     # Validation confusion matrix\n",
    "#     if val_generator is not None:\n",
    "#         val_patch_labels = get_all_labels(val_dataloader)\n",
    "#         val_patch_predictions = predict(model, val_dataloader)\n",
    "        \n",
    "#         print(\"Validation Confusion Matrix of Patches\")\n",
    "#         print(\"-\" * 30)\n",
    "#         print_confusion_matrix(val_patch_labels, val_patch_predictions)\n",
    "        \n",
    "#         print(\"Validation Confusion Matrix of Whole Images\")\n",
    "#         print(\"-\" * 30)\n",
    "#         val_whole_image_filenames = get_all_whole_image_filenames(val_dataloader)\n",
    "#         val_whole_image_labels = get_all_whole_image_labels(val_dataloader)\n",
    "#         val_whole_image_predictions = get_all_whole_image_predictions(val_patch_predictions)\n",
    "#         print_confusion_matrix(val_whole_image_labels, val_whole_image_predictions)\n",
    "        \n",
    "#         # Generate prediction label results file\n",
    "#         write_patch_predictions(run_id, 'val', output_dir, val_dataloader, val_patch_predictions)\n",
    "        \n",
    "#         write_whole_image_predictions(run_id, 'val', output_dir, \n",
    "#                                   val_whole_image_filenames, \n",
    "#                                   val_whole_image_labels, \n",
    "#                                   val_whole_image_predictions)\n",
    "    \n",
    "\n",
    "\n",
    "def train_and_test(run_id, model, group, num_epochs, output_dir):    \n",
    "    model_trained, history = train(run_id, model, group, num_epochs)\n",
    "    \n",
    "    # Save weights\n",
    "    model_trained.save_weights(os.path.join(output_dir, \"{}_weights.h5\".format(run_id)))\n",
    "    \n",
    "    # Plot history metrics\n",
    "    plot_metrics(run_id, output_dir, model_trained, history, group.train_generator, group.val_generator)\n",
    "    \n",
    "    # Classify test data\n",
    "    return predict(model_trained, group.test_generator)\n",
    "\n",
    "\n",
    "def predict(model, data_generator):\n",
    "    y_hat_logits = model.predict(data_generator)\n",
    "    y_hat = tf.map_fn(lambda x: tf.argmax(x), y_hat_logits, dtype=tf.int64)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "\n",
    "def predict_whole_images(patch_predictions, rows, columns, csvfile):\n",
    "    y_hat_test = patch_predictions\n",
    "    y_hat_patch_groups = np.split(y_hat_test, int(len(y_hat_test)/(rows * columns)))\n",
    "    y_hat_whole_images = list(map(lambda x: stats.mode(x).mode[0], y_hat_patch_groups))\n",
    "\n",
    "    for k, v in sorted(Counter(y_hat_whole_images).items()): \n",
    "        print(str(k) + ': '+ str(v))    \n",
    "\n",
    "    one_hots = [np.zeros((5,1)) for pred in y_hat_whole_images]\n",
    "    for i in range(len(one_hots)):\n",
    "        pred = y_hat_whole_images[i]  # the index of the one-hot encoding\n",
    "        one_hots[i][pred] = 1\n",
    "    with open(csvfile, 'w') as predictions_file:\n",
    "        writer = csv.writer(predictions_file)\n",
    "        for pred in one_hots:\n",
    "            pred = np.array(pred, dtype=int)\n",
    "            writer.writerow(pred.T.tolist()[0])\n",
    "    print('Finished generating predictions to', csvfile)\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y, y_hat):\n",
    "    confusion_matrix = np.zeros((5, 5))\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ground_truth = y==labels[i]\n",
    "            prediction = y_hat==labels[j]\n",
    "            confusion_matrix[i, j] = sum(np.bitwise_and(ground_truth, prediction))\n",
    "    df = pd.DataFrame(confusion_matrix, dtype=int)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def write_patch_predictions(run_id, phase, output_dir, dataloader, patch_predictions):\n",
    "    df = pd.DataFrame(columns=['file_name', 'annotation', 'prediction'])\n",
    "    i = 0\n",
    "    for file_names, _, labels in dataloader:\n",
    "        for j in range(BATCH_SIZE):\n",
    "            if len(file_names) > j:\n",
    "                df = df.append({'file_name': file_names[j], \n",
    "                                'annotation': labels[j].item(), \n",
    "                                'prediction': patch_predictions[i]}, ignore_index=True)\n",
    "                i += 1\n",
    "\n",
    "    df.to_csv(os.path.join(output_dir, \"{}_{}_patch_predictions.csv\".format(run_id, phase)), index=False)\n",
    "    \n",
    "\n",
    "def write_whole_image_predictions(run_id, phase, output_dir, filenames, labels, predictions):\n",
    "    print('filenames:', len(filenames))\n",
    "    print('labels:', len(labels))\n",
    "    print('predictions:', len(predictions))\n",
    "    \n",
    "    df = pd.DataFrame(columns=['file_name', 'annotation', 'prediction'])\n",
    "    for i in range(len(filenames)):\n",
    "        df = df.append({'file_name': filenames[i], \n",
    "                        'annotation': labels[i], \n",
    "                        'prediction': predictions[i]}, ignore_index=True)\n",
    "        \n",
    "    df.to_csv(os.path.join(output_dir, \"{}_{}_whole_image_predictions.csv\".format(run_id, phase)), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "The following hyperparameters can be tuned:\n",
    "1. `n1` - Number of neurons in the first classifier dense layer\n",
    "2. `n2` - Number of neurons in the second classifier dense layer\n",
    "3. `d` - Dropout rate after classifier dense layers\n",
    "4. class weights - `[1,1,1,1,1]` (default) or `[1,1,5,5,1]`\n",
    "5. batch normalization - `no` or `yes`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(name, model, class_weights=None, num_epochs=40):\n",
    "    run_id = shortuuid.uuid()\n",
    "    \n",
    "    # output directory\n",
    "    output_dir = os.path.join(\"output_tf\", run_id)\n",
    "    os.makedirs(output_dir)\n",
    "    print(\"Output generated to:\", output_dir)\n",
    "    \n",
    "    \n",
    "    y_hat_test = train_and_test(run_id, model, train_val_split_group(class_weights), num_epochs, output_dir)\n",
    "    predictions_file = os.path.join(output_dir, \"{}_predict_c2_{}_test.csv\".format(run_id, name))\n",
    "    print('predictions file:', predictions_file)\n",
    "    predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)\n",
    "    return y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading annotations...\n",
      "Computing class weights...\n",
      "{0: 0.4205128205128205, 1: 1.1081081081081081, 2: 1.5769230769230769, 3: 1.561904761904762, 4: 2.2465753424657535}\n",
      "Defining train data generator...\n",
      "Found 20500 validated image filenames belonging to 5 classes.\n",
      "Defining validation data generator...\n",
      "Found 5125 validated image filenames belonging to 5 classes.\n",
      "Found 5000 images belonging to 1 classes.\n",
      "predictions file: output_tf/AJr5eR3kg6UEQAUdUnVjJd/AJr5eR3kg6UEQAUdUnVjJd_predict_c2_h1_test.csv\n",
      "0: 68\n",
      "1: 56\n",
      "2: 16\n",
      "3: 5\n",
      "4: 55\n",
      "Finished generating predictions to output_tf/AJr5eR3kg6UEQAUdUnVjJd/AJr5eR3kg6UEQAUdUnVjJd_predict_c2_h1_test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5000,), dtype=int64, numpy=array([2, 2, 2, ..., 0, 0, 0])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = \"AJr5eR3kg6UEQAUdUnVjJd\"\n",
    "name = \"h1\"\n",
    "group = train_val_split_group(None)\n",
    "output_dir = \"output_tf/AJr5eR3kg6UEQAUdUnVjJd\"\n",
    "y_hat_test = predict(model, group.test_generator)\n",
    "predictions_file = os.path.join(output_dir, \"{}_predict_c2_{}_test.csv\".format(run_id, name))\n",
    "print('predictions file:', predictions_file)\n",
    "predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)\n",
    "y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1: 1024-128-5\n",
    "\n",
    "* DNN Structure: 1024-128-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output generated to: output_tf/AJr5eR3kg6UEQAUdUnVjJd\n",
      "Reading annotations...\n",
      "Computing class weights...\n",
      "{0: 0.4205128205128205, 1: 1.1081081081081081, 2: 1.5769230769230769, 3: 1.561904761904762, 4: 2.2465753424657535}\n",
      "Defining train data generator...\n",
      "Found 20500 validated image filenames belonging to 5 classes.\n",
      "Defining validation data generator...\n",
      "Found 5125 validated image filenames belonging to 5 classes.\n",
      "Found 1800 images belonging to 1 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, None, None, 512)   14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 15,371,845\n",
      "Trainable params: 657,157\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Fitting model...\n",
      "group.train_patch_ann_df.shape[0]= 20500\n",
      "int(group.train_patch_ann_df.shape[0] / BATCH_SIZE) 640\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 640 steps, validate for 160 steps\n",
      "Epoch 1/20\n",
      "640/640 [==============================] - 51s 80ms/step - loss: 1.0485 - accuracy: 0.5318 - val_loss: 0.8103 - val_accuracy: 0.6201\n",
      "Epoch 2/20\n",
      "640/640 [==============================] - 48s 75ms/step - loss: 0.8553 - accuracy: 0.6223 - val_loss: 0.7490 - val_accuracy: 0.6416\n",
      "Epoch 3/20\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.7935 - accuracy: 0.6499 - val_loss: 0.7061 - val_accuracy: 0.7027\n",
      "Epoch 4/20\n",
      "640/640 [==============================] - 48s 75ms/step - loss: 0.7374 - accuracy: 0.6757 - val_loss: 0.6569 - val_accuracy: 0.7289\n",
      "Epoch 5/20\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.7097 - accuracy: 0.6859 - val_loss: 0.6625 - val_accuracy: 0.6826\n",
      "Epoch 6/20\n",
      "640/640 [==============================] - 48s 74ms/step - loss: 0.6757 - accuracy: 0.7000 - val_loss: 0.6467 - val_accuracy: 0.7063\n",
      "Epoch 7/20\n",
      "640/640 [==============================] - 48s 75ms/step - loss: 0.6443 - accuracy: 0.7180 - val_loss: 0.6355 - val_accuracy: 0.7096\n",
      "Epoch 8/20\n",
      "640/640 [==============================] - 48s 75ms/step - loss: 0.6203 - accuracy: 0.7264 - val_loss: 0.6185 - val_accuracy: 0.7113\n",
      "Epoch 9/20\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.6013 - accuracy: 0.7360 - val_loss: 0.6354 - val_accuracy: 0.7109\n",
      "Epoch 10/20\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.5808 - accuracy: 0.7439 - val_loss: 0.6080 - val_accuracy: 0.7252\n",
      "Epoch 11/20\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.5691 - accuracy: 0.7502 - val_loss: 0.5844 - val_accuracy: 0.7293\n",
      "Epoch 12/20\n",
      "640/640 [==============================] - 48s 75ms/step - loss: 0.5478 - accuracy: 0.7610 - val_loss: 0.5956 - val_accuracy: 0.7463\n",
      "Epoch 13/20\n",
      "640/640 [==============================] - 50s 78ms/step - loss: 0.5317 - accuracy: 0.7611 - val_loss: 0.5779 - val_accuracy: 0.7592\n",
      "Epoch 14/20\n",
      "640/640 [==============================] - 48s 76ms/step - loss: 0.5206 - accuracy: 0.7678 - val_loss: 0.5827 - val_accuracy: 0.7686\n",
      "Epoch 15/20\n",
      "640/640 [==============================] - 51s 80ms/step - loss: 0.5050 - accuracy: 0.7759 - val_loss: 0.5945 - val_accuracy: 0.7258\n",
      "Epoch 16/20\n",
      "640/640 [==============================] - 50s 78ms/step - loss: 0.4869 - accuracy: 0.7813 - val_loss: 0.5737 - val_accuracy: 0.7619\n",
      "Epoch 17/20\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.4803 - accuracy: 0.7830 - val_loss: 0.5621 - val_accuracy: 0.7348\n",
      "Epoch 18/20\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.4646 - accuracy: 0.7877 - val_loss: 0.5741 - val_accuracy: 0.7604\n",
      "Epoch 19/20\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.4542 - accuracy: 0.7963 - val_loss: 0.5894 - val_accuracy: 0.7865\n",
      "Epoch 20/20\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.4525 - accuracy: 0.7961 - val_loss: 0.6095 - val_accuracy: 0.7227\n",
      "\n",
      "Metrics\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXRddb338fe3mU7mZuqQzmAtbbGlJVYExSqCxQsURaRcRKhIRYSFPj4C4oAXedZyePC68Fal3AuIEyhcsCqDgPjgVaYUytAylbbQNIWmSZo2zZx8nz/OTjhNk/a0yT4nzf681jore/hln292T3+fc357n73N3RERkegak+4CREQkvRQEIiIRpyAQEYk4BYGISMQpCEREIk5BICIScaEFgZndYmbbzezFQdabmd1oZhvM7HkzWxhWLSIiMrgwPxHcBizZz/pTgZnBYwXwsxBrERGRQYQWBO7+GNCwnyZLgds97glgrJlNDKseEREZWGYan3sSsCVhviZYtq1/QzNbQfxTA/n5+cceddRRKSlQRGS0WLNmzQ53rxhoXTqDIGnuvgpYBVBVVeXV1dVprkhE5PBiZm8Mti6dZw1tBaYkzE8OlomISAqlMwhWA58Nzh46Dmhy932GhUREJFyhDQ2Z2W+BxUC5mdUA1wJZAO7+c+A+4OPABqAFWB5WLSIiMrjQgsDdzz3Aege+FNbzi4hIcvTNYhGRiFMQiIhEnIJARCTiFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGISMQpCEREIk5BICIScQoCEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTiFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4kINAjNbYmavmNkGM7t6gPVTzexRM3vWzJ43s4+HWY+IiOwrtCAwswxgJXAqMAc418zm9Gv2TeB37r4AWAb8NKx6RERkYJkhbnsRsMHdNwKY2R3AUmB9QhsHioLpYqA2xHpERNLC3Wlq7aSz2wEwAwvWmVnCNPTNWXw+mMTMyM4YQ3bm8L9/DzMIJgFbEuZrgPf1a/Md4C9mdjmQD3x0oA2Z2QpgBcDUqVOHvVARkaFo6+ymdmcrtTvbqN3ZytadrdTubGVb0zvz7V09Q36e6888ms8cN20YKt5bmEGQjHOB29z9BjN7P/BLMzva3ffaY+6+ClgFUFVV5WmoU0QiqL2rm12tXTS1dtLU2slbTQN39PV7Ovb6PTMYV5jDxOJcZk8s4qTZ45hQnBt/N+9ObyfmHv+0APHhkWAymH6nq+udXDi1JJS/M8wg2ApMSZifHCxLdBGwBMDdHzezGFAObA+xLhEZZdyd7h6ns9vp6O6hs7uHrm6ns7unb76jq4fdbV3sCjr1XW2dfZ18fLp3+TttBnsXn5+dQeXYXCrH5nL0pGImjY31zU8am8v4olgoQzhhCTMIngZmmtkM4gGwDPjXfm3eBE4CbjOz2UAMqAuxJhEZQdq7umlq6aSxpZPGlg52tnSys6WDxpZOdrZ2sHNPsLw13lG3d8U79M7u3sc7Hb8fwlhBxhijKJZJUW4WRbEsinOzmFAcoyiWRVFufD5x/YTieIdfFMvEegfwR4HQgsDdu8zsMuBBIAO4xd3Xmdl1QLW7rwa+CtxsZl8h/mnoQvdD+ecUkZGiq7uHuuZ2tjW19Q2lvNXUxlu72hI6+3gH39LRPeh2sjPGMDYvi5K8bIrzsphamkcsK4OsjDFkZxpZGWP6HtkZwXxmv/lgWe98QU4mxXlZfR19fnbGqOrQD5Udbv1uVVWVV1dXp7sMkUjq7O7h7V3xDr63o9/W1Ma2pta++e272+jp163EssYwoShGaX42JXnZjM3LDjr5rITp+M+xedmU5GWRm6VOejiZ2Rp3rxpoXboPFotImnX3OA17OtjR3P7OY3d8vq65nR3NHdTtfmdd//eOedkZTAyGTGaOK2dicYwJxbnBzxgTi2MU52apUx/BFAQiI1hndw9bGlrYXL+HmsZWurrfOcME9j6zJNE7Z5+8s76rx2nc07FPx96wp2Ofd/AA2ZljqCjIobwgm8riGPMmFfd17BPHvtPRF+aMrvHyKFIQiKRZT49T29TKph172LxjDxuDn5t27GFLYyvdA/XShyiWNYbyghzKC3KYXJLHgqlj4519YU7f8vKCbMoLc9TBR4iCQCQFurp7qN/T0dfBb6rfw6a6PWyu38Pm+hY6Ek5TzM3KYHp5PnMrizltXiXTy/OZUZ7PlNJccjIy3tlo77dObZ9FwXLba9kYM2JZY9S5yz4UBCKHwN1pbu+ivrmD+j3xcfT65g7qm9up39NBXXN7fLq5g/o9HTS2dOw1tp6VYUwry2d6WT6LZ41jelm8s59Rns/4ohx11pJSCgKRBO7OzpZO3toVP93xreBMmLeD+d7Ofseejr3exScqimVSXpBDWUE2R1YUsGhGNmUFOVQUZDO1LJ8jyvOpHJtLxhh19jIyKAgkMrq6e9i+u32fDn5bcI5772mR/b9NagblBTmML8qhoiCHWRMKKSvIpjw/3tmXFeRQlp9NeUEOJflZ5GRmDFKByMikIJBRw93Z0dzBmw0t1DS28GZ9C282tLClsYUtDa1sa2rd5+yY7IwxjC/OYWJRLvMmj+Vjc2OML4oxoSh+RsyE4hjjCnPIyjh8LhcgcrAUBHJYaenoYktDK1saEjv5YLqhldbOvb+pWlGYw9TSPBbNKGVySS4Ti3OZUJzD+KIYE4tzKcnT+e0iCgIZMXqHbrY1xS/nm/iz9yqPO5r3vspjfnYGU0rzmFaWzwfeVcHU0lymluUxpSSPySV55GZrmEbkQBQEkjI7mtv7rtme2Llva2pj285W3t7dvs858/nZGX1fXpozsYgppXnxR0kuU0vzKM3P1jt6kSFSEEgo3J3N9S08tameJzc28OSmBrbubN2rTXbmGCqL40M0xx1ZRmVxLhPHxvp+TiwefVd5FBmJFAQyLNyd17Y38+SmBp7cWM9TmxrYvrsdgLL8bBbNKGX5CdOZWppHZfAOX+/mRUYGBYEcku4e56Vtu3hqUwNPbqrn6c2NNAR3aZpQFOP9R5axaEYp75tRxpEV+erwRUYwBYEkpb2rm/W1vR1/A09vbmB3WxcAU0pz+chR41g0o5TjZpQxpTRXHb/IYURBIPvY1dbJ+tpdrKvdxbraJtbX7mLD9ma6ggO5R1Tkc9q8St43o5RFM0qpHJub5opFZCgUBBHm7mzf3d7X2a8LHm82tPS1qSjMYW5l/ObbR1cWUzW9lIrCnDRWLSLDTUEQEV3dPWxpbGVdbVNfh7++tmmv8/Knl+XxnknFnPPeKcypLGJuZRHjCmNprFpEUkFBMIq0d3WzpaGVN+r38EZ9C28ElziOf+u2pW9oJ3OMMXN8IYtnjWNuZRFzK4uZPbGQwlhWmv8CEUkHBcFhpqWjq6+Tf6O+hc0J07VNrXtd6rgwJ5Np5XnMmVjEqUdPYHp5PnMmFjFzfIEujCYifRQEI1xXdw+Pb6znD2treezVur5z83uV5WcztSx+LZ1pZXnBI59p+tatiCRJQTACuTvPvLmTPz5Xy5+er2VHcweFOZl8ZPY43j2+kOll+Uwry2NqWR5FGs4RkSFSEIwgr7y1m9XPbWX1c7VsaWglO3MMH509jjPmT2LxrApiWRrOEZHhpyBIsy0NLax+rpY/PlfLy2/tJmOMccK7yvnySe/mlLnjdQBXREKnIEiDHc3t/Pn5bax+rpY1bzQCcOy0Eq5bOpePv2ci5QU6T19EUkdBkCLN7V088OJb/GHtVv75ej3dPc5REwq5csksTp9XyZTSvHSXKCIRpSAIUU+P88TGeu5aU8P9L75Fa2c3k0tyueRDR3DG/EnMmlCY7hJFRBQEYXijfg93r6nh7me2snVnK4WxTM5cMIlPHTuJhVNLdEqniIwoCoJh0tzexX3Pb+OuNTU8tbkBM/jgzAquOvUoTpkzXmf8iMiIpSAYgoGGfo6oyOfKJbP4xIJJTCzWVTlFZORTEByCgYZ+PrFwEp86djILpozV0I+IHFYUBEnq6Orh3me3auhHREYdBUES3J2v3LmWP7+wTUM/IjLqKAiSsPLRDfz5hW1cuWQWX/zQkRr6EZFRZUyYGzezJWb2ipltMLOrB2nzaTNbb2brzOw3YdZzKB5a/zb/9y+vcuYxlQoBERmVQvtEYGYZwErgZKAGeNrMVrv7+oQ2M4GvAye4e6OZjQurnkPx2tu7+cqda5k3uZjvnTVPISAio1KYnwgWARvcfaO7dwB3AEv7tbkYWOnujQDuvj3Eeg7KzpYOPn97NbGsDG46/1gdDBaRUSvMIJgEbEmYrwmWJXo38G4z+4eZPWFmSwbakJmtMLNqM6uuq6sLqdx3dHX3cPlvn6V2Zys3nb9QB4VFZFQL9RhBEjKBmcBi4FzgZjMb27+Ru69y9yp3r6qoqAi9qO/d/zJ/f20H1595NMdOKw39+URE0umAQWBmp5vZoQTGVmBKwvzkYFmiGmC1u3e6+ybgVeLBkDZ3r6nhP/9nExceP51z3js1naWIiKREMh38OcBrZvYDMzvqILb9NDDTzGaYWTawDFjdr829xD8NYGblxIeKNh7EcwyrZ99s5Ov3vMD7jyjjG/8yO11liIik1AGDwN0/AywAXgduM7PHgzH7/V5D2d27gMuAB4GXgN+5+zozu87MzgiaPQjUm9l64FHga+5eP4S/55C9vauNL/xyDeOLcvjpeQvJykj3qJmISGqYuyfX0KwMOB/4MvGO/V3Aje7+k/DK21dVVZVXV1cP6zbbOrtZtuoJXn17N/996fEcNaFoWLcvIpJuZrbG3asGWpfMMYIzzOwe4G9AFrDI3U8F5gNfHc5C08Hd+cY9L7J2y05+9On5CgERiZxkvlB2FvDv7v5Y4kJ3bzGzi8IpK3Vu+cdm7n6mhitOmsmSoyemuxwRkZRLJgi+A2zrnTGzXGC8u29290fCKiwV/ue1HfyfP6/nY3PHc8VJaT1ZSUQkbZI5Ivp7oCdhvjtYdlh7o34PX/rNM8wcV8gNnz6GMWN0+QgRiaZkgiAzuEQEAMF0dnglha+5vYuLb6/GDG7+bBUFOboIq4hEVzJBUJdwuidmthTYEV5J4erpid9b4PW6Paz814VMLctLd0kiImmVzFvhS4Bfm9l/AEb8+kGfDbWqEP344Vd5aP3bXHv6HE54V3m6yxERSbsDBoG7vw4cZ2YFwXxz6FWF5L4XtnHjXzfw6arJXHj89HSXIyIyIiQ1OG5m/wLMBWK91+R39+tCrGvYra/dxVd/9xwLpo7lu2cerXsLiIgEkvlC2c+JX2/ocuJDQ2cD00Kua9iteaOBsXlZ3PSZY8nJ1L0FRER6HfASE2b2vLvPS/hZANzv7h9MTYl7G8olJprbu3SGkIhE0pAuMQG0BT9bzKwS6AQOy6/gKgRERPaVTM/4x+BmMT8EngEcuDnUqkREJGX2GwTBDWkecfedwN1m9icg5u5NKalORERCt9+hIXfvAVYmzLcrBERERpdkjhE8YmZnmc63FBEZlZIJgi8Qv8hcu5ntMrPdZrYr5LpERCRFkvlm8X5vSSkiIoe3AwaBmZ040PL+N6oREZHDUzKnj34tYToGLALWAB8JpSIREUmpZIaGTk+cN7MpwI9Dq0hERFIqmYPF/dUAs4e7EBERSY9kjhH8hPi3iSEeHMcQ/4axiIiMAskcI0i8wlsX8Ft3/0dI9YiISIolEwR3AW3u3g1gZhlmlufuLeGWJiIiqZDUN4uB3IT5XODhcMoREZFUSyYIYom3pwymdcd3EZFRIpkg2GNmC3tnzOxYoDW8kkREJJWSOUbwZeD3ZlZL/FaVE4jfulJEREaBZL5Q9rSZHQXMCha94u6d4ZYlIiKpkszN678E5Lv7i+7+IlBgZpeGX5qIiKRCMscILg7uUAaAuzcCF4dXkoiIpFIyQZCReFMaM8sAssMrSUREUimZg8UPAHea2U3B/BeA+8MrSUREUimZILgKWAFcEsw/T/zMIRERGQUOODQU3MD+SWAz8XsRfAR4KZmNm9kSM3vFzDaY2dX7aXeWmbmZVSVXtoiIDJdBPxGY2buBc4PHDuBOAHf/cDIbDo4lrAROJn7p6qfNbLW7r+/XrhC4gnjYiIhIiu3vE8HLxN/9n+buH3D3nwDdB7HtRcAGd9/o7h3AHcDSAdp9F/g+0HYQ2xYRkWGyvyD4JLANeNTMbjazk4h/szhZk4AtCfM1wbI+waUrprj7n/e3ITNbYWbVZlZdV1d3ECWIiMiBDBoE7n6vuy8DjgIeJX6piXFm9jMzO2WoT2xmY4AfAV89UFt3X+XuVe5eVVFRMdSnFhGRBMkcLN7j7r8J7l08GXiW+JlEB7IVmJIwPzlY1qsQOBr4m5ltBo4DVuuAsYhIah3UPYvdvTF4d35SEs2fBmaa2QwzywaWAasTttXk7uXuPt3dpwNPAGe4e/XAmxMRkTAcys3rk+LuXcBlwIPETzf9nbuvM7PrzOyMsJ5XREQOTjJfKDtk7n4fcF+/Zd8epO3iMGsREZGBhfaJQEREDg8KAhGRiFMQiIhEnIJARCTiFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGISMQpCEREIk5BICIScQoCEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTiFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYgLNQjMbImZvWJmG8zs6gHW/y8zW29mz5vZI2Y2Lcx6RERkX6EFgZllACuBU4E5wLlmNqdfs2eBKnefB9wF/CCsekREZGBhfiJYBGxw943u3gHcASxNbODuj7p7SzD7BDA5xHpERGQAYQbBJGBLwnxNsGwwFwH3D7TCzFaYWbWZVdfV1Q1jiSIiMiIOFpvZZ4Aq4IcDrXf3Ve5e5e5VFRUVqS1ORGSUywxx21uBKQnzk4NlezGzjwLfAD7k7u0h1iMiIgMI8xPB08BMM5thZtnAMmB1YgMzWwDcBJzh7ttDrEVERAYRWhC4exdwGfAg8BLwO3dfZ2bXmdkZQbMfAgXA781srZmtHmRzIiISkjCHhnD3+4D7+i37dsL0R8N8fhERObARcbBYRETSR0EgIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGISMQpCEREIk5BICIScQoCEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJuFDvRyAicrA6Ozupqamhra0t3aUclmKxGJMnTyYrKyvp31EQiMiIUlNTQ2FhIdOnT8fM0l3OYcXdqa+vp6amhhkzZiT9exoaEpERpa2tjbKyMoXAITAzysrKDvrTlIJAREYchcChO5R9pyAQEYk4BYGISMQpCERE0qCrqyvdJfTRWUMiMmL92x/Xsb5217Buc05lEdeePne/bc4880y2bNlCW1sbV1xxBStWrOCBBx7gmmuuobu7m/Lych555BGam5u5/PLLqa6uxsy49tprOeussygoKKC5uRmAu+66iz/96U/cdtttXHjhhcRiMZ599llOOOEEli1bxhVXXEFbWxu5ubnceuutzJo1i+7ubq666ioeeOABxowZw8UXX8zcuXO58cYbuffeewF46KGH+OlPf8o999wz5H2iIBAR6eeWW26htLSU1tZW3vve97J06VIuvvhiHnvsMWbMmEFDQwMA3/3udykuLuaFF14AoLGx8YDbrqmp4Z///CcZGRns2rWLv//972RmZvLwww9zzTXXcPfdd7Nq1So2b97M2rVryczMpKGhgZKSEi699FLq6uqoqKjg1ltv5XOf+9yw/L0KAhEZsQ70zj0sN954Y9877S1btrBq1SpOPPHEvnPzS0tLAXj44Ye54447+n6vpKTkgNs+++yzycjIAKCpqYkLLriA1157DTOjs7Ozb7uXXHIJmZmZez3f+eefz69+9SuWL1/O448/zu233z4sf6+CQEQkwd/+9jcefvhhHn/8cfLy8li8eDHHHHMML7/8ctLbSDyFs/85/fn5+X3T3/rWt/jwhz/MPffcw+bNm1m8ePF+t7t8+XJOP/10YrEYZ599dl9QDJUOFouIJGhqaqKkpIS8vDxefvllnnjiCdra2njsscfYtGkTQN/Q0Mknn8zKlSv7frd3aGj8+PG89NJL9PT07HcMv6mpiUmTJgFw22239S0/+eSTuemmm/oOKPc+X2VlJZWVlVx//fUsX7582P5mBYGISIIlS5bQ1dXF7NmzufrqqznuuOOoqKhg1apVfPKTn2T+/Pmcc845AHzzm9+ksbGRo48+mvnz5/Poo48C8L3vfY/TTjuN448/nokTJw76XFdeeSVf//rXWbBgwV5nEX3+859n6tSpzJs3j/nz5/Ob3/ymb915553HlClTmD179rD9zebuw7axVKiqqvLq6up0lyEiIXnppZeGtZMbbS677DIWLFjARRddNGibgfahma1x96qB2usYgYjIYeLYY48lPz+fG264YVi3qyAQETlMrFmzJpTt6hiBiIw4h9uQ9UhyKPtOQSAiI0osFqO+vl5hcAh670cQi8UO6vc0NCQiI8rkyZOpqamhrq4u3aUclnrvUHYwFAQiMqJkZWUd1N21ZOhCHRoysyVm9oqZbTCzqwdYn2NmdwbrnzSz6WHWIyIi+wotCMwsA1gJnArMAc41szn9ml0ENLr7u4B/B74fVj0iIjKwMD8RLAI2uPtGd+8A7gCW9muzFPhFMH0XcJLpHnUiIikV5jGCScCWhPka4H2DtXH3LjNrAsqAHYmNzGwFsCKYbTazVw6xpvL+2x5hVN/QqL6hG+k1qr5DN22wFYfFwWJ3XwWsGup2zKx6sK9YjwSqb2hU39CN9BpVXzjCHBraCkxJmJ8cLBuwjZllAsVAfYg1iYhIP2EGwdPATDObYWbZwDJgdb82q4ELgulPAX91fYtERCSlQhsaCsb8LwMeBDKAW9x9nZldB1S7+2rgv4BfmtkGoIF4WIRpyMNLIVN9Q6P6hm6k16j6QnDYXYZaRESGl641JCIScQoCEZGIG5VBMJIvbWFmU8zsUTNbb2brzOyKAdosNrMmM1sbPL6dqvqC599sZi8Ez73P7eAs7sZg/z1vZgtTWNushP2y1sx2mdmX+7VJ+f4zs1vMbLuZvZiwrNTMHjKz14KfJYP87gVBm9fM7IKB2oRQ2w/N7OXg3+8eMxs7yO/u97UQco3fMbOtCf+OHx/kd/f7/z3E+u5MqG2zma0d5HdTsg+HxN1H1YP4genXgSOAbOA5YE6/NpcCPw+mlwF3prC+icDCYLoQeHWA+hYDf0rjPtwMlO9n/ceB+wEDjgOeTOO/9VvAtHTvP+BEYCHwYsKyHwBXB9NXA98f4PdKgY3Bz5JguiQFtZ0CZAbT3x+otmReCyHX+B3gfyfxGtjv//ew6uu3/gbg2+nch0N5jMZPBCP60hbuvs3dnwmmdwMvEf+G9eFkKXC7xz0BjDWzwe/QHZ6TgNfd/Y00PPde3P0x4me+JUp8nf0COHOAX/0Y8JC7N7h7I/AQsCTs2tz9L+7ee7f0J4h/zydtBtl/yUjm//uQ7a++oO/4NPDb4X7eVBmNQTDQpS36d7R7XdoC6L20RUoFQ1ILgCcHWP1+M3vOzO43s7kpLQwc+IuZrQku79FfMvs4FZYx+H++dO6/XuPdfVsw/RYwfoA2I2Fffo74J7yBHOi1ELbLguGrWwYZWhsJ+++DwNvu/tog69O9Dw9oNAbBYcHMCoC7gS+7+65+q58hPtwxH/gJcG+Ky/uAuy8kfuXYL5nZiSl+/gMKvqR4BvD7AVane//tw+NjBCPuXG0z+wbQBfx6kCbpfC38DDgSOAbYRnz4ZSQ6l/1/Ghjx/59GYxCM+EtbmFkW8RD4tbv/d//17r7L3ZuD6fuALDMrT1V97r41+LkduIf4x+9EyezjsJ0KPOPub/dfke79l+Dt3iGz4Of2AdqkbV+a2YXAacB5QVDtI4nXQmjc/W1373b3HuDmQZ47ra/FoP/4JHDnYG3SuQ+TNRqDYERf2iIYT/wv4CV3/9EgbSb0HrMws0XE/51SElRmlm9mhb3TxA8qvtiv2Wrgs8HZQ8cBTQlDIKky6LuwdO6/fhJfZxcAfxigzYPAKWZWEgx9nBIsC5WZLQGuBM5w95ZB2iTzWgizxsTjTp8Y5LmT+f8epo8CL7t7zUAr070Pk5buo9VhPIif1fIq8bMJvhEsu474ix4gRnxIYQPwFHBECmv7APEhgueBtcHj48AlwCVBm8uAdcTPgHgCOD6F9R0RPO9zQQ29+y+xPiN+06HXgReAqhT/++YT79iLE5aldf8RD6VtQCfxceqLiB93egR4DXgYKA3aVgH/mfC7nwteixuA5SmqbQPxsfXe12DvWXSVwH37ey2kcP/9Mnh9PU+8c5/Yv8Zgfp//76moL1h+W+/rLqFtWvbhUB66xISISMSNxqEhERE5CAoCEZGIUxCIiEScgkBEJOIUBCIiEacgEOnHzLr7XeF02K5oaWbTE69gKTIShHarSpHDWKu7H5PuIkRSRZ8IRJIUXFf+B8G15Z8ys3cFy6eb2V+Di6M9YmZTg+Xjg2v9Pxc8jg82lWFmN1v8fhR/MbPctP1RIigIRAaS229o6JyEdU3u/h7gP4AfB8t+AvzC3ecRv3jbjcHyG4H/5/GL3y0k/s1SgJnASnefC+wEzgr57xHZL32zWKQfM2t294IBlm8GPuLuG4MLB77l7mVmtoP45Q86g+Xb3L3czOqAye7enrCN6cTvPzAzmL8KyHL368P/y0QGpk8EIgfHB5k+GO0J093oWJ2kmYJA5OCck/Dz8WD6n8SveglwHvD3YPoR4IsAZpZhZsWpKlLkYOidiMi+cvvdiPwBd+89hbTEzJ4n/q7+3Dhr7pkAAABcSURBVGDZ5cCtZvY1oA5YHiy/AlhlZhcRf+f/ReJXsBQZUXSMQCRJwTGCKnffke5aRIaThoZERCJOnwhERCJOnwhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTi/j9Q7kCDdrfL/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions file: output_tf/AJr5eR3kg6UEQAUdUnVjJd/AJr5eR3kg6UEQAUdUnVjJd_predict_c2_h1_test.csv\n",
      "0: 34\n",
      "1: 16\n",
      "4: 22\n",
      "Finished generating predictions to output_tf/AJr5eR3kg6UEQAUdUnVjJd/AJr5eR3kg6UEQAUdUnVjJd_predict_c2_h1_test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1800,), dtype=int64, numpy=array([2, 2, 0, ..., 0, 0, 0])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(1024, 128, 0.5)\n",
    "run_trial(\"h1\", model, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2: 2048-256-5\n",
    "\n",
    "* DNN Structure: 2048-256-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model(2048, 256, 0.5)\n",
    "run_trial(\"h2\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H3: 512-64-5\n",
    "\n",
    "* DNN Structure: 512-64-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(512, 64, 0.5)\n",
    "run_trial(\"h3\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H4: Best from above, dropout 0.25\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.25\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(2048, 256, 0.25)\n",
    "run_trial(\"h4\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5: Best from above, dropout 0.1\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.1\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(2048, 256, 0.1)\n",
    "run_trial(\"h5\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H6: Best from above, skewed class weights\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,5,5,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.5)\n",
    "run_trial(\"h6\", model, class_weights=[1., 1., 2., 2., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H7: Best from above, batch normalization\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: \n",
    "* Batch normalization: yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.5, batch_normalization=True).to(device)\n",
    "run_trial(\"h7\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_n1 = 1024\n",
    "optimal_n2 = 128\n",
    "optimal_d = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with all C2 data and optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(optimal_n1, optimal_n2, optimal_d)\n",
    "# model.load_state_dict(torch.load('cnn_pytorch_c2.pt'))\n",
    "y_hat_test = train_and_test(model, group_3(), num_epochs=40)\n",
    "predictions_file = \"predict_c2_{}.csv\".format(shortuuid.uuid())\n",
    "print('predictions file:', predictions_file)\n",
    "predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
