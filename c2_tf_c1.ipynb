{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shortuuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "import shortuuid\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Get reproducible results\n",
    "random_state = 46\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "\n",
    "\n",
    "def ann_file(data_dir):\n",
    "    return os.path.join(data_dir, \"TrainAnnotations.csv\")\n",
    "\n",
    "\n",
    "TRAIN_DATA_DIR = \"data/TrainData-C1\"\n",
    "TRAIN_DATA_ANN_FILE = ann_file(TRAIN_DATA_DIR)\n",
    "\n",
    "TRAIN_SPLIT_DATA_DIR           = \"data-c1/train/split\"\n",
    "TRAIN_SPLIT_ANN_FILE           = ann_file(TRAIN_SPLIT_DATA_DIR)\n",
    "TRAIN_SPLIT_AUGMENTED_DATA_DIR = \"data-c1/train/augmented\"\n",
    "TRAIN_SPLIT_AUGMENTED_ANN_FILE = ann_file(TRAIN_SPLIT_AUGMENTED_DATA_DIR)\n",
    "TRAIN_SPLIT_PATCHES_DATA_DIR   = \"data-c1/train/patches\"\n",
    "TRAIN_SPLIT_PATCHES_ANN_FILE   = ann_file(TRAIN_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TRAIN_ALL_AUGMENTED_DATA_DIR   = \"data-c1/train-all/augmented\"\n",
    "TRAIN_ALL_AUGMENTED_ANN_FILE   = ann_file(TRAIN_ALL_AUGMENTED_DATA_DIR)\n",
    "TRAIN_ALL_PATCHES_DATA_DIR     = \"data-c1/train-all/patches\"\n",
    "TRAIN_ALL_PATCHES_ANN_FILE     = ann_file(TRAIN_ALL_PATCHES_DATA_DIR)\n",
    "\n",
    "VAL_SPLIT_DATA_DIR         = \"data-c1/val/split\"\n",
    "VAL_SPLIT_ANN_FILE         = ann_file(VAL_SPLIT_DATA_DIR)\n",
    "VAL_SPLIT_PATCHES_DATA_DIR = \"data-c1/val/patches\"\n",
    "VAL_SPLIT_PATCHES_ANN_FILE = ann_file(VAL_SPLIT_PATCHES_DATA_DIR)\n",
    "\n",
    "TEST_DATA_DIR         = \"data/TestData/\"\n",
    "\n",
    "TEST_PATCHES_DATA_DIR = \"data/test/\"\n",
    "#TEST_PATCHES_DATA_DIR = \"data/test/patches\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU configuration\n",
    "If you have a GPU, enable experimental memory growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Generate random, stratified 80/20 split for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories for splits already exist. Skipping\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists(TRAIN_SPLIT_DATA_DIR) or os.path.exists(VAL_SPLIT_DATA_DIR)):\n",
    "    print(\"Data directories for splits already exist. Skipping\")\n",
    "else:\n",
    "    # Generate 80/20 split\n",
    "\n",
    "    print(\"Reading {} annotations...\".format(TRAIN_DATA_ANN_FILE))\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "\n",
    "    print(\"Splitting data into training and validation sets...\")\n",
    "    train_df, val_df = train_test_split(ann_df,\n",
    "                                        train_size=0.80,\n",
    "                                        random_state=138,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=ann_df[['annotation']].to_numpy(dtype=np.int32).flatten())\n",
    "\n",
    "    os.makedirs(TRAIN_SPLIT_DATA_DIR)\n",
    "    os.makedirs(VAL_SPLIT_DATA_DIR)\n",
    "    \n",
    "    print(\"Copying files for training split...\")\n",
    "    for _, row in train_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(TRAIN_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating training split annotations...\")\n",
    "    train_df.sort_values('file_name').to_csv(TRAIN_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Copying files for validation split...\")\n",
    "    for _, row in val_df.iterrows():\n",
    "        filename = row['file_name']\n",
    "        src = os.path.join(TRAIN_DATA_DIR, filename)\n",
    "        dest = os.path.join(VAL_SPLIT_DATA_DIR, filename)\n",
    "        shutil.copyfile(src, dest)\n",
    "        \n",
    "    print(\"Generating validation split annotations...\")\n",
    "    val_df.sort_values('file_name').to_csv(VAL_SPLIT_ANN_FILE, index=False)\n",
    "        \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data\n",
    "Because the training dataset is unbalanced, augment the training data set by generating\n",
    "new images for the lower numbered samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DESIRED_CLASS_SAMPLE_COUNT = 400\n",
    "RANDOM_STATE = 13\n",
    "\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "PATCH_ROWS = 5\n",
    "PATCH_COLUMNS = 5\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.endswith(IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def augment_data(src_dir, src_ann_file, dest_dir, dest_ann_file, class_sample_count=500):\n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "    ann_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'}) \n",
    "    new_samples = {}\n",
    "    \n",
    "    for i in range(5):\n",
    "        class_df = ann_df.query(\"annotation == '{}'\".format(i))\n",
    "        num_class_samples = class_df.shape[0]\n",
    "        num_to_create = class_sample_count - num_class_samples\n",
    "            \n",
    "        print(\"Creating {} images for class {}\".format(num_to_create, i))\n",
    "        samples = class_df.sample(n=num_to_create, replace=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "        for idx, row in samples.iterrows():\n",
    "            new_filename = row['file_name'].split('.')[0] + \"_\" + shortuuid.uuid() + \".png\"\n",
    "    \n",
    "            # Apply transformations to each randomly selected sample\n",
    "            img = Image.open(src_dir + \"/\" + row['file_name'])\n",
    "            image_transforms = transforms.Compose([\n",
    "                #transforms.RandomAffine(degrees=20, translate=(0.2, 0.2)),\n",
    "                #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                #transforms.RandomResizedCrop((480, 640), scale=(1.0, 1.2)),\n",
    "                \n",
    "                transforms.RandomRotation((90,90), expand=True),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomHorizontalFlip()\n",
    "            ])\n",
    "            transformed_img = image_transforms(img)\n",
    "            transformed_img.save(os.path.join(dest_dir, new_filename))\n",
    "    \n",
    "            new_samples[new_filename] = row['annotation']\n",
    "    \n",
    "    # Add to new dataframe\n",
    "    balanced_df = pd.read_csv(src_ann_file, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    balanced_df = balanced_df.append(pd.DataFrame.from_records([(k, v) for k, v in new_samples.items()],\n",
    "                                                 columns=['file_name', 'annotation']))\n",
    "    \n",
    "    # Write new annotations\n",
    "    balanced_df.sort_values('file_name').to_csv(dest_ann_file, index=False)\n",
    "    \n",
    "    # Copy images from training data split\n",
    "    for file in glob.glob(src_dir + \"/*\"):\n",
    "        if is_image_file(file):\n",
    "            shutil.copy(file, os.path.join(dest_dir, os.path.basename(file)))\n",
    "\n",
    "\n",
    "def generate_image_patches(img, rows, cols):\n",
    "    \"\"\"\n",
    "    Generates a list of in-memory image overlapping patches\n",
    "    \n",
    "    Args:\n",
    "        rows - number of rows of patchs to cover the height of the image\n",
    "        cols - number of colums of patches to cover the width of the image\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    sizeX = img.shape[1]\n",
    "    sizeY = img.shape[0]\n",
    "    \n",
    "    patch_sizeX = 224\n",
    "    patch_sizeY = 224\n",
    "    patch_relative_centerX = 112\n",
    "    patch_relative_centerY = 112\n",
    "\n",
    "    for i in range(0,rows):\n",
    "        for j in range(0, cols):\n",
    "            center = (patch_relative_centerX + (sizeX - patch_sizeX)/(rows - 1)*i, \n",
    "                      patch_relative_centerY + (sizeY - patch_sizeY)/(cols - 1)*j)\n",
    "            patches.append(cv2.getRectSubPix(img, (patch_sizeX, patch_sizeY), center))\n",
    "            \n",
    "    return patches\n",
    "\n",
    "\n",
    "def generate_patch_files(in_dir, out_dir, rows, cols):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    images = [f for f in os.listdir(in_dir) if os.path.isfile(os.path.join(in_dir, f)) and is_image_file(f)]   \n",
    "    for im in images:\n",
    "        img = cv2.imread(os.path.join(in_dir, im))\n",
    "        patches = generate_image_patches(img, rows, cols)\n",
    "        \n",
    "        for i in range(0,rows):\n",
    "            for j in range(0, cols):\n",
    "                patch = patches[i*rows + j]\n",
    "                patch_name = im.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                cv2.imwrite(out_dir + '/' + patch_name, patch)\n",
    "\n",
    "\n",
    "def generate_patch_annotations_df(df, rows, cols):\n",
    "    patches_ann = {}\n",
    "    \n",
    "    for ind in df.index: \n",
    "        file_name = df['file_name'][ind]\n",
    "        annotation = df['annotation'][ind]\n",
    "        \n",
    "        for i in range(0, rows):\n",
    "            for j in range(0, cols):\n",
    "                patch_name = file_name.split('.')[0] + '_' + str(i) + '_' + str(j) + '.png'\n",
    "                patches_ann[patch_name] = annotation\n",
    "    \n",
    "    return pd.DataFrame.from_records([(k, v) for k, v in patches_ann.items()], \n",
    "                                     columns=['file_name', 'annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run data augmentation\n",
    "\n",
    "Perform the data augmentation on the training data set split to balance the class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented SPLIT training data already exists. Skipping.\n",
      "Augmented ALL training data already exists. Skipping.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TRAIN_SPLIT_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented SPLIT training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for SPLIT training data...\")\n",
    "    augment_data(TRAIN_SPLIT_DATA_DIR,\n",
    "                 TRAIN_SPLIT_ANN_FILE,\n",
    "                 TRAIN_SPLIT_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_SPLIT_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=400)    \n",
    "    print(\"Done.\")\n",
    "\n",
    "if os.path.exists(TRAIN_ALL_AUGMENTED_DATA_DIR):\n",
    "    print(\"Augmented ALL training data already exists. Skipping.\")\n",
    "else:\n",
    "    print(\"Balancing class samples for ALL training data...\")\n",
    "    augment_data(TRAIN_DATA_DIR,\n",
    "                 TRAIN_DATA_ANN_FILE,\n",
    "                 TRAIN_ALL_AUGMENTED_DATA_DIR,\n",
    "                 TRAIN_ALL_AUGMENTED_ANN_FILE,\n",
    "                 class_sample_count=500)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-c1/train/patches exists. Skipping.\n",
      "data-c1/val/patches exists. Skipping.\n",
      "data/test/ exists. Skipping.\n",
      "data-c1/train-all/patches exists. Skipping.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# SPLIT train patches\n",
    "if os.path.exists(TRAIN_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT training data patches...\")\n",
    "    generate_patch_files(TRAIN_SPLIT_AUGMENTED_DATA_DIR, TRAIN_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "    \n",
    "# SPLIT val patches\n",
    "if os.path.exists(VAL_SPLIT_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(VAL_SPLIT_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating SPLIT validation data patches...\")\n",
    "    generate_patch_files(VAL_SPLIT_DATA_DIR, VAL_SPLIT_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating SPLIT validation patch data annotations...\")\n",
    "    image_df = pd.read_csv(VAL_SPLIT_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(VAL_SPLIT_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "# test patches\n",
    "if os.path.exists(TEST_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TEST_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating test data patches...\")\n",
    "    generate_patch_files(TEST_DATA_DIR, TEST_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    \n",
    "# ALL train patches\n",
    "if os.path.exists(TRAIN_ALL_PATCHES_DATA_DIR):\n",
    "    print(\"{} exists. Skipping.\".format(TRAIN_ALL_PATCHES_DATA_DIR))\n",
    "else:\n",
    "    print(\"Generating ALL train data patches...\")\n",
    "    generate_patch_files(TRAIN_ALL_AUGMENTED_DATA_DIR, TRAIN_ALL_PATCHES_DATA_DIR, PATCH_ROWS, PATCH_COLUMNS)\n",
    "\n",
    "    print(\"Generating ALL training patch data annotations...\")\n",
    "    image_df = pd.read_csv(TRAIN_ALL_AUGMENTED_ANN_FILE)\n",
    "    patch_annotations_df = generate_patch_annotations_df(image_df, PATCH_ROWS, PATCH_COLUMNS)\n",
    "    patch_annotations_df.sort_values('file_name').to_csv(TRAIN_ALL_PATCHES_ANN_FILE, index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoybeanDataGroup():\n",
    "    def __init__(self, class_weights, \n",
    "                 train_generator,\n",
    "                 val_generator=None,\n",
    "                 test_generator=None,\n",
    "                 train_patch_ann_df=None,\n",
    "                 val_patch_ann_df=None,\n",
    "                 train_whole_image_ann_df=None,\n",
    "                 val_whole_image_ann_df=None):\n",
    "        self.class_weights = class_weights\n",
    "        self.train_generator = train_generator\n",
    "        self.val_generator = val_generator\n",
    "        self.test_generator = test_generator\n",
    "        self.train_patch_ann_df = train_patch_ann_df\n",
    "        self.val_patch_ann_df = val_patch_ann_df\n",
    "        self.train_whole_image_ann_df = train_whole_image_ann_df\n",
    "        self.val_whole_image_ann_df = val_whole_image_ann_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split_group(class_weights=None):\n",
    "    \n",
    "    train_whole_image_ann_df = pd.read_csv(TRAIN_SPLIT_AUGMENTED_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_whole_image_ann_df   = pd.read_csv(VAL_SPLIT_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    train_ann_df = pd.read_csv(TRAIN_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    val_ann_df   = pd.read_csv(VAL_SPLIT_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(train_ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "        \n",
    "        \n",
    "    data_gen_args = dict(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True,\n",
    "        #rotation_range=10,\n",
    "        #width_shift_range=0.2,\n",
    "        #height_shift_range=0.2,\n",
    "        #horizontal_flip=True\n",
    "    )\n",
    "    train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    val_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "    print(\"Defining train data generator...\")\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_ann_df,\n",
    "            directory=TRAIN_SPLIT_PATCHES_DATA_DIR,\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"annotation\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='sparse',\n",
    "            target_size=(224,224)\n",
    "    )\n",
    "    print(\"Defining validation data generator...\")\n",
    "    val_generator = val_datagen.flow_from_dataframe(\n",
    "            dataframe=val_ann_df,\n",
    "            directory=VAL_SPLIT_PATCHES_DATA_DIR,\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"annotation\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='sparse',\n",
    "            target_size=(224,224)\n",
    "    )\n",
    "        \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory=TEST_PATCHES_DATA_DIR,\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        target_size=(224,224)\n",
    "    )\n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, \n",
    "                            train_generator, \n",
    "                            val_generator, \n",
    "                            test_generator,\n",
    "                            train_ann_df,\n",
    "                            val_ann_df,\n",
    "                            train_whole_image_ann_df, \n",
    "                            val_whole_image_ann_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def all_train_data_group(class_weights=None):\n",
    "    \n",
    "    train_whole_image_ann_df = pd.read_csv(TRAIN_DATA_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})    \n",
    "    \n",
    "    print(\"Reading annotations...\")\n",
    "    ann_df = pd.read_csv(TRAIN_DATA_PATCHES_ANN_FILE, dtype={'file_name': 'object', 'annotation': 'category'})\n",
    "    \n",
    "    if class_weights is None:\n",
    "        print(\"Computing class weights...\")\n",
    "        class_weights = compute_class_weights(ann_df, 'annotation')\n",
    "        print(class_weights)\n",
    "    else:\n",
    "        print(\"Using class weights:\", class_weights)\n",
    "        \n",
    "        \n",
    "    data_gen_args = dict(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True,\n",
    "    )\n",
    "    train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "    print(\"Defining train data generator...\")\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=ann_df,\n",
    "            directory=TRAIN_DATA_PATCHES_DIR,\n",
    "            x_col=\"file_name\",\n",
    "            y_col=\"annotation\",\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='sparse',\n",
    "            target_size=(224,224)\n",
    "    )\n",
    "   \n",
    "        \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization=True\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        directory=TEST_DATA_PATCHES_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        target_size=(224,224)\n",
    "    )  \n",
    "    \n",
    "    \n",
    "    return SoybeanDataGroup(class_weights, \n",
    "                            train_generator, \n",
    "                            None, \n",
    "                            test_generator,\n",
    "                            ann_df,\n",
    "                            None,\n",
    "                            train_whole_image_ann_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This model is based on the VGG16 network with custom classifier layers \n",
    "with the feature layers initialized with weights based on the ImageNet data. \n",
    "\n",
    "The number of neurons and dropout rates in the classifier layers are parameterized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_model(n1, n2, dropout):\n",
    "    vgg_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "    vgg_model.trainable = False\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        vgg_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(n1, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(n2, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(5)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "For training and validation, this trains a model across a configured number of epochs and outputs the training and validation loss and accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def compute_class_weights(df, y_col):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping class labels to 'balanced' weights based on the\n",
    "    frequency of the weights across the labels in the specified dataframe\n",
    "    \"\"\"\n",
    "    y = df[[y_col]].to_numpy().flatten()\n",
    "    weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "    return {label: weight for label, weight in enumerate(weights)}\n",
    "    \n",
    "\n",
    "def train(run_id, model, group, num_epochs):     \n",
    "    model.summary()\n",
    "\n",
    "    print('Fitting model...')\n",
    "    \n",
    "    print('group.train_patch_ann_df.shape[0]=', group.train_patch_ann_df.shape[0])\n",
    "    print('int(group.train_patch_ann_df.shape[0] / BATCH_SIZE)', int(group.train_patch_ann_df.shape[0] / BATCH_SIZE))\n",
    "\n",
    "    history = model.fit(group.train_generator, \n",
    "                        steps_per_epoch=int(group.train_patch_ann_df.shape[0] / BATCH_SIZE), \n",
    "                        epochs=num_epochs,  \n",
    "                        class_weight=group.class_weights,\n",
    "                        validation_data=group.val_generator,\n",
    "                        validation_steps=int(group.val_patch_ann_df.shape[0] / BATCH_SIZE),\n",
    "                        verbose=1)\n",
    "\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def get_all_labels(ann_df):\n",
    "    return ann_df['annotation'].to_numpy(dtype=int)\n",
    "\n",
    "\n",
    "def get_all_whole_image_filenames(whole_image_ann_df):\n",
    "    return whole_image_ann_df['file_name'].to_numpy()\n",
    "    \n",
    "\n",
    "def get_all_whole_image_labels(ann_df):\n",
    "    return get_all_labels(ann_df)\n",
    "\n",
    "\n",
    "def get_all_whole_image_predictions(patch_preds):\n",
    "    patch_pred_groups = np.split(patch_preds, int(len(patch_preds)/(PATCH_ROWS * PATCH_COLUMNS)))\n",
    "    image_preds = np.array(list(map(lambda x: stats.mode(x).mode[0], patch_pred_groups)))\n",
    "    return image_preds\n",
    "\n",
    "\n",
    "def plot_metrics(run_id, output_dir, model, history, group):\n",
    "    \n",
    "    print()\n",
    "    print('Metrics')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='Training loss')\n",
    "    plt.plot(history.history['val_loss'], label = 'Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'Validation accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Training confusion matrix\n",
    "    train_patch_labels = get_all_labels(group.train_patch_ann_df)\n",
    "    train_patch_predictions = predict(model, group.train_generator)\n",
    "    \n",
    "    print(\"Training Confusion Matrix of Patches\")\n",
    "    print(\"-\" * 30)\n",
    "    print_confusion_matrix(train_patch_labels, train_patch_predictions)\n",
    "    \n",
    "    # Generate prediction label results file\n",
    "    write_patch_predictions(run_id, 'train', output_dir, group.train_patch_ann_df, train_patch_predictions)\n",
    "    \n",
    "    \n",
    "    print(\"Training Confusion Matrix of Whole Images\")\n",
    "    print(\"-\" * 30)\n",
    "    train_whole_image_filenames = get_all_whole_image_filenames(group.train_whole_image_ann_df)\n",
    "    train_whole_image_labels = get_all_whole_image_labels(group.train_whole_image_ann_df)\n",
    "    train_whole_image_predictions = get_all_whole_image_predictions(train_patch_predictions)\n",
    "    print_confusion_matrix(train_whole_image_labels, train_whole_image_predictions)\n",
    "    \n",
    "    write_whole_image_predictions(run_id, 'train', output_dir, \n",
    "                                  train_whole_image_filenames, \n",
    "                                  train_whole_image_labels, \n",
    "                                  train_whole_image_predictions)\n",
    "    \n",
    "    # Validation confusion matrix\n",
    "    if group.val_generator is not None:\n",
    "        val_patch_labels = get_all_labels(group.val_patch_ann_df)\n",
    "        val_patch_predictions = predict(model, group.val_generator)\n",
    "        \n",
    "        print(\"Validation Confusion Matrix of Patches\")\n",
    "        print(\"-\" * 30)\n",
    "        print_confusion_matrix(val_patch_labels, val_patch_predictions)\n",
    "      \n",
    "        print(\"Validation Confusion Matrix of Whole Images\")\n",
    "        print(\"-\" * 30)\n",
    "        val_whole_image_filenames = get_all_whole_image_filenames(group.val_whole_image_ann_df)\n",
    "        val_whole_image_labels = get_all_whole_image_labels(group.val_whole_image_ann_df)\n",
    "        val_whole_image_predictions = get_all_whole_image_predictions(val_patch_predictions)\n",
    "        print_confusion_matrix(val_whole_image_labels, val_whole_image_predictions)\n",
    "        \n",
    "        # Generate prediction label results file\n",
    "        write_patch_predictions(run_id, 'val', output_dir, group.val_patch_ann_df, val_patch_predictions)\n",
    "        \n",
    "        write_whole_image_predictions(run_id, 'val', output_dir, \n",
    "                                  val_whole_image_filenames, \n",
    "                                  val_whole_image_labels, \n",
    "                                  val_whole_image_predictions)\n",
    "    \n",
    "\n",
    "\n",
    "def train_and_test(run_id, model, group, num_epochs, output_dir):    \n",
    "    model_trained, history = train(run_id, model, group, num_epochs)\n",
    "    \n",
    "    # Save weights\n",
    "    model_trained.save_weights(os.path.join(output_dir, \"{}_weights.h5\".format(run_id)))\n",
    "    \n",
    "    # Plot history metrics\n",
    "    plot_metrics(run_id, output_dir, model_trained, history, group)\n",
    "    \n",
    "    # Classify test data\n",
    "    return predict(model_trained, group.test_generator)\n",
    "\n",
    "\n",
    "def predict(model, data_generator):\n",
    "    y_hat_logits = model.predict(data_generator)\n",
    "    y_hat = tf.map_fn(lambda x: tf.argmax(x), y_hat_logits, dtype=tf.int64)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "\n",
    "def predict_whole_images(patch_predictions, rows, columns, csvfile):\n",
    "    y_hat_test = patch_predictions\n",
    "    y_hat_patch_groups = np.split(y_hat_test, int(len(y_hat_test)/(rows * columns)))\n",
    "    y_hat_whole_images = list(map(lambda x: stats.mode(x).mode[0], y_hat_patch_groups))\n",
    "\n",
    "    print(\"Distribution\")\n",
    "    print('-' * 8)\n",
    "    for k, v in sorted(Counter(y_hat_whole_images).items()): \n",
    "        print(str(k) + ': '+ str(v))    \n",
    "\n",
    "    one_hots = [np.zeros((5,1)) for pred in y_hat_whole_images]\n",
    "    for i in range(len(one_hots)):\n",
    "        pred = y_hat_whole_images[i]  # the index of the one-hot encoding\n",
    "        one_hots[i][pred] = 1\n",
    "    with open(csvfile, 'w') as predictions_file:\n",
    "        writer = csv.writer(predictions_file)\n",
    "        for pred in one_hots:\n",
    "            pred = np.array(pred, dtype=int)\n",
    "            writer.writerow(pred.T.tolist()[0])\n",
    "    print('Finished generating predictions to', csvfile)\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y, y_hat):\n",
    "    confusion_matrix = np.zeros((5, 5))\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ground_truth = y==labels[i]\n",
    "            prediction = y_hat==labels[j]\n",
    "            confusion_matrix[i, j] = sum(np.bitwise_and(ground_truth, prediction))\n",
    "    df = pd.DataFrame(confusion_matrix, dtype=int)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def write_patch_predictions(run_id, phase, output_dir, patch_ann_df, patch_predictions):\n",
    "    df = pd.DataFrame(columns=['file_name', 'annotation', 'prediction'])\n",
    "    i = 0\n",
    "    for idx, row in patch_ann_df.iterrows():\n",
    "        file_name, label = row\n",
    "        df = df.append({'file_name': file_name, \n",
    "                        'annotation': label, \n",
    "                        'prediction': patch_predictions[i]}, ignore_index=True)\n",
    "        i += 1\n",
    "\n",
    "    df.to_csv(os.path.join(output_dir, \"{}_{}_patch_predictions.csv\".format(run_id, phase)), index=False)\n",
    "    \n",
    "\n",
    "def write_whole_image_predictions(run_id, phase, output_dir, filenames, labels, predictions):\n",
    "    print('filenames:', len(filenames))\n",
    "    print('labels:', len(labels))\n",
    "    print('predictions:', len(predictions))\n",
    "    \n",
    "    df = pd.DataFrame(columns=['file_name', 'annotation', 'prediction'])\n",
    "    for i in range(len(filenames)):\n",
    "        df = df.append({'file_name': filenames[i], \n",
    "                        'annotation': labels[i], \n",
    "                        'prediction': predictions[i]}, ignore_index=True)\n",
    "        \n",
    "    df.to_csv(os.path.join(output_dir, \"{}_{}_whole_image_predictions.csv\".format(run_id, phase)), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "The following hyperparameters can be tuned:\n",
    "1. `n1` - Number of neurons in the first classifier dense layer\n",
    "2. `n2` - Number of neurons in the second classifier dense layer\n",
    "3. `d` - Dropout rate after classifier dense layers\n",
    "4. class weights - `[1,1,1,1,1]` (default) or `[1,1,5,5,1]`\n",
    "5. batch normalization - `no` or `yes`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(name, model, class_weights=None, num_epochs=40):\n",
    "    run_id = shortuuid.uuid()\n",
    "    \n",
    "    # output directory\n",
    "    output_dir = os.path.join(\"output_tf\", run_id)\n",
    "    os.makedirs(output_dir)\n",
    "    print(\"Output generated to:\", output_dir)\n",
    "    \n",
    "    \n",
    "    y_hat_test = train_and_test(run_id, model, train_val_split_group(class_weights), num_epochs, output_dir)\n",
    "    predictions_file = os.path.join(output_dir, \"{}_predict_c2_{}_test.csv\".format(run_id, name))\n",
    "    print('predictions file:', predictions_file)\n",
    "    predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)\n",
    "    return y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H1: 1024-128-5\n",
    "\n",
    "* DNN Structure: 1024-128-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output generated to: output_tf/9NWUwLF34eEFEqzMVD4xgp\n",
      "Reading annotations...\n",
      "Computing class weights...\n",
      "{0: 0.4205128205128205, 1: 1.1081081081081081, 2: 1.5769230769230769, 3: 1.561904761904762, 4: 2.2465753424657535}\n",
      "Defining train data generator...\n",
      "Found 20500 validated image filenames belonging to 5 classes.\n",
      "Defining validation data generator...\n",
      "Found 5125 validated image filenames belonging to 5 classes.\n",
      "Found 5000 images belonging to 1 classes.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, None, None, 512)   14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 15,371,845\n",
      "Trainable params: 657,157\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Fitting model...\n",
      "group.train_patch_ann_df.shape[0]= 20500\n",
      "int(group.train_patch_ann_df.shape[0] / BATCH_SIZE) 640\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 640 steps, validate for 160 steps\n",
      "Epoch 1/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 1.0440 - accuracy: 0.5345 - val_loss: 0.7758 - val_accuracy: 0.6318\n",
      "Epoch 2/40\n",
      "640/640 [==============================] - 51s 79ms/step - loss: 0.8524 - accuracy: 0.6201 - val_loss: 0.7505 - val_accuracy: 0.6729\n",
      "Epoch 3/40\n",
      "640/640 [==============================] - 48s 76ms/step - loss: 0.7908 - accuracy: 0.6517 - val_loss: 0.6804 - val_accuracy: 0.6916\n",
      "Epoch 4/40\n",
      "640/640 [==============================] - 50s 78ms/step - loss: 0.7418 - accuracy: 0.6707 - val_loss: 0.6735 - val_accuracy: 0.6893\n",
      "Epoch 5/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.7050 - accuracy: 0.6909 - val_loss: 0.6299 - val_accuracy: 0.7527\n",
      "Epoch 6/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.6725 - accuracy: 0.7014 - val_loss: 0.6384 - val_accuracy: 0.7008\n",
      "Epoch 7/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.6463 - accuracy: 0.7099 - val_loss: 0.6248 - val_accuracy: 0.7117\n",
      "Epoch 8/40\n",
      "640/640 [==============================] - 50s 77ms/step - loss: 0.6303 - accuracy: 0.7199 - val_loss: 0.5946 - val_accuracy: 0.7588\n",
      "Epoch 9/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.5987 - accuracy: 0.7357 - val_loss: 0.6049 - val_accuracy: 0.7307\n",
      "Epoch 10/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.5764 - accuracy: 0.7436 - val_loss: 0.5913 - val_accuracy: 0.7717\n",
      "Epoch 11/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.5563 - accuracy: 0.7511 - val_loss: 0.5850 - val_accuracy: 0.7240\n",
      "Epoch 12/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.5518 - accuracy: 0.7550 - val_loss: 0.5900 - val_accuracy: 0.7443\n",
      "Epoch 13/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.5322 - accuracy: 0.7607 - val_loss: 0.5754 - val_accuracy: 0.7299\n",
      "Epoch 14/40\n",
      "640/640 [==============================] - 48s 75ms/step - loss: 0.5231 - accuracy: 0.7633 - val_loss: 0.5969 - val_accuracy: 0.7385\n",
      "Epoch 15/40\n",
      "640/640 [==============================] - 50s 77ms/step - loss: 0.5091 - accuracy: 0.7694 - val_loss: 0.5887 - val_accuracy: 0.7428\n",
      "Epoch 16/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.5048 - accuracy: 0.7740 - val_loss: 0.5651 - val_accuracy: 0.7637\n",
      "Epoch 17/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.4895 - accuracy: 0.7776 - val_loss: 0.5774 - val_accuracy: 0.7455\n",
      "Epoch 18/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.4704 - accuracy: 0.7879 - val_loss: 0.5907 - val_accuracy: 0.7508\n",
      "Epoch 19/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.4653 - accuracy: 0.7896 - val_loss: 0.5701 - val_accuracy: 0.7561\n",
      "Epoch 20/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.4496 - accuracy: 0.7893 - val_loss: 0.5990 - val_accuracy: 0.7404\n",
      "Epoch 21/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.4411 - accuracy: 0.7978 - val_loss: 0.6222 - val_accuracy: 0.7887\n",
      "Epoch 22/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.4336 - accuracy: 0.7979 - val_loss: 0.6739 - val_accuracy: 0.7379\n",
      "Epoch 23/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.4299 - accuracy: 0.8011 - val_loss: 0.6213 - val_accuracy: 0.7400\n",
      "Epoch 24/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.4335 - accuracy: 0.7995 - val_loss: 0.5840 - val_accuracy: 0.7580\n",
      "Epoch 25/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.4077 - accuracy: 0.8089 - val_loss: 0.6334 - val_accuracy: 0.7189\n",
      "Epoch 26/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.4064 - accuracy: 0.8118 - val_loss: 0.5980 - val_accuracy: 0.7586\n",
      "Epoch 27/40\n",
      "640/640 [==============================] - 48s 76ms/step - loss: 0.3994 - accuracy: 0.8137 - val_loss: 0.6519 - val_accuracy: 0.7570\n",
      "Epoch 28/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.3917 - accuracy: 0.8138 - val_loss: 0.6185 - val_accuracy: 0.7189\n",
      "Epoch 29/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.3786 - accuracy: 0.8222 - val_loss: 0.5902 - val_accuracy: 0.7377\n",
      "Epoch 30/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.3833 - accuracy: 0.8177 - val_loss: 0.6256 - val_accuracy: 0.7521\n",
      "Epoch 31/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.3639 - accuracy: 0.8281 - val_loss: 0.6068 - val_accuracy: 0.7584\n",
      "Epoch 32/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.3552 - accuracy: 0.8285 - val_loss: 0.6161 - val_accuracy: 0.7703\n",
      "Epoch 33/40\n",
      "640/640 [==============================] - 50s 78ms/step - loss: 0.3576 - accuracy: 0.8341 - val_loss: 0.6498 - val_accuracy: 0.7461\n",
      "Epoch 34/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.3515 - accuracy: 0.8387 - val_loss: 0.6398 - val_accuracy: 0.7463\n",
      "Epoch 35/40\n",
      "640/640 [==============================] - 50s 78ms/step - loss: 0.3522 - accuracy: 0.8346 - val_loss: 0.6334 - val_accuracy: 0.7598\n",
      "Epoch 36/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.3316 - accuracy: 0.8421 - val_loss: 0.6238 - val_accuracy: 0.7760\n",
      "Epoch 37/40\n",
      "640/640 [==============================] - 49s 77ms/step - loss: 0.3419 - accuracy: 0.8394 - val_loss: 0.6414 - val_accuracy: 0.7855\n",
      "Epoch 38/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.3381 - accuracy: 0.8397 - val_loss: 0.6839 - val_accuracy: 0.7533\n",
      "Epoch 39/40\n",
      "640/640 [==============================] - 50s 79ms/step - loss: 0.3288 - accuracy: 0.8428 - val_loss: 0.6111 - val_accuracy: 0.7703\n",
      "Epoch 40/40\n",
      "640/640 [==============================] - 49s 76ms/step - loss: 0.3117 - accuracy: 0.8486 - val_loss: 0.6776 - val_accuracy: 0.7521\n",
      "\n",
      "Metrics\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions file: output_tf/9NWUwLF34eEFEqzMVD4xgp/9NWUwLF34eEFEqzMVD4xgp_predict_c2_h1_test.csv\n",
      "0: 82\n",
      "1: 53\n",
      "2: 3\n",
      "3: 20\n",
      "4: 42\n",
      "Finished generating predictions to output_tf/9NWUwLF34eEFEqzMVD4xgp/9NWUwLF34eEFEqzMVD4xgp_predict_c2_h1_test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5000,), dtype=int64, numpy=array([1, 2, 0, ..., 0, 0, 0])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(1024, 128, 0.5)\n",
    "run_trial(\"h1\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2: 2048-256-5\n",
    "\n",
    "* DNN Structure: 2048-256-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model(2048, 256, 0.5)\n",
    "run_trial(\"h2\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H3: 512-64-5\n",
    "\n",
    "* DNN Structure: 512-64-5\n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(512, 64, 0.5)\n",
    "run_trial(\"h3\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H4: Best from above, dropout 0.25\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.25\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.25)\n",
    "run_trial(\"h4\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H5: Best from above, dropout 0.1\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.1\n",
    "* Class weights: [1,1,1,1,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.1)\n",
    "run_trial(\"h5\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H6: Best from above, skewed class weights\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: [1,1,5,5,1]\n",
    "* Batch normalization: no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n1, n2, 0.5)\n",
    "run_trial(\"h6\", model, class_weights=[1., 1., 2., 2., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H7: Best from above, batch normalization\n",
    "\n",
    "* DNN Structure: \n",
    "* Dropout: 0.5\n",
    "* Class weights: \n",
    "* Batch normalization: yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not currently available in Keras\n",
    "\n",
    "#model = get_model(n1, n2, 0.5)\n",
    "#run_trial(\"h7\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_n1 = 1024\n",
    "optimal_n2 = 128\n",
    "optimal_d = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with all C2 data and optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(optimal_n1, optimal_n2, optimal_d)\n",
    "# model.load_state_dict(torch.load('cnn_pytorch_c2.pt'))\n",
    "y_hat_test = train_and_test(model, group_3(), num_epochs=40)\n",
    "predictions_file = \"predict_c2_{}.csv\".format(shortuuid.uuid())\n",
    "print('predictions file:', predictions_file)\n",
    "predict_whole_images(y_hat_test, PATCH_ROWS, PATCH_COLUMNS, predictions_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
